#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸Šä¸‹æ–‡å¤šè‡‚è€è™æœº (Contextual Multi-Armed Bandit)
Contextual Multi-Armed Bandit for intelligent strategy selection

æ ¸å¿ƒåˆ›æ–°ï¼š
1. åŸºäºä»»åŠ¡ä¸Šä¸‹æ–‡ç‰¹å¾çš„æ™ºèƒ½ç­–ç•¥é€‰æ‹©
2. LinUCB å’Œ Contextual Thompson Sampling å®ç°
3. æŒä¹…åŒ–å­¦ä¹ å‚æ•°å­˜å‚¨ (SQLite/DuckDB)
4. å·¥å…·å¥åº·çŠ¶æ€å’Œç¯å¢ƒæ„ŸçŸ¥
5. å¯éªŒè¯çš„æˆåŠŸä¿¡å·å®šä¹‰
"""

import time
import json
import sqlite3
import logging
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field, asdict
from pathlib import Path
import hashlib
from enum import Enum

logger = logging.getLogger(__name__)

class SuccessMetric(Enum):
    """æˆåŠŸæŒ‡æ ‡å®šä¹‰"""
    CONTRACT_SUCCESS = "contract_success"      # åˆåŒæˆåŠŸåˆ¤å®šé€šè¿‡
    EXECUTION_SUCCESS = "execution_success"    # æ‰§è¡ŒæˆåŠŸ
    USER_SATISFACTION = "user_satisfaction"    # ç”¨æˆ·æ»¡æ„åº¦
    COST_EFFICIENCY = "cost_efficiency"        # æˆæœ¬æ•ˆç‡
    TIME_EFFICIENCY = "time_efficiency"        # æ—¶é—´æ•ˆç‡

@dataclass
class ContextFeatures:
    """ä¸Šä¸‹æ–‡ç‰¹å¾å‘é‡"""
    # ä»»åŠ¡ç­¾åç‰¹å¾
    task_intent: str = "question"              # ä»»åŠ¡æ„å›¾
    task_domain: str = "general"               # ä»»åŠ¡é¢†åŸŸ
    task_complexity: float = 0.5               # å¤æ‚åº¦ [0,1]
    input_length: int = 0                      # è¾“å…¥é•¿åº¦
    input_structure_score: float = 0.5         # è¾“å…¥ç»“æ„åŒ–ç¨‹åº¦
    
    # å·¥å…·å¯è¾¾æ€§ç‰¹å¾
    tool_health_scores: Dict[str, float] = field(default_factory=dict)  # å·¥å…·å¥åº·åˆ†æ•°
    tool_latencies: Dict[str, float] = field(default_factory=dict)      # å·¥å…·å»¶è¿Ÿ
    tool_rate_limits: Dict[str, bool] = field(default_factory=dict)     # é€Ÿç‡é™åˆ¶çŠ¶æ€
    
    # ç¯å¢ƒç‰¹å¾
    current_hour: int = 12                     # å½“å‰å°æ—¶
    network_quality: float = 1.0              # ç½‘ç»œè´¨é‡ [0,1]
    system_load: float = 0.5                  # ç³»ç»Ÿè´Ÿè½½ [0,1]
    
    # å†å²ç»©æ•ˆç‰¹å¾
    historical_success_rates: Dict[str, float] = field(default_factory=dict)  # å†å²æˆåŠŸç‡
    recent_failures: Dict[str, int] = field(default_factory=dict)             # æœ€è¿‘å¤±è´¥æ¬¡æ•°
    
    # é¢„ç®—å’Œçº¦æŸ
    time_budget: float = 30.0                  # æ—¶é—´é¢„ç®—(ç§’)
    cost_budget: float = 1.0                   # æˆæœ¬é¢„ç®—
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸è¡¨ç¤º"""
        return asdict(self)
    
    def to_vector(self, feature_names: List[str]) -> np.ndarray:
        """è½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
        vector = []
        feature_dict = asdict(self)
        
        for name in feature_names:
            if name in feature_dict:
                value = feature_dict[name]
                if isinstance(value, (int, float)):
                    vector.append(float(value))
                elif isinstance(value, str):
                    # å­—ç¬¦ä¸²ç‰¹å¾ç¼–ç 
                    vector.append(hash(value) % 100 / 100.0)
                elif isinstance(value, dict):
                    # å­—å…¸ç‰¹å¾èšåˆ
                    if value:
                        vector.append(np.mean(list(value.values())))
                    else:
                        vector.append(0.0)
                else:
                    vector.append(0.0)
            else:
                vector.append(0.0)
        
        return np.array(vector)

@dataclass
class ActionOutcome:
    """åŠ¨ä½œç»“æœ"""
    action_id: str
    context_features: ContextFeatures
    success_metrics: Dict[SuccessMetric, float]
    execution_time: float
    cost: float
    timestamp: float
    additional_info: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def overall_reward(self) -> float:
        """è®¡ç®—ç»¼åˆå¥–åŠ±"""
        # åŠ æƒç»„åˆä¸åŒæˆåŠŸæŒ‡æ ‡
        weights = {
            SuccessMetric.CONTRACT_SUCCESS: 0.4,
            SuccessMetric.EXECUTION_SUCCESS: 0.3,
            SuccessMetric.USER_SATISFACTION: 0.2,
            SuccessMetric.COST_EFFICIENCY: 0.05,
            SuccessMetric.TIME_EFFICIENCY: 0.05
        }
        
        reward = 0.0
        for metric, weight in weights.items():
            if metric in self.success_metrics:
                reward += weight * self.success_metrics[metric]
        
        return reward

class ContextualBanditStorage:
    """æŒä¹…åŒ–å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "contextual_bandit.db"):
        self.db_path = Path(db_path)
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS bandit_parameters (
                    algorithm TEXT,
                    action_id TEXT,
                    parameter_name TEXT,
                    parameter_value BLOB,
                    updated_at REAL,
                    PRIMARY KEY (algorithm, action_id, parameter_name)
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS action_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    action_id TEXT,
                    context_hash TEXT,
                    context_features TEXT,
                    reward REAL,
                    execution_time REAL,
                    cost REAL,
                    timestamp REAL
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS feature_metadata (
                    feature_name TEXT PRIMARY KEY,
                    feature_type TEXT,
                    normalization_params TEXT,
                    updated_at REAL
                )
            """)
    
    def save_parameters(self, algorithm: str, action_id: str, parameters: Dict[str, np.ndarray]):
        """ä¿å­˜æ¨¡å‹å‚æ•°"""
        with sqlite3.connect(self.db_path) as conn:
            for param_name, param_value in parameters.items():
                conn.execute("""
                    INSERT OR REPLACE INTO bandit_parameters 
                    (algorithm, action_id, parameter_name, parameter_value, updated_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (algorithm, action_id, param_name, param_value.tobytes(), time.time()))
    
    def load_parameters(self, algorithm: str, action_id: str) -> Dict[str, np.ndarray]:
        """åŠ è½½æ¨¡å‹å‚æ•°"""
        parameters = {}
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT parameter_name, parameter_value 
                FROM bandit_parameters 
                WHERE algorithm = ? AND action_id = ?
            """, (algorithm, action_id))
            
            for param_name, param_blob in cursor.fetchall():
                parameters[param_name] = np.frombuffer(param_blob)
        
        return parameters
    
    def save_action_outcome(self, outcome: ActionOutcome):
        """ä¿å­˜åŠ¨ä½œç»“æœ"""
        context_hash = hashlib.md5(
            json.dumps(asdict(outcome.context_features), sort_keys=True).encode()
        ).hexdigest()
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO action_history 
                (action_id, context_hash, context_features, reward, execution_time, cost, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                outcome.action_id,
                context_hash,
                json.dumps(asdict(outcome.context_features)),
                outcome.overall_reward,
                outcome.execution_time,
                outcome.cost,
                outcome.timestamp
            ))

class LinUCBBandit:
    """Linear Upper Confidence Bound Bandit"""
    
    def __init__(self, feature_dim: int, alpha: float = 1.0, storage: Optional[ContextualBanditStorage] = None):
        self.feature_dim = feature_dim
        self.alpha = alpha
        self.storage = storage
        
        # æ¯ä¸ªåŠ¨ä½œçš„å‚æ•°
        self.A = {}  # åæ–¹å·®çŸ©é˜µ A_a
        self.b = {}  # å¥–åŠ±å‘é‡ b_a
        self.theta = {}  # å‚æ•°ä¼°è®¡ theta_a
        
        logger.info(f"ğŸ¯ LinUCB Bandit åˆå§‹åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {feature_dim}")
    
    def _init_action(self, action_id: str):
        """åˆå§‹åŒ–åŠ¨ä½œå‚æ•°"""
        if action_id not in self.A:
            # å°è¯•ä»å­˜å‚¨åŠ è½½
            if self.storage:
                params = self.storage.load_parameters("linucb", action_id)
                if params:
                    self.A[action_id] = params.get("A", np.eye(self.feature_dim))
                    self.b[action_id] = params.get("b", np.zeros(self.feature_dim))
                    self.theta[action_id] = params.get("theta", np.zeros(self.feature_dim))
                    logger.debug(f"ğŸ“‚ ä»å­˜å‚¨åŠ è½½ LinUCB å‚æ•°: {action_id}")
                    return
            
            # æ–°åˆå§‹åŒ–
            self.A[action_id] = np.eye(self.feature_dim)
            self.b[action_id] = np.zeros(self.feature_dim)
            self.theta[action_id] = np.zeros(self.feature_dim)
            logger.debug(f"ğŸ†• æ–°å»º LinUCB åŠ¨ä½œ: {action_id}")
    
    def predict(self, context: ContextFeatures, available_actions: List[str]) -> Tuple[str, float, Dict[str, float]]:
        """é¢„æµ‹æœ€ä½³åŠ¨ä½œ"""
        feature_names = [
            "task_complexity", "input_length", "input_structure_score",
            "current_hour", "network_quality", "system_load", "time_budget", "cost_budget"
        ]
        x = context.to_vector(feature_names)
        
        if len(x) != self.feature_dim:
            # è°ƒæ•´ç‰¹å¾å‘é‡ç»´åº¦
            if len(x) < self.feature_dim:
                x = np.pad(x, (0, self.feature_dim - len(x)))
            else:
                x = x[:self.feature_dim]
        
        ucb_values = {}
        
        for action_id in available_actions:
            self._init_action(action_id)
            
            # è®¡ç®— UCB å€¼
            A_inv = np.linalg.inv(self.A[action_id])
            theta = A_inv @ self.b[action_id]
            
            confidence_bonus = self.alpha * np.sqrt(x.T @ A_inv @ x)
            ucb_value = theta.T @ x + confidence_bonus
            
            ucb_values[action_id] = float(ucb_value)
        
        # é€‰æ‹©æœ€é«˜ UCB å€¼çš„åŠ¨ä½œ
        best_action = max(ucb_values, key=ucb_values.get)
        best_value = ucb_values[best_action]
        
        logger.debug(f"ğŸ¯ LinUCB é¢„æµ‹: {best_action} (UCB: {best_value:.3f})")
        return best_action, best_value, ucb_values
    
    def fit(self, context: ContextFeatures, action_id: str, reward: float):
        """æ›´æ–°æ¨¡å‹å‚æ•°"""
        feature_names = [
            "task_complexity", "input_length", "input_structure_score", 
            "current_hour", "network_quality", "system_load", "time_budget", "cost_budget"
        ]
        x = context.to_vector(feature_names)
        
        if len(x) != self.feature_dim:
            if len(x) < self.feature_dim:
                x = np.pad(x, (0, self.feature_dim - len(x)))
            else:
                x = x[:self.feature_dim]
        
        self._init_action(action_id)
        
        # æ›´æ–°å‚æ•°
        self.A[action_id] += np.outer(x, x)
        self.b[action_id] += reward * x
        
        # ä¿å­˜åˆ°å­˜å‚¨
        if self.storage:
            params = {
                "A": self.A[action_id],
                "b": self.b[action_id],
                "theta": np.linalg.inv(self.A[action_id]) @ self.b[action_id]
            }
            self.storage.save_parameters("linucb", action_id, params)
        
        logger.debug(f"ğŸ“ˆ LinUCB å‚æ•°æ›´æ–°: {action_id}, å¥–åŠ±: {reward:.3f}")

class ContextualThompsonSampling:
    """ä¸Šä¸‹æ–‡ Thompson Sampling"""
    
    def __init__(self, feature_dim: int, alpha: float = 1.0, beta: float = 1.0, 
                 storage: Optional[ContextualBanditStorage] = None):
        self.feature_dim = feature_dim
        self.alpha = alpha  # å…ˆéªŒç²¾åº¦
        self.beta = beta    # å™ªå£°ç²¾åº¦
        self.storage = storage
        
        # æ¯ä¸ªåŠ¨ä½œçš„è´å¶æ–¯çº¿æ€§å›å½’å‚æ•°
        self.S = {}  # åæ–¹å·®çŸ©é˜µçš„é€†
        self.mu = {}  # å‡å€¼å‘é‡
        
        logger.info(f"ğŸ² Contextual Thompson Sampling åˆå§‹åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {feature_dim}")
    
    def _init_action(self, action_id: str):
        """åˆå§‹åŒ–åŠ¨ä½œå‚æ•°"""
        if action_id not in self.S:
            # å°è¯•ä»å­˜å‚¨åŠ è½½
            if self.storage:
                params = self.storage.load_parameters("thompson", action_id)
                if params:
                    self.S[action_id] = params.get("S", self.alpha * np.eye(self.feature_dim))
                    self.mu[action_id] = params.get("mu", np.zeros(self.feature_dim))
                    logger.debug(f"ğŸ“‚ ä»å­˜å‚¨åŠ è½½ Thompson å‚æ•°: {action_id}")
                    return
            
            # æ–°åˆå§‹åŒ–
            self.S[action_id] = self.alpha * np.eye(self.feature_dim)
            self.mu[action_id] = np.zeros(self.feature_dim)
            logger.debug(f"ğŸ†• æ–°å»º Thompson åŠ¨ä½œ: {action_id}")
    
    def predict(self, context: ContextFeatures, available_actions: List[str]) -> Tuple[str, float, Dict[str, float]]:
        """é¢„æµ‹æœ€ä½³åŠ¨ä½œ"""
        feature_names = [
            "task_complexity", "input_length", "input_structure_score",
            "current_hour", "network_quality", "system_load", "time_budget", "cost_budget"
        ]
        x = context.to_vector(feature_names)
        
        if len(x) != self.feature_dim:
            if len(x) < self.feature_dim:
                x = np.pad(x, (0, self.feature_dim - len(x)))
            else:
                x = x[:self.feature_dim]
        
        sampled_values = {}
        
        for action_id in available_actions:
            self._init_action(action_id)
            
            # ä»åéªŒåˆ†å¸ƒé‡‡æ · theta
            try:
                S_inv = np.linalg.inv(self.S[action_id])
                theta_sample = np.random.multivariate_normal(self.mu[action_id], S_inv)
                sampled_value = float(theta_sample.T @ x)
                sampled_values[action_id] = sampled_value
            except np.linalg.LinAlgError:
                # å¤„ç†å¥‡å¼‚çŸ©é˜µ
                sampled_values[action_id] = np.random.normal(0, 1)
        
        # é€‰æ‹©æœ€é«˜é‡‡æ ·å€¼çš„åŠ¨ä½œ
        best_action = max(sampled_values, key=sampled_values.get)
        best_value = sampled_values[best_action]
        
        logger.debug(f"ğŸ² Thompson é¢„æµ‹: {best_action} (é‡‡æ ·å€¼: {best_value:.3f})")
        return best_action, best_value, sampled_values
    
    def fit(self, context: ContextFeatures, action_id: str, reward: float):
        """æ›´æ–°æ¨¡å‹å‚æ•°"""
        feature_names = [
            "task_complexity", "input_length", "input_structure_score",
            "current_hour", "network_quality", "system_load", "time_budget", "cost_budget"
        ]
        x = context.to_vector(feature_names)
        
        if len(x) != self.feature_dim:
            if len(x) < self.feature_dim:
                x = np.pad(x, (0, self.feature_dim - len(x)))
            else:
                x = x[:self.feature_dim]
        
        self._init_action(action_id)
        
        # è´å¶æ–¯æ›´æ–°
        self.S[action_id] += self.beta * np.outer(x, x)
        
        try:
            S_inv = np.linalg.inv(self.S[action_id])
            self.mu[action_id] = S_inv @ (self.S[action_id] @ self.mu[action_id] + self.beta * reward * x)
        except np.linalg.LinAlgError:
            logger.warning(f"âš ï¸ Thompson Sampling çŸ©é˜µå¥‡å¼‚ï¼Œè·³è¿‡æ›´æ–°: {action_id}")
            return
        
        # ä¿å­˜åˆ°å­˜å‚¨
        if self.storage:
            params = {
                "S": self.S[action_id],
                "mu": self.mu[action_id]
            }
            self.storage.save_parameters("thompson", action_id, params)
        
        logger.debug(f"ğŸ“ˆ Thompson å‚æ•°æ›´æ–°: {action_id}, å¥–åŠ±: {reward:.3f}")

class ContextualBanditManager:
    """ä¸Šä¸‹æ–‡Banditç®¡ç†å™¨"""
    
    def __init__(self, feature_dim: int = 8, algorithm: str = "linucb", 
                 storage_path: str = "contextual_bandit.db"):
        self.feature_dim = feature_dim
        self.algorithm = algorithm
        self.storage = ContextualBanditStorage(storage_path)
        
        # åˆå§‹åŒ–ç®—æ³•
        if algorithm == "linucb":
            self.bandit = LinUCBBandit(feature_dim, storage=self.storage)
        elif algorithm == "thompson":
            self.bandit = ContextualThompsonSampling(feature_dim, storage=self.storage)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_selections = 0
        self.action_counts = {}
        self.recent_rewards = []
        
        logger.info(f"ğŸ¯ ContextualBandit ç®¡ç†å™¨åˆå§‹åŒ–: {algorithm}, ç‰¹å¾ç»´åº¦: {feature_dim}")
    
    def select_action(self, context: ContextFeatures, available_actions: List[str]) -> Tuple[str, float, Dict[str, Any]]:
        """é€‰æ‹©æœ€ä½³åŠ¨ä½œ"""
        if not available_actions:
            raise ValueError("å¯ç”¨åŠ¨ä½œåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        self.total_selections += 1
        
        # é¢„æµ‹æœ€ä½³åŠ¨ä½œ
        best_action, confidence, all_values = self.bandit.predict(context, available_actions)
        
        # æ›´æ–°ç»Ÿè®¡
        self.action_counts[best_action] = self.action_counts.get(best_action, 0) + 1
        
        selection_info = {
            "algorithm": self.algorithm,
            "confidence": confidence,
            "all_action_values": all_values,
            "selection_count": self.total_selections,
            "action_usage_count": self.action_counts[best_action],
            "context_summary": {
                "task_complexity": context.task_complexity,
                "task_domain": context.task_domain,
                "task_intent": context.task_intent
            }
        }
        
        logger.info(f"ğŸ¯ é€‰æ‹©åŠ¨ä½œ: {best_action} (ç½®ä¿¡åº¦: {confidence:.3f})")
        return best_action, confidence, selection_info
    
    def update_reward(self, context: ContextFeatures, action_id: str, outcome: ActionOutcome):
        """æ›´æ–°å¥–åŠ±ä¿¡å·"""
        reward = outcome.overall_reward
        
        # æ›´æ–°æ¨¡å‹
        self.bandit.fit(context, action_id, reward)
        
        # ä¿å­˜ç»“æœ
        self.storage.save_action_outcome(outcome)
        
        # æ›´æ–°ç»Ÿè®¡
        self.recent_rewards.append(reward)
        if len(self.recent_rewards) > 100:
            self.recent_rewards = self.recent_rewards[-50:]
        
        logger.info(f"ğŸ“ˆ æ›´æ–°å¥–åŠ±: {action_id}, å¥–åŠ±: {reward:.3f}")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = {
            "total_selections": self.total_selections,
            "action_counts": self.action_counts.copy(),
            "recent_avg_reward": np.mean(self.recent_rewards) if self.recent_rewards else 0.0,
            "algorithm": self.algorithm,
            "feature_dim": self.feature_dim
        }
        
        if self.action_counts:
            most_used = max(self.action_counts, key=self.action_counts.get)
            stats["most_used_action"] = most_used
            stats["usage_distribution"] = {
                action: count / self.total_selections 
                for action, count in self.action_counts.items()
            }
        
        return stats
