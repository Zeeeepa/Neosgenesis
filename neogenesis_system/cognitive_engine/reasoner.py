
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è½»é‡çº§åˆ†æåŠ©æ‰‹ - ä¸“æ³¨äºå¿«é€Ÿä»»åŠ¡è¯„ä¼°å’Œå¤æ‚åº¦åˆ†æ
Lightweight Analysis Assistant - focused on rapid task assessment and complexity analysis

æ ¸å¿ƒèŒè´£ï¼š
1. ä»»åŠ¡å¤æ‚åº¦åˆ†æ (å¿«é€Ÿå¯å‘å¼æ–¹æ³•)
2. ä»»åŠ¡ç½®ä¿¡åº¦è¯„ä¼° (åŸºäºå†å²æ•°æ®å’Œæ¨¡å¼)
3. é¢†åŸŸæ¨æ–­å’Œç»Ÿè®¡åˆ†æ (è¾…åŠ©å†³ç­–æ”¯æŒ)

"""

import time
import logging
import json
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

# LLM ç›¸å…³å¯¼å…¥ - æ”¯æŒå¤šç§å¯¼å…¥æ–¹å¼
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..providers.llm_manager import LLMManager
    from ..providers.impl.ollama_client import create_ollama_client, OllamaClient
    from ..providers.llm_base import LLMConfig, LLMProvider, LLMMessage
    LLM_AVAILABLE = True
except ImportError as e1:
    try:
        # å°è¯•ç»å¯¹å¯¼å…¥
        from neogenesis_system.providers.llm_manager import LLMManager
        from neogenesis_system.providers.impl.ollama_client import create_ollama_client, OllamaClient
        from neogenesis_system.providers.llm_base import LLMConfig, LLMProvider, LLMMessage
        LLM_AVAILABLE = True
    except ImportError as e2:
        try:
            # å°è¯•ç›´æ¥å¯¼å…¥ï¼ˆé€‚ç”¨äºç›´æ¥è¿è¡Œçš„æƒ…å†µï¼‰
            import sys
            import os
            current_dir = os.path.dirname(os.path.abspath(__file__))
            parent_dir = os.path.dirname(current_dir)
            if parent_dir not in sys.path:
                sys.path.insert(0, parent_dir)
            
            from providers.llm_manager import LLMManager
            from providers.impl.ollama_client import create_ollama_client, OllamaClient
            from providers.llm_base import LLMConfig, LLMProvider, LLMMessage
            LLM_AVAILABLE = True
        except ImportError as e3:
            logger.debug(f"LLMç»„ä»¶å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨çº¯å¯å‘å¼æ¨¡å¼: ç›¸å¯¹å¯¼å…¥({e1}), ç»å¯¹å¯¼å…¥({e2}), ç›´æ¥å¯¼å…¥({e3})")
            LLM_AVAILABLE = False
            # å®šä¹‰ç©ºçš„ç±»å‹æç¤º
            LLMManager = None
            OllamaClient = None


# ==================== è·¯ç”±åˆ†ç±»æ•°æ®ç»“æ„å®šä¹‰ ====================

class TaskComplexity(Enum):
    """ä»»åŠ¡å¤æ‚åº¦æšä¸¾"""
    SIMPLE = "simple"         # ç®€å•ä»»åŠ¡ï¼šç›´æ¥å›ç­”æˆ–å•æ­¥æ“ä½œ
    MODERATE = "moderate"     # ä¸­ç­‰ä»»åŠ¡ï¼šéœ€è¦ä¸€å®šåˆ†æå’Œå¤šæ­¥éª¤
    COMPLEX = "complex"       # å¤æ‚ä»»åŠ¡ï¼šéœ€è¦æ·±å…¥åˆ†æå’Œç³»ç»Ÿæ€§æ–¹æ³•
    EXPERT = "expert"         # ä¸“å®¶çº§ä»»åŠ¡ï¼šéœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚æ¨ç†

class TaskDomain(Enum):
    """ä»»åŠ¡é¢†åŸŸæšä¸¾"""
    WEB_DEV = "web_development"          # Webå¼€å‘
    DATA_SCIENCE = "data_science"        # æ•°æ®ç§‘å­¦
    API_DEV = "api_development"          # APIå¼€å‘  
    SYSTEM_ADMIN = "system_admin"        # ç³»ç»Ÿç®¡ç†
    DATABASE = "database"                # æ•°æ®åº“
    SECURITY = "security"                # å®‰å…¨
    MOBILE_DEV = "mobile_development"    # ç§»åŠ¨å¼€å‘
    ML_AI = "machine_learning"           # æœºå™¨å­¦ä¹ /AI
    GENERAL = "general"                  # é€šç”¨ä»»åŠ¡

class TaskIntent(Enum):
    """ä»»åŠ¡æ„å›¾æšä¸¾"""
    QUESTION = "question"                # å’¨è¯¢é—®é¢˜
    TASK_EXECUTION = "task_execution"    # ä»»åŠ¡æ‰§è¡Œ
    ANALYSIS = "analysis"                # åˆ†æè¯·æ±‚
    CREATION = "creation"                # åˆ›å»ºå†…å®¹
    DEBUGGING = "debugging"              # è°ƒè¯•é—®é¢˜
    OPTIMIZATION = "optimization"        # ä¼˜åŒ–æ”¹è¿›

class TaskUrgency(Enum):
    """ä»»åŠ¡ç´§æ€¥åº¦æšä¸¾"""
    LOW = "low"                         # ä½ä¼˜å…ˆçº§
    MEDIUM = "medium"                   # ä¸­ç­‰ä¼˜å…ˆçº§  
    HIGH = "high"                       # é«˜ä¼˜å…ˆçº§
    CRITICAL = "critical"               # ç´§æ€¥å…³é”®

class RouteStrategy(Enum):
    """è·¯ç”±ç­–ç•¥æšä¸¾"""
    DIRECT_RESPONSE = "direct_response"              # ç›´æ¥å›ç­”
    MULTI_STAGE_PROCESSING = "multi_stage_processing" # å¤šé˜¶æ®µå¤„ç†
    WORKFLOW_PLANNING = "workflow_planning"          # å·¥ä½œæµè§„åˆ’
    IDEATION_MODE = "ideation_mode"                 # åˆ›æ„æ¨¡å¼
    DIAGNOSTIC_MODE = "diagnostic_mode"             # è¯Šæ–­æ¨¡å¼
    EXPERT_CONSULTATION = "expert_consultation"      # ä¸“å®¶å’¨è¯¢

@dataclass
class TriageClassification:
    """ä»»åŠ¡åˆ†è¯Šåˆ†ç±»ç»“æœ"""
    complexity: TaskComplexity
    domain: TaskDomain
    intent: TaskIntent
    urgency: TaskUrgency
    route_strategy: RouteStrategy
    confidence: float                    # åˆ†ç±»ç½®ä¿¡åº¦ (0.0-1.0)
    reasoning: str                       # åˆ†ç±»ç†ç”±
    key_factors: List[str]              # å…³é”®å› ç´ 
    estimated_time: Optional[int] = None # é¢„ä¼°å¤„ç†æ—¶é—´(åˆ†é’Ÿ)
    required_resources: Optional[List[str]] = None  # æ‰€éœ€èµ„æº


@dataclass  
class PriorReasoner:
    """
    æ™ºèƒ½è·¯ç”±ç½‘å…³ - å‡çº§ç‰ˆä»»åŠ¡åˆ†æå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¿«é€Ÿä»»åŠ¡å¤æ‚åº¦åˆ†æ (å¯å‘å¼ + LLMåŒé‡åˆ†æ)
    2. æ™ºèƒ½ç½®ä¿¡åº¦è¯„ä¼° (åŸºäºè¯­ä¹‰ç†è§£)
    3. è¾“å…¥åˆ†ç±»å’Œè·¯ç”±å†³ç­– (LLMé©±åŠ¨)
    4. å‘åå…¼å®¹çš„æ¥å£è®¾è®¡
    """
    
    def __init__(self, 
                 api_key: str = "",
                 llm_manager: Optional[LLMManager] = None,
                 ollama_config: Optional[Dict[str, Any]] = None,
                 enable_llm: bool = True):
        """
        åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±ç½‘å…³
        
        Args:
            api_key: ä¿ç•™ç”¨äºå‘åå…¼å®¹
            llm_manager: å¯é€‰çš„LLMç®¡ç†å™¨å®ä¾‹
            ollama_config: Ollamaé…ç½®å‚æ•°
            enable_llm: æ˜¯å¦å¯ç”¨LLMåŠŸèƒ½
        """
        # ä¿æŒå‘åå…¼å®¹
        self.api_key = api_key
        self.assessment_cache = {}  # è¯„ä¼°ç¼“å­˜
        self.confidence_history = []  # ç½®ä¿¡åº¦å†å²
        
        # LLM èƒ½åŠ›é›†æˆ
        self.enable_llm = enable_llm and LLM_AVAILABLE
        self.llm_manager = None
        self.ollama_client = None
        
        if self.enable_llm:
            try:
                # å°è¯•åˆå§‹åŒ–LLMèƒ½åŠ›ï¼ˆä¼˜å…ˆä½¿ç”¨LLMManagerï¼Œå…¶æ¬¡Geminiï¼Œæœ€åOllamaï¼‰
                self._init_llm_capabilities(llm_manager, ollama_config)
            except Exception as e:
                logger.warning(f"âš ï¸ LLMèƒ½åŠ›åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨å¯å‘å¼æ¨¡å¼: {e}")
                self.enable_llm = False
        
        # æ—¥å¿—ä¿¡æ¯
        mode = "LLMå¢å¼ºæ™ºèƒ½åˆ†æ" if self.enable_llm else "è½»é‡çº§å¯å‘å¼åˆ†æ"
        logger.info(f"ğŸ§  PriorReasoner å·²åˆå§‹åŒ– ({mode}æ¨¡å¼)")
        
        if self.enable_llm and self.llm_manager:
            logger.info("âœ… LLMç®¡ç†å™¨å·²é›†æˆï¼Œæ”¯æŒæ™ºèƒ½è·¯ç”±åŠŸèƒ½")
        if self.enable_llm and self.ollama_client:
            logger.info("âœ… Ollamaå®¢æˆ·ç«¯å·²è¿æ¥ï¼Œæ”¯æŒæœ¬åœ°å¿«é€Ÿæ¨ç†")
    
    def _init_llm_capabilities(self, llm_manager: Optional[LLMManager], ollama_config: Optional[Dict[str, Any]]):
        """
        åˆå§‹åŒ–LLMèƒ½åŠ› - ä¼˜å…ˆä½¿ç”¨Gemini 2.5 Flash
        
        Args:
            llm_manager: å¤–éƒ¨æä¾›çš„LLMç®¡ç†å™¨
            ollama_config: Ollamaé…ç½®å‚æ•°ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
        """
        # æ–¹å¼1ï¼šä½¿ç”¨å¤–éƒ¨æä¾›çš„LLMç®¡ç†å™¨ï¼ˆæ¨èï¼‰
        if llm_manager:
            self.llm_manager = llm_manager
            logger.info("ğŸ”— ä½¿ç”¨å¤–éƒ¨æä¾›çš„LLMç®¡ç†å™¨ï¼ˆæ”¯æŒGeminiç­‰å¤šæä¾›å•†ï¼‰")
            return
        
        # æ–¹å¼2ï¼šå°è¯•åˆ›å»ºå†…ç½®LLMç®¡ç†å™¨ï¼ˆä¼˜å…ˆGeminiï¼‰
        try:
            self.llm_manager = LLMManager()
            if self.llm_manager.initialized:
                logger.info("ğŸš€ å†…ç½®LLMç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œä¼˜å…ˆä½¿ç”¨Gemini 2.5 Flash")
                return
        except Exception as e:
            logger.warning(f"âš ï¸ å†…ç½®LLMç®¡ç†å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # æ–¹å¼3ï¼šç›´æ¥å°è¯•åˆ›å»ºGeminiå®¢æˆ·ç«¯
        try:
            import os
            gemini_api_key = os.getenv("GEMINI_API_KEY", "")
            if gemini_api_key:
                try:
                    from ..providers.impl.gemini_client import create_gemini_client
                except ImportError:
                    try:
                        from neogenesis_system.providers.impl.gemini_client import create_gemini_client
                    except ImportError:
                        logger.debug("âš ï¸ Geminiå®¢æˆ·ç«¯æ¨¡å—ä¸å­˜åœ¨ï¼Œè·³è¿‡Geminiåˆå§‹åŒ–")
                        raise ImportError("gemini_client module not found")
                
                gemini_client = create_gemini_client(
                    api_key=gemini_api_key,
                    model="gemini-2.5-flash",
                    temperature=0.1,  # åˆ†ç±»ä»»åŠ¡éœ€è¦ç¡®å®šæ€§
                    max_tokens=1000   # å¿«é€Ÿå“åº”
                )
                
                # åŒ…è£…ä¸ºç®€å•çš„ç®¡ç†å™¨æ¥å£
                class SimpleGeminiManager:
                    def __init__(self, client):
                        self.client = client
                        self.initialized = True
                    
                    def call_api(self, prompt, system_message=None, **kwargs):
                        messages = []
                        if system_message:
                            messages.append({"role": "system", "content": system_message})
                        messages.append({"role": "user", "content": prompt})
                        
                        try:
                            from ..providers.llm_base import LLMMessage
                        except ImportError:
                            try:
                                from neogenesis_system.providers.llm_base import LLMMessage
                            except ImportError:
                                # å®šä¹‰åŸºæœ¬çš„æ¶ˆæ¯ç±»
                                class LLMMessage:
                                    def __init__(self, role, content):
                                        self.role = role
                                        self.content = content
                        
                        llm_messages = [LLMMessage(role=msg["role"], content=msg["content"]) for msg in messages]
                        response = self.client.chat_completion(llm_messages, **kwargs)
                        
                        if response.success:
                            return response.content
                        else:
                            raise Exception(response.error_message)
                
                self.llm_manager = SimpleGeminiManager(gemini_client)
                logger.info("âœ… ç›´æ¥åˆ›å»ºGeminiå®¢æˆ·ç«¯æˆåŠŸï¼Œç”¨äºå¿«é€Ÿä»»åŠ¡åˆ†æ")
                return
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç›´æ¥åˆ›å»ºGeminiå®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        # æ–¹å¼4ï¼šå›é€€åˆ°Ollamaï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
        default_ollama_config = {
            "model_name": "deepseek-r1:7b",  # ä½¿ç”¨å·²å®‰è£…çš„deepseek-r1:7bæ¨¡å‹è¿›è¡Œå¿«é€Ÿåˆ†ç±»
            "base_url": "http://localhost:11434",
            "temperature": 0.1,             # åˆ†ç±»ä»»åŠ¡éœ€è¦ç¡®å®šæ€§
            "max_tokens": 500,              # å¿«é€Ÿå“åº”
            "timeout": (5, 30)              # å¿«é€Ÿè¶…æ—¶
        }
        
        # åˆå¹¶ç”¨æˆ·é…ç½®
        if ollama_config:
            default_ollama_config.update(ollama_config)
        
        try:
            # åˆ›å»ºOllamaå®¢æˆ·ç«¯
            self.ollama_client = create_ollama_client(**default_ollama_config)
            logger.info(f"ğŸ¤– å›é€€åˆ°Ollamaå®¢æˆ·ç«¯: {default_ollama_config['model_name']}")
            
            # å¿«é€Ÿå¥åº·æ£€æŸ¥
            if hasattr(self.ollama_client, 'validate_config'):
                is_healthy = self.ollama_client.validate_config()
                if not is_healthy:
                    logger.warning("âš ï¸ Ollamaå®¢æˆ·ç«¯å¥åº·æ£€æŸ¥å¤±è´¥")
                    self.ollama_client = None
                else:
                    logger.debug("âœ… Ollamaå®¢æˆ·ç«¯å¥åº·æ£€æŸ¥é€šè¿‡")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ›å»ºOllamaå®¢æˆ·ç«¯å¤±è´¥: {e}")
            self.ollama_client = None
            
        # æ–¹å¼5ï¼šå¦‚æœæ‰€æœ‰LLMéƒ½ä¸å¯ç”¨ï¼Œè®°å½•è­¦å‘Š
        if not self.ollama_client and not self.llm_manager:
            try:
                self.llm_manager = LLMManager()
                logger.debug("ğŸ”§ åˆ›å»ºé»˜è®¤LLMç®¡ç†å™¨ä½œä¸ºå›é€€")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ›å»ºLLMç®¡ç†å™¨å¤±è´¥: {e}")
                raise Exception("æ— æ³•åˆå§‹åŒ–ä»»ä½•LLMèƒ½åŠ›")
    
    def _call_llm(self, prompt: str, temperature: float = 0.1, max_tokens: int = 500,
                  enable_streaming: bool = False) -> Optional[str]:
        """
        é€šç”¨LLMè°ƒç”¨æ¥å£ï¼ˆæ”¯æŒæµå¼ï¼‰
        
        Args:
            prompt: è¾“å…¥æç¤º
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            LLMå“åº”å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not self.enable_llm:
            return None
            
        try:
            # æ–¹å¼1ï¼šä¼˜å…ˆä½¿ç”¨LLMç®¡ç†å™¨ï¼ˆæ”¯æŒæµå¼ï¼‰
            if self.llm_manager:
                if enable_streaming and hasattr(self.llm_manager, 'call_api_with_streaming'):
                    response_content = self.llm_manager.call_api_with_streaming(
                        prompt=prompt,
                        temperature=temperature,
                        enable_streaming=True
                    )
                else:
                    response_content = self.llm_manager.call_api(
                        prompt=prompt,
                        temperature=temperature
                    )
                
                if response_content:
                    logger.debug(f"âœ… LLMç®¡ç†å™¨è°ƒç”¨æˆåŠŸ: {response_content[:50]}...")
                    return response_content
            
            # æ–¹å¼2ï¼šä½¿ç”¨Ollamaå®¢æˆ·ç«¯ä½œä¸ºå›é€€
            if self.ollama_client:
                messages = [LLMMessage(role="user", content=prompt)]
                response = self.ollama_client.chat_completion(
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                if response.success:
                    logger.debug(f"âœ… Ollamaè°ƒç”¨æˆåŠŸ: {response.content[:50]}...")
                    return response.content
                else:
                    logger.warning(f"âš ï¸ Ollamaè°ƒç”¨å¤±è´¥: {response.error_message}")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ LLMè°ƒç”¨å¼‚å¸¸: {e}")
            
        return None
    
    def _call_llm_streaming(self, prompt: str, temperature: float = 0.7, max_tokens: int = 800) -> Optional[str]:
        """
        æµå¼LLMè°ƒç”¨ - å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹
        
        Args:
            prompt: è¾“å…¥æç¤º
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            å®Œæ•´çš„LLMå“åº”å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not self.enable_llm:
            return None
            
        try:
            full_response = ""
            
            # æ–¹å¼1ï¼šä½¿ç”¨LLMç®¡ç†å™¨çš„æµå¼API
            if self.llm_manager and hasattr(self.llm_manager, 'call_api_streaming_generator'):
                try:
                    # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨ - é€tokenå®æ—¶è¾“å‡º
                    print("\n", end="", flush=True)  # ç¡®ä¿åœ¨æ–°è¡Œå¼€å§‹
                    for chunk in self.llm_manager.call_api_streaming_generator(
                        prompt=prompt,
                        temperature=temperature,
                        max_tokens=max_tokens
                    ):
                        if chunk:
                            print(chunk, end="", flush=True)
                            full_response += chunk
                    
                    print()  # æ¢è¡Œ
                    logger.debug(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_response)}å­—ç¬¦")
                    return full_response
                    
                except AttributeError as e:
                    # å¦‚æœæ²¡æœ‰æµå¼ç”Ÿæˆå™¨ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼ä½†é€å­—æ‰“å°
                    logger.warning(f"âš ï¸ LLMç®¡ç†å™¨ä¸æ”¯æŒæµå¼ç”Ÿæˆå™¨: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæµå¼è¾“å‡º")
                    response_content = self.llm_manager.call_api(
                        prompt=prompt,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    if response_content:
                        # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                        import time
                        print("\n", end="", flush=True)
                        for char in response_content:
                            print(char, end="", flush=True)
                            time.sleep(0.01)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                        print()  # æ¢è¡Œ
                        return response_content
                except Exception as e:
                    logger.error(f"âŒ æµå¼ç”Ÿæˆå™¨è°ƒç”¨å¤±è´¥: {e}")
                    # ç»§ç»­å°è¯•å…¶ä»–æ–¹å¼
            
            # æ–¹å¼2ï¼šä½¿ç”¨Ollamaå®¢æˆ·ç«¯ï¼ˆæ”¯æŒçœŸå®æµå¼ï¼‰
            if self.ollama_client:
                messages = [LLMMessage(role="user", content=prompt)]
                
                # æ£€æŸ¥æ˜¯å¦æ”¯æŒæµå¼
                if hasattr(self.ollama_client, 'chat_completion_stream'):
                    # çœŸå®æµå¼è¾“å‡º
                    for chunk in self.ollama_client.chat_completion_stream(
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens
                    ):
                        if chunk and hasattr(chunk, 'content'):
                            print(chunk.content, end="", flush=True)
                            full_response += chunk.content
                    
                    print()  # æ¢è¡Œ
                    logger.debug(f"âœ… Ollamaæµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_response)}å­—ç¬¦")
                    return full_response
                else:
                    # å›é€€åˆ°æ™®é€šæ¨¡å¼ + æ¨¡æ‹Ÿæµå¼
                    logger.debug("âš ï¸ Ollamaä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæµå¼è¾“å‡º")
                    response = self.ollama_client.chat_completion(
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    if response.success:
                        import time
                        for char in response.content:
                            print(char, end="", flush=True)
                            time.sleep(0.01)
                        print()  # æ¢è¡Œ
                        return response.content
                        
        except Exception as e:
            logger.warning(f"âš ï¸ æµå¼LLMè°ƒç”¨å¼‚å¸¸: {e}")
            print(f"\nâš ï¸ æµå¼è¾“å‡ºå¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼")
            # å›é€€åˆ°æ™®é€šè°ƒç”¨
            return self._call_llm(prompt, temperature, max_tokens, enable_streaming=False)
            
        return None
    
    def _call_llm_with_fallback(self, prompt: str, fallback_result: Any, **kwargs) -> Any:
        """
        å¸¦å›é€€æœºåˆ¶çš„LLMè°ƒç”¨
        
        Args:
            prompt: LLMæç¤º
            fallback_result: LLMè°ƒç”¨å¤±è´¥æ—¶çš„å›é€€ç»“æœ
            **kwargs: LLMè°ƒç”¨å‚æ•°
            
        Returns:
            LLMç»“æœæˆ–å›é€€ç»“æœ
        """
        llm_result = self._call_llm(prompt, **kwargs)
        
        if llm_result is not None:
            return llm_result
        else:
            logger.debug("ğŸ”§ LLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼å›é€€ç»“æœ")
            return fallback_result
        
    def assess_task_confidence(self, user_query: str, execution_context: Optional[Dict] = None) -> float:
        """
        è¯„ä¼°ä»»åŠ¡çš„ç½®ä¿¡åº¦ - å‡çº§ç‰ˆï¼šæ”¯æŒLLMå¢å¼ºåˆ†æ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"confidence_{user_query}_{hash(str(execution_context))}"
        if cache_key in self.assessment_cache:
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„ç½®ä¿¡åº¦è¯„ä¼°")
            return self.assessment_cache[cache_key]
        
        # è·å–å¯å‘å¼åˆ†æç»“æœï¼ˆä½œä¸ºåŸºçº¿å’Œå›é€€ï¼‰
        heuristic_confidence = self._heuristic_confidence_assessment(user_query, execution_context)
        
        # å°è¯•LLMå¢å¼ºåˆ†æ
        if self.enable_llm:
            try:
                llm_confidence = self._llm_confidence_assessment(user_query, execution_context)
                if llm_confidence is not None:
                    # æ™ºèƒ½åˆå¹¶ï¼šLLMåˆ†æä¸ºä¸»ï¼Œå¯å‘å¼åˆ†æä½œä¸ºæ ¡å‡†
                    final_confidence = self._merge_confidence_scores(heuristic_confidence, llm_confidence)
                    logger.debug(f"ğŸ§  ç½®ä¿¡åº¦è¯„ä¼° - å¯å‘å¼:{heuristic_confidence:.3f}, LLM:{llm_confidence:.3f}, åˆå¹¶:{final_confidence:.3f}")
                else:
                    final_confidence = heuristic_confidence
                    logger.debug(f"ğŸ”§ LLMç½®ä¿¡åº¦è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼ç»“æœ:{final_confidence:.3f}")
            except Exception as e:
                logger.warning(f"âš ï¸ LLMç½®ä¿¡åº¦è¯„ä¼°å¼‚å¸¸: {e}")
                final_confidence = heuristic_confidence
        else:
            final_confidence = heuristic_confidence
            logger.debug(f"ğŸ“Š å¯å‘å¼ç½®ä¿¡åº¦è¯„ä¼°:{final_confidence:.3f}")
        
        # ç¼“å­˜ç»“æœ
        self.assessment_cache[cache_key] = final_confidence
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.assessment_cache) > 100:
            oldest_key = next(iter(self.assessment_cache))
            del self.assessment_cache[oldest_key]
        
        logger.info(f"ğŸ“Š ä»»åŠ¡ç½®ä¿¡åº¦è¯„ä¼°å®Œæˆ: {final_confidence:.3f}")
        return final_confidence
    
    def _heuristic_confidence_assessment(self, user_query: str, execution_context: Optional[Dict] = None) -> float:
        """
        å¯å‘å¼ç½®ä¿¡åº¦è¯„ä¼°ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            å¯å‘å¼ç½®ä¿¡åº¦åˆ†æ•°
        """
        # åŸºäºæŸ¥è¯¢å¤æ‚åº¦çš„è¯„ä¼°
        base_confidence = 0.7
        
        # æŸ¥è¯¢é•¿åº¦å½±å“
        query_length = len(user_query)
        if query_length < 20:
            base_confidence += 0.1
        elif query_length > 100:
            base_confidence -= 0.1
        elif query_length > 200:
            base_confidence -= 0.2
            
        # æŠ€æœ¯æœ¯è¯­æ£€æµ‹
        tech_terms = [
            'API', 'api', 'ç®—æ³•', 'æ•°æ®åº“', 'ç³»ç»Ÿ', 'æ¶æ„', 'ä¼˜åŒ–',
            'æœºå™¨å­¦ä¹ ', 'ML', 'AI', 'äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ',
            'ç½‘ç»œ', 'çˆ¬è™«', 'æ•°æ®åˆ†æ', 'å®æ—¶', 'æ€§èƒ½'
        ]
        tech_count = sum(1 for term in tech_terms if term in user_query)
        if tech_count > 0:
            base_confidence += min(0.15, tech_count * 0.05)
        
        # å¤æ‚åº¦å…³é”®è¯æ£€æµ‹
        complexity_indicators = [
            'å¤æ‚', 'å›°éš¾', 'æŒ‘æˆ˜', 'é«˜çº§', 'ä¸“ä¸š',
            'å¤šæ­¥éª¤', 'åˆ†å¸ƒå¼', 'å¹¶å‘', 'å¼‚æ­¥', 'é›†æˆ'
        ]
        complexity_count = sum(1 for indicator in complexity_indicators if indicator in user_query)
        if complexity_count > 0:
            base_confidence -= min(0.2, complexity_count * 0.05)
        
        # æ˜ç¡®æ€§å…³é”®è¯æ£€æµ‹
        clarity_indicators = [
            'ç®€å•', 'ç›´æ¥', 'åŸºç¡€', 'å¿«é€Ÿ', 'æ ‡å‡†',
            'å¸®åŠ©', 'è¯·', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä»€ä¹ˆ'
        ]
        clarity_count = sum(1 for indicator in clarity_indicators if indicator in user_query)
        if clarity_count > 0:
            base_confidence += min(0.1, clarity_count * 0.03)
        
        # æ‰§è¡Œä¸Šä¸‹æ–‡å½±å“
        if execution_context:
            context_factors = len(execution_context)
            if context_factors > 3:
                base_confidence += 0.05  # æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜ç½®ä¿¡åº¦
            
            # å®æ—¶æ€§è¦æ±‚
            if execution_context.get('real_time_requirements'):
                base_confidence -= 0.05
            
            # æ€§èƒ½è¦æ±‚
            if execution_context.get('performance_critical'):
                base_confidence -= 0.03
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        return min(1.0, max(0.2, base_confidence))
    
    def _llm_confidence_assessment(self, user_query: str, execution_context: Optional[Dict] = None) -> Optional[float]:
        """
        LLMå¢å¼ºçš„ç½®ä¿¡åº¦è¯„ä¼°
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            LLMè¯„ä¼°çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        context_info = ""
        if execution_context:
            context_info = f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{json.dumps(execution_context, ensure_ascii=False, indent=2)}"
            
        prompt = f"""
è¯·åˆ†æä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ï¼Œè¯„ä¼°æˆ‘ä»¬èƒ½æˆåŠŸå®Œæˆè¿™ä¸ªä»»åŠ¡çš„ç½®ä¿¡åº¦ã€‚

ç”¨æˆ·æŸ¥è¯¢: "{user_query}"{context_info}

è¯·ä»ä»¥ä¸‹ç»´åº¦åˆ†æ:
1. ä»»åŠ¡æ¸…æ™°åº¦ - éœ€æ±‚æ˜¯å¦æ˜ç¡®
2. æŠ€æœ¯å¯è¡Œæ€§ - æŠ€æœ¯å®ç°éš¾åº¦
3. èµ„æºéœ€æ±‚ - æ‰€éœ€èµ„æºæ˜¯å¦åˆç†
4. å¤æ‚åº¦è¯„ä¼° - ä»»åŠ¡å¤æ‚ç¨‹åº¦
5. é£é™©å› ç´  - æ½œåœ¨çš„å¤±è´¥é£é™©

è¯·è¿”å›ä¸€ä¸ª0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚

æ ¼å¼:
ç½®ä¿¡åº¦: 0.85
ç†ç”±: ä»»åŠ¡éœ€æ±‚æ˜ç¡®ï¼ŒæŠ€æœ¯å¯è¡Œæ€§é«˜ï¼Œå¤æ‚åº¦é€‚ä¸­
"""
        
        llm_response = self._call_llm(prompt, temperature=0.1, max_tokens=300)
        if llm_response is None:
            return None
            
        # è§£æLLMå“åº”ï¼Œæå–ç½®ä¿¡åº¦åˆ†æ•°
        try:
            # ç®€å•çš„æ­£åˆ™è§£æ
            import re
            confidence_match = re.search(r'ç½®ä¿¡åº¦[ï¼š:]\s*([0-9.]+)', llm_response)
            if confidence_match:
                confidence_score = float(confidence_match.group(1))
                # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
                confidence_score = min(1.0, max(0.0, confidence_score))
                
                logger.debug(f"ğŸ¤– LLMç½®ä¿¡åº¦åˆ†æ: {confidence_score} - {llm_response[:50]}...")
                return confidence_score
            else:
                logger.warning(f"âš ï¸ æ— æ³•ä»LLMå“åº”ä¸­è§£æç½®ä¿¡åº¦: {llm_response[:100]}")
                return None
                
        except Exception as e:
            logger.warning(f"âš ï¸ LLMç½®ä¿¡åº¦å“åº”è§£æå¤±è´¥: {e}")
            return None
    
    def _merge_confidence_scores(self, heuristic_score: float, llm_score: float) -> float:
        """
        æ™ºèƒ½åˆå¹¶å¯å‘å¼å’ŒLLMç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            heuristic_score: å¯å‘å¼åˆ†æåˆ†æ•°
            llm_score: LLMåˆ†æåˆ†æ•°
            
        Returns:
            åˆå¹¶åçš„ç½®ä¿¡åº¦åˆ†æ•°
        """
        # åŠ æƒå¹³å‡ï¼šLLMæƒé‡æ›´é«˜ï¼ˆ0.7ï¼‰ï¼Œå¯å‘å¼ä½œä¸ºæ ¡å‡†ï¼ˆ0.3ï¼‰
        merged_score = llm_score * 0.7 + heuristic_score * 0.3
        
        # å¦‚æœä¸¤ä¸ªåˆ†æ•°å·®å¼‚å¾ˆå¤§ï¼Œé™ä½ç½®ä¿¡åº¦ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        score_diff = abs(llm_score - heuristic_score)
        if score_diff > 0.3:
            # å·®å¼‚æƒ©ç½š
            penalty = min(0.15, score_diff * 0.2)
            merged_score -= penalty
            logger.debug(f"ğŸ” ç½®ä¿¡åº¦åˆ†æ•°å·®å¼‚è¾ƒå¤§({score_diff:.3f})ï¼Œåº”ç”¨æƒ©ç½š:{penalty:.3f}")
        
        return min(1.0, max(0.1, merged_score))

    # ==================== æ ¸å¿ƒ LLM è·¯ç”±åˆ†æå¼•æ“ ====================

    def _llm_route_analysis(self, user_query: str, execution_context: Optional[Dict] = None) -> Optional[TriageClassification]:
        """
        æ ¸å¿ƒ LLM è·¯ç”±åˆ†ææ–¹æ³• - å–ä»£ä¼ ç»Ÿå…³é”®è¯åŒ¹é…
        
        ä½¿ç”¨æœ¬åœ° LLM è¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æï¼Œæä¾›ç²¾ç¡®çš„ä»»åŠ¡åˆ†ç±»å’Œè·¯ç”±å†³ç­–
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            LLM åˆ†æç»“æœï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        if not self.enable_llm:
            logger.debug("ğŸ”§ LLM æœªå¯ç”¨ï¼Œè·³è¿‡ LLM è·¯ç”±åˆ†æ")
            return None
            
        logger.info(f"ğŸ§  å¯åŠ¨æ ¸å¿ƒ LLM è·¯ç”±åˆ†æ: {user_query[:50]}...")
        
        try:
            # æ„å»ºä¸“é—¨çš„è·¯ç”±åˆ†ææç¤º
            route_prompt = self._build_route_analysis_prompt(user_query, execution_context)
            
            # è°ƒç”¨æœ¬åœ° LLM è¿›è¡Œåˆ†æ
            llm_response = self._call_llm(route_prompt, temperature=0.1, max_tokens=1000)
            
            if llm_response is None:
                logger.warning("âš ï¸ LLM è·¯ç”±åˆ†æè°ƒç”¨å¤±è´¥")
                return None
            
            # è§£æ LLM çš„å†³ç­–ç»“æœ
            classification = self._parse_llm_route_decision(llm_response, user_query)
            
            if classification:
                logger.info(f"âœ… LLM è·¯ç”±åˆ†ææˆåŠŸ: {classification.domain.value} -> {classification.route_strategy.value}")
                return classification
            else:
                logger.warning("âš ï¸ LLM è·¯ç”±å†³ç­–è§£æå¤±è´¥")
                return None
                
        except Exception as e:
            logger.error(f"âŒ LLM è·¯ç”±åˆ†æå¼‚å¸¸: {e}")
            return None

    def _build_route_analysis_prompt(self, user_query: str, execution_context: Optional[Dict] = None) -> str:
        """
        æ„å»ºä¸“é—¨çš„ LLM è·¯ç”±åˆ†ææç¤º
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            ä¼˜åŒ–çš„è·¯ç”±åˆ†ææç¤º
        """
        # ä¸Šä¸‹æ–‡ä¿¡æ¯å¤„ç†
        context_section = ""
        if execution_context:
            context_section = f"\n\n**æ‰§è¡Œä¸Šä¸‹æ–‡:**\n{json.dumps(execution_context, ensure_ascii=False, indent=2)}"
        
        # æ„å»ºä¸“é—¨çš„è·¯ç”±åˆ†ææç¤º
        route_prompt = f"""# ğŸš¦ ä»»åŠ¡è·¯ç”±åˆ†è¯Šç³»ç»Ÿ

## âš ï¸ è§’è‰²çº¦æŸ - æå…¶é‡è¦ï¼

**ä½ æ˜¯ä¸€ä¸ªçº¯ç²¹çš„"ä»»åŠ¡åˆ†è¯Šå‘˜"ï¼Œåªè´Ÿè´£ä»»åŠ¡åˆ†ç±»å’Œè·¯ç”±å†³ç­–ã€‚**

ğŸš« **ä¸¥ç¦è¡Œä¸ºï¼š**
- ç»å¯¹ä¸èƒ½å›ç­”ç”¨æˆ·çš„é—®é¢˜å†…å®¹
- ä¸èƒ½æä¾›ä»»ä½•æŠ€æœ¯è§£å†³æ–¹æ¡ˆ
- ä¸èƒ½ç»™å‡ºä»»ä½•å»ºè®®æˆ–æŒ‡å¯¼  
- ä¸èƒ½è§£é‡Šæ¦‚å¿µæˆ–æŠ€æœ¯åŸç†
- ä¸èƒ½è¿›è¡Œä»»ä½•å½¢å¼çš„é—®ç­”

âœ… **å”¯ä¸€èŒè´£ï¼š**
- ä»…å¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡ŒæŠ€æœ¯ç»´åº¦åˆ†ç±»
- ä»…åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦å’Œå¤„ç†ç­–ç•¥
- ä»…è¾“å‡ºæ ‡å‡†åŒ–çš„è·¯ç”±å†³ç­–JSON

## ğŸ“‹ å¾…åˆ†ææŸ¥è¯¢

**ç”¨æˆ·è¾“å…¥:** "{user_query}"{context_section}

## ğŸ¯ åˆ†è¯Šç»´åº¦ï¼ˆä»…åˆ†ç±»ï¼Œä¸è§£ç­”ï¼‰

### å¤æ‚åº¦åˆ†è¯Š
- **simple**: é—®å€™è¯­ã€å•ä¸€æ¦‚å¿µæŸ¥è¯¢ï¼ˆå¦‚"ä½ å¥½"ã€"ä»€ä¹ˆæ˜¯HTTP"ï¼‰
- **moderate**: éœ€è¦å¤šæ­¥éª¤åˆ†æçš„æŠ€æœ¯é—®é¢˜
- **complex**: ç³»ç»Ÿè®¾è®¡æˆ–æ¶æ„çº§é—®é¢˜
- **expert**: éœ€è¦æ·±åº¦ä¸“ä¸šçŸ¥è¯†çš„é«˜éš¾åº¦é—®é¢˜

### æŠ€æœ¯é¢†åŸŸåˆ†è¯Š
- **web_development**: Webç›¸å…³æŠ€æœ¯
- **data_science**: æ•°æ®åˆ†æ/MLç›¸å…³
- **system_admin**: ç³»ç»Ÿè¿ç»´ç›¸å…³
- **database**: æ•°æ®åº“ç›¸å…³
- **general**: é€šç”¨æˆ–è·¨é¢†åŸŸ

### æ„å›¾åˆ†è¯Š
- **question**: çŸ¥è¯†æŸ¥è¯¢ç±»ï¼ˆå¦‚"ä»€ä¹ˆæ˜¯..."ã€"å¦‚ä½•..."ï¼‰
- **task_execution**: å…·ä½“ä»»åŠ¡æ‰§è¡Œ
- **debugging**: é—®é¢˜æ’æŸ¥ä¿®å¤

### ç´§æ€¥åº¦åˆ†è¯Š
- **low**: å­¦ä¹ æ€§æŸ¥è¯¢
- **medium**: å¸¸è§„å·¥ä½œä»»åŠ¡
- **high**: é‡è¦é¡¹ç›®éœ€æ±‚
- **critical**: ç”Ÿäº§ç¯å¢ƒç´§æ€¥æƒ…å†µ

### å¤„ç†ç­–ç•¥åˆ†è¯Š
- **direct_response**: ç®€å•é—®é¢˜ï¼Œå¿«é€Ÿé€šé“å¤„ç†
- **multi_stage_processing**: ä¸­ç­‰å¤æ‚åº¦ï¼Œæ ‡å‡†æµç¨‹å¤„ç†
- **workflow_planning**: å¤æ‚ä»»åŠ¡ï¼Œéœ€è¦è¯¦ç»†è§„åˆ’
- **expert_consultation**: é«˜éš¾åº¦ï¼Œéœ€è¦ä¸“å®¶çº§å¤„ç†

## ğŸ“„ è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼Œæ— å…¶ä»–å†…å®¹ï¼‰

```json
{{
  "complexity": "simple|moderate|complex|expert",
  "domain": "é¢†åŸŸä»£ç ",
  "intent": "æ„å›¾ä»£ç ", 
  "urgency": "ç´§æ€¥åº¦ä»£ç ",
  "route_strategy": "å¤„ç†ç­–ç•¥ä»£ç ",
  "confidence": 0.85,
  "reasoning": "åˆ†è¯Šç†ç”±ï¼ˆ50å­—å†…ï¼Œä»…è¯´æ˜åˆ†ç±»ä¾æ®ï¼‰",
  "key_factors": ["æŠ€æœ¯ç»´åº¦1", "æŠ€æœ¯ç»´åº¦2"],
  "estimated_time": é¢„ä¼°åˆ†é’Ÿæ•°,
  "required_resources": ["èµ„æºç±»å‹1", "èµ„æºç±»å‹2"]
}}
```

âš ï¸ **å†æ¬¡å¼ºè°ƒï¼šä½ åªæ˜¯ä¸€ä¸ªåˆ†è¯Šå‘˜ï¼Œç»å¯¹ä¸èƒ½å›ç­”é—®é¢˜å†…å®¹ï¼**"""
        
        return route_prompt

    def _parse_llm_route_decision(self, llm_response: str, user_query: str) -> Optional[TriageClassification]:
        """
        è§£æ LLM çš„è·¯ç”±å†³ç­–ç»“æœ
        
        Args:
            llm_response: LLM çš„åŸå§‹å“åº”
            user_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            è§£æåçš„åˆ†ç±»ç»“æœï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        try:
            # å¤šç§æ–¹å¼æå–JSON
            import re
            
            # æ–¹å¼1ï¼šæå–```jsonå—ä¸­çš„å†…å®¹
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', llm_response, re.DOTALL)
            
            if not json_match:
                # æ–¹å¼2ï¼šæå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                json_match = re.search(r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})', llm_response, re.DOTALL)
            
            if not json_match:
                # æ–¹å¼3ï¼šå°è¯•æ›´å®½æ¾çš„åŒ¹é…
                json_match = re.search(r'(\{.*\})', llm_response, re.DOTALL)
            
            if not json_match:
                logger.warning(f"âš ï¸ æ— æ³•ä» LLM å“åº”ä¸­æå– JSON å†³ç­–ç»“æœ")
                return None
            
            # è§£æJSONå†³ç­–æ•°æ®
            decision_data = json.loads(json_match.group(1))
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['complexity', 'domain', 'intent', 'urgency', 'route_strategy']
            missing_fields = [field for field in required_fields if field not in decision_data]
            
            if missing_fields:
                logger.warning(f"âš ï¸ LLM å†³ç­–ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
                return None
            
            # æ„å»ºåˆ†ç±»ç»“æœ
            classification = TriageClassification(
                complexity=TaskComplexity(decision_data['complexity']),
                domain=TaskDomain(decision_data['domain']),
                intent=TaskIntent(decision_data['intent']),
                urgency=TaskUrgency(decision_data['urgency']),
                route_strategy=RouteStrategy(decision_data['route_strategy']),
                confidence=float(decision_data.get('confidence', 0.8)),
                reasoning=decision_data.get('reasoning', 'LLM è·¯ç”±åˆ†æç»“æœ'),
                key_factors=decision_data.get('key_factors', []),
                estimated_time=decision_data.get('estimated_time'),
                required_resources=decision_data.get('required_resources', [])
            )
            
            logger.debug(f"âœ… LLM è·¯ç”±å†³ç­–è§£ææˆåŠŸ: {classification.complexity.value}/{classification.domain.value}")
            return classification
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ LLM è·¯ç”±å†³ç­– JSON è§£æå¤±è´¥: {e}")
            return None
        except (ValueError, KeyError) as e:
            logger.warning(f"âš ï¸ LLM è·¯ç”±å†³ç­–æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ LLM è·¯ç”±å†³ç­–å¤„ç†å¼‚å¸¸: {e}")
            return None

    def _fallback_keyword_analysis(self, user_query: str, execution_context: Optional[Dict] = None) -> TriageClassification:
        """
        å›é€€å…³é”®è¯åˆ†ææ–¹æ³• - å°è£…ä¼ ç»Ÿçš„å…³é”®è¯åŒ¹é…é€»è¾‘
        
        å½“ LLM åˆ†æå¤±è´¥æ—¶ä½¿ç”¨æ­¤æ–¹æ³•ä½œä¸ºå¯é å›é€€
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            åŸºäºå…³é”®è¯åŒ¹é…çš„åˆ†ç±»ç»“æœ
        """
        logger.info(f"ğŸ”§ å¯åŠ¨å›é€€å…³é”®è¯åˆ†æ: {user_query[:50]}...")
        
        # ä½¿ç”¨ä¼ ç»Ÿçš„å¤æ‚åº¦åˆ†æ
        complexity_analysis = self.analyze_task_complexity(user_query)
        confidence_score = self._heuristic_confidence_assessment(user_query, execution_context)
        
        # å¤æ‚åº¦åˆ°åˆ†ç±»çš„æ˜ å°„
        complexity_score = complexity_analysis.get('complexity_score', 0.5)
        
        if complexity_score < 0.3:
            complexity = TaskComplexity.SIMPLE
            route_strategy = RouteStrategy.DIRECT_RESPONSE
        elif complexity_score < 0.6:
            complexity = TaskComplexity.MODERATE
            route_strategy = RouteStrategy.MULTI_STAGE_PROCESSING
        elif complexity_score < 0.8:
            complexity = TaskComplexity.COMPLEX
            route_strategy = RouteStrategy.WORKFLOW_PLANNING
        else:
            complexity = TaskComplexity.EXPERT
            route_strategy = RouteStrategy.EXPERT_CONSULTATION
        
        # ä¼ ç»Ÿé¢†åŸŸæ¨æ–­
        estimated_domain = complexity_analysis.get('estimated_domain', 'general')
        domain_mapping = {
            'web_development': TaskDomain.WEB_DEV,
            'data_science': TaskDomain.DATA_SCIENCE,
            'api_development': TaskDomain.API_DEV,
            'database': TaskDomain.DATABASE,
            'system_admin': TaskDomain.SYSTEM_ADMIN,
            'security': TaskDomain.SECURITY,
            'mobile_development': TaskDomain.MOBILE_DEV,
            'performance': TaskDomain.SYSTEM_ADMIN,
            'automation': TaskDomain.SYSTEM_ADMIN,
        }
        domain = domain_mapping.get(estimated_domain, TaskDomain.GENERAL)
        
        # ä¼ ç»Ÿæ„å›¾åˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
        intent = self._keyword_intent_analysis(user_query)
        
        # ä¼ ç»Ÿç´§æ€¥åº¦è¯„ä¼°ï¼ˆåŸºäºå…³é”®è¯ï¼‰
        urgency = self._keyword_urgency_analysis(user_query)
        
        # æ„å»ºå›é€€åˆ†ç±»ç»“æœ
        fallback_classification = TriageClassification(
            complexity=complexity,
            domain=domain,
            intent=intent,
            urgency=urgency,
            route_strategy=route_strategy,
            confidence=confidence_score * 0.8,  # å…³é”®è¯åˆ†æç½®ä¿¡åº¦ç•¥ä½
            reasoning=f"å…³é”®è¯åˆ†æï¼šå¤æ‚åº¦{complexity_score:.2f}ï¼Œé¢†åŸŸ{estimated_domain}",
            key_factors=list(complexity_analysis.get('complexity_factors', {}).keys())[:3],
            estimated_time=self._estimate_processing_time(complexity, domain),
            required_resources=self._estimate_required_resources(domain, complexity)
        )
        
        logger.info(f"âœ… å…³é”®è¯åˆ†æå®Œæˆ: {domain.value} -> {route_strategy.value}")
        return fallback_classification

    def _keyword_intent_analysis(self, user_query: str) -> TaskIntent:
        """åŸºäºå…³é”®è¯çš„æ„å›¾åˆ†æ"""
        query_lower = user_query.lower()
        
        # é—®é¢˜å’¨è¯¢å…³é”®è¯
        if any(word in query_lower for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'è¯·æ•™', 'å­¦ä¹ ', 'è§£é‡Š', 'ä»‹ç»']):
            return TaskIntent.QUESTION
        
        # åˆ›å»ºå¼€å‘å…³é”®è¯
        elif any(word in query_lower for word in ['åˆ›å»º', 'ç”Ÿæˆ', 'è®¾è®¡', 'å¼€å‘', 'å†™', 'æ„å»º', 'å®ç°']):
            return TaskIntent.CREATION
        
        # åˆ†æè¯„ä¼°å…³é”®è¯
        elif any(word in query_lower for word in ['åˆ†æ', 'è¯„ä¼°', 'æ¯”è¾ƒ', 'æ£€æŸ¥', 'å®¡æŸ¥', 'æµ‹è¯•']):
            return TaskIntent.ANALYSIS
        
        # è°ƒè¯•ä¿®å¤å…³é”®è¯
        elif any(word in query_lower for word in ['ä¿®å¤', 'è§£å†³', 'è°ƒè¯•', 'é”™è¯¯', 'é—®é¢˜', 'æ•…éšœ', 'å¼‚å¸¸']):
            return TaskIntent.DEBUGGING
        
        # ä¼˜åŒ–æ”¹è¿›å…³é”®è¯
        elif any(word in query_lower for word in ['ä¼˜åŒ–', 'æå‡', 'æ”¹è¿›', 'åŠ é€Ÿ', 'æ€§èƒ½', 'æ•ˆç‡']):
            return TaskIntent.OPTIMIZATION
        
        # é»˜è®¤ä¸ºä»»åŠ¡æ‰§è¡Œ
        else:
            return TaskIntent.TASK_EXECUTION

    def _keyword_urgency_analysis(self, user_query: str) -> TaskUrgency:
        """åŸºäºå…³é”®è¯çš„ç´§æ€¥åº¦åˆ†æ"""
        query_lower = user_query.lower()
        
        # ç´§æ€¥å…³é”®è¯
        if any(word in query_lower for word in ['ç´§æ€¥', 'æ€¥', 'é©¬ä¸Š', 'ç«‹å³', 'ç°åœ¨', 'critical', 'urgent', 'asap']):
            return TaskUrgency.CRITICAL
        
        # é‡è¦å…³é”®è¯
        elif any(word in query_lower for word in ['é‡è¦', 'å°½å¿«', 'ä¼˜å…ˆ', 'important', 'priority', 'high']):
            return TaskUrgency.HIGH
        
        # å­¦ä¹ æ¢ç´¢å…³é”®è¯
        elif any(word in query_lower for word in ['å­¦ä¹ ', 'äº†è§£', 'æ¢ç´¢', 'ç ”ç©¶', 'è¯•è¯•', 'çœ‹çœ‹']):
            return TaskUrgency.LOW
        
        # é»˜è®¤ä¸ºä¸­ç­‰ä¼˜å…ˆçº§
        else:
            return TaskUrgency.MEDIUM

    # ==================== è·¯ç”±ä¸“å®¶åŠŸèƒ½å®ç° ====================

    def classify_and_route(self, user_query: str, execution_context: Optional[Dict] = None) -> TriageClassification:
        """
        æ ¸å¿ƒè·¯ç”±åŠŸèƒ½ - å‡çº§ç‰ˆæ™ºèƒ½ä»»åŠ¡åˆ†è¯Š
        
        æ–°æ¶æ„ï¼šLLM æ ¸å¿ƒåˆ†æ + å…³é”®è¯å›é€€æœºåˆ¶
        - ä¸»è¦ï¼šä½¿ç”¨æœ¬åœ° LLM è¿›è¡Œæ·±åº¦è¯­ä¹‰ç†è§£å’Œè·¯ç”±å†³ç­–
        - å›é€€ï¼šå½“ LLM åˆ†æå¤±è´¥æ—¶ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºå¯é å›é€€
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            TriageClassification: å®Œæ•´çš„æ™ºèƒ½åˆ†ç±»ç»“æœ
        """
        # æ³¨æ„ï¼šè¿™ä¸ªæ—¥å¿—è¢«ç®€åŒ–ï¼Œè¯¦ç»†æµç¨‹æ—¥å¿—ç”±Agenté…ç½®æ§åˆ¶
        # logger.info(f"ğŸš€ å¯åŠ¨æ™ºèƒ½è·¯ç”±åˆ†æ: {user_query[:50]}...")
        
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šLLM æ ¸å¿ƒè·¯ç”±åˆ†æ
        llm_result = self._llm_route_analysis(user_query, execution_context)
        if llm_result:
            # logger.info(f"âœ… LLM è·¯ç”±æˆåŠŸ: {llm_result.domain.value} -> {llm_result.route_strategy.value} (ç½®ä¿¡åº¦: {llm_result.confidence:.2f})")
            return llm_result
        
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šå…³é”®è¯å›é€€åˆ†æ
        # logger.info("ğŸ”§ LLM åˆ†æä¸å¯ç”¨ï¼Œå¯åŠ¨å…³é”®è¯å›é€€åˆ†æ")
        fallback_result = self._fallback_keyword_analysis(user_query, execution_context)
        
        # logger.info(f"ğŸ“Š å›é€€åˆ†æå®Œæˆ: {fallback_result.domain.value} -> {fallback_result.route_strategy.value} (ç½®ä¿¡åº¦: {fallback_result.confidence:.2f})")
        return fallback_result





    def _estimate_processing_time(self, complexity: TaskComplexity, domain: TaskDomain) -> int:
        """ä¼°ç®—å¤„ç†æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
        base_time = {
            TaskComplexity.SIMPLE: 5,
            TaskComplexity.MODERATE: 15,
            TaskComplexity.COMPLEX: 45,
            TaskComplexity.EXPERT: 120
        }
        
        domain_multiplier = {
            TaskDomain.GENERAL: 1.0,
            TaskDomain.WEB_DEV: 1.2,
            TaskDomain.DATA_SCIENCE: 1.5,
            TaskDomain.ML_AI: 1.8,
            TaskDomain.SYSTEM_ADMIN: 1.3,
            TaskDomain.SECURITY: 1.4
        }
        
        base = base_time.get(complexity, 30)
        multiplier = domain_multiplier.get(domain, 1.0)
        return int(base * multiplier)

    def _estimate_required_resources(self, domain: TaskDomain, complexity: TaskComplexity) -> List[str]:
        """ä¼°ç®—æ‰€éœ€èµ„æº"""
        resources = []
        
        # åŸºç¡€èµ„æº
        if complexity in [TaskComplexity.COMPLEX, TaskComplexity.EXPERT]:
            resources.extend(["ä¸“ä¸šçŸ¥è¯†åº“", "æŠ€æœ¯æ–‡æ¡£"])
        
        # é¢†åŸŸç‰¹å®šèµ„æº
        domain_resources = {
            TaskDomain.WEB_DEV: ["å‰ç«¯æ¡†æ¶", "åç«¯æŠ€æœ¯æ ˆ"],
            TaskDomain.DATA_SCIENCE: ["æ•°æ®é›†", "åˆ†æå·¥å…·"],
            TaskDomain.ML_AI: ["è®¡ç®—èµ„æº", "æ¨¡å‹åº“"],
            TaskDomain.DATABASE: ["æ•°æ®åº“å®ä¾‹", "æŸ¥è¯¢å·¥å…·"],
            TaskDomain.SYSTEM_ADMIN: ["ç³»ç»Ÿæƒé™", "ç›‘æ§å·¥å…·"],
            TaskDomain.SECURITY: ["å®‰å…¨å·¥å…·", "å¨èƒæƒ…æŠ¥"]
        }
        
        resources.extend(domain_resources.get(domain, []))
        return resources[:4]  # é™åˆ¶æœ€å¤š4ä¸ªèµ„æº
    
    def get_thinking_seed(self, user_query: str, execution_context: Optional[Dict] = None,
                         enable_streaming: bool = False) -> str:
        """
        ç”Ÿæˆæ€ç»´ç§å­ - LLMå¢å¼ºç‰ˆï¼ˆä¿ç•™å¯å‘å¼å›é€€ï¼‰
        
        æ–°ç­–ç•¥ï¼š
        1. ä½¿ç”¨å¯å‘å¼åˆ†ææå–å…³é”®ç‰¹å¾ï¼ˆå¿«é€Ÿä¸”å…è´¹ï¼‰
        2. åŸºäºç‰¹å¾æ„å»ºä¸“ä¸šæç¤ºè¯å¹¶è°ƒç”¨LLMç”Ÿæˆé«˜è´¨é‡ç§å­
        3. LLMå¤±è´¥æ—¶å›é€€åˆ°æ”¹è¿›çš„å¯å‘å¼ç”Ÿæˆ
        4. æœ€ç»ˆå›é€€åˆ°ç´§æ€¥ç§å­ç”Ÿæˆ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            é«˜è´¨é‡çš„æ€ç»´ç§å­
        """
        logger.info(f"ğŸŒ± å¯åŠ¨LLMå¢å¼ºæ€ç»´ç§å­ç”Ÿæˆ: {user_query[:30]}...")
        
        try:
            # ========== ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿå¯å‘å¼åˆ†æï¼ˆæä¾›ç»“æ„åŒ–è¾“å…¥ï¼‰ ==========
            analysis = self.get_quick_analysis_summary(user_query, execution_context)
            confidence = analysis['confidence_score']
            complexity = analysis['complexity_score']
            domain = analysis['domain']
            key_factors = analysis.get('key_factors', [])
            
            logger.debug(f"ğŸ“Š å¯å‘å¼åˆ†æå®Œæˆ: {domain}é¢†åŸŸ, å¤æ‚åº¦{complexity:.2f}, ç½®ä¿¡åº¦{confidence:.2f}")
            
            # ========== ç¬¬äºŒæ­¥ï¼šå°è¯•LLMå¢å¼ºç”Ÿæˆ ==========
            if self.enable_llm:
                llm_seed = self._generate_llm_enhanced_seed(
                    user_query=user_query,
                    analysis_context={
                        'domain': domain,
                        'complexity_score': complexity,
                        'confidence_score': confidence,
                        'key_factors': key_factors,
                        'requires_multi_step': analysis.get('requires_multi_step', False)
                    },
                    execution_context=execution_context,
                    enable_streaming=enable_streaming
                )
                
                if llm_seed:
                    logger.info(f"âœ… LLMå¢å¼ºç§å­ç”ŸæˆæˆåŠŸ (é•¿åº¦: {len(llm_seed)}å­—ç¬¦)")
                    return llm_seed
            
            # ========== ç¬¬ä¸‰æ­¥ï¼šå›é€€åˆ°æ”¹è¿›çš„å¯å‘å¼ç”Ÿæˆ ==========
            logger.info("ğŸ”§ LLMä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„å¯å‘å¼ç§å­ç”Ÿæˆ")
            return self._generate_heuristic_seed(user_query, analysis, execution_context)
            
        except Exception as e:
            logger.error(f"âŒ æ€ç»´ç§å­ç”Ÿæˆå¼‚å¸¸: {e}")
            # æœ€ç»ˆå›é€€
            return self._generate_emergency_fallback_seed(user_query)
    
    def _generate_llm_enhanced_seed(self, user_query: str, analysis_context: Dict,
                                    execution_context: Optional[Dict] = None,
                                    enable_streaming: bool = False) -> Optional[str]:
        """
        LLMå¢å¼ºçš„æ€ç»´ç§å­ç”Ÿæˆï¼ˆæ ¸å¿ƒæ–°æ–¹æ³•ï¼‰
        
        ä½¿ç”¨é«˜è´¨é‡æç¤ºè¯è°ƒç”¨LLMï¼Œç»“åˆå¯å‘å¼åˆ†æçš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œç”Ÿæˆä¸“ä¸šçš„æ€ç»´ç§å­ã€‚
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            analysis_context: å¯å‘å¼åˆ†ææä¾›çš„ä¸Šä¸‹æ–‡ï¼ˆé¢†åŸŸã€å¤æ‚åº¦ã€ç½®ä¿¡åº¦ç­‰ï¼‰
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            enable_streaming: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            LLMç”Ÿæˆçš„é«˜è´¨é‡æ€ç»´ç§å­ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = ""
            if execution_context:
                context_items = []
                if execution_context.get('real_time_requirements'):
                    context_items.append("âš¡ éœ€è¦å…³æ³¨å®æ—¶æ€§")
                if execution_context.get('performance_critical'):
                    context_items.append("ğŸš€ æ€§èƒ½ä¼˜åŒ–æ˜¯å…³é”®")
                if execution_context.get('domain_specific'):
                    context_items.append(f"ğŸ¯ ä¸“ä¸šé¢†åŸŸ: {execution_context.get('domain_specific')}")
                
                if context_items:
                    context_info = "\n\n**æ‰§è¡Œè¦æ±‚**:\n" + "\n".join(f"- {item}" for item in context_items)
            
            # æ„å»ºåˆ†æèƒŒæ™¯
            analysis_background = f"""
**ä»»åŠ¡ç‰¹å¾åˆ†æ**ï¼ˆåŸºäºå¿«é€Ÿè¯„ä¼°ï¼‰:
- é¢†åŸŸè¯†åˆ«: {analysis_context['domain']}
- å¤æ‚åº¦è¯„åˆ†: {analysis_context['complexity_score']:.2f} (0-1é‡çº§)
- ç½®ä¿¡åº¦è¯„åˆ†: {analysis_context['confidence_score']:.2f} (0-1é‡çº§)
- å…³é”®å› ç´ : {', '.join(analysis_context['key_factors'][:3]) if analysis_context['key_factors'] else 'é€šç”¨åˆ†æ'}
- å¤šæ­¥éª¤éœ€æ±‚: {'æ˜¯' if analysis_context.get('requires_multi_step') else 'å¦'}
"""
            
            # ğŸ¯ æ ¸å¿ƒæç¤ºè¯ï¼šé«˜è´¨é‡ã€ç»“æ„åŒ–ã€ä¸“ä¸šåŒ–ã€å»æ¨¡æ¿åŒ–
            llm_prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é—®é¢˜åˆ†æä¸“å®¶ï¼Œæ“…é•¿å¿«é€Ÿç†è§£é—®é¢˜æœ¬è´¨å¹¶ç”Ÿæˆé«˜è´¨é‡çš„åˆ†ææ€è·¯ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{user_query}

{analysis_background}
{context_info}

ã€ä»»åŠ¡è¦æ±‚ã€‘
è¯·ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„"æ€ç»´ç§å­"ï¼ˆåˆæ­¥åˆ†ææ€è·¯ï¼‰ï¼Œè¦æ±‚ï¼š

1. **æ·±åº¦ç†è§£**ï¼š
   - å‡†ç¡®è¯†åˆ«é—®é¢˜çš„æ ¸å¿ƒæ„å›¾å’Œå…³é”®éœ€æ±‚ï¼ˆç”¨æˆ·çœŸæ­£æƒ³è§£å†³ä»€ä¹ˆï¼Ÿï¼‰
   - åˆ†æé—®é¢˜æ‰€å±çš„æŠ€æœ¯æˆ–çŸ¥è¯†é¢†åŸŸï¼ˆæ¶‰åŠå“ªäº›ä¸“ä¸šé¢†åŸŸï¼Ÿï¼‰
   - åˆ¤æ–­é—®é¢˜çš„å¤æ‚åº¦å’Œè§£å†³éš¾åº¦ï¼ˆéœ€è¦ä»€ä¹ˆçº§åˆ«çš„ä¸“ä¸šçŸ¥è¯†ï¼Ÿï¼‰
   - ç†è§£é—®é¢˜çš„åº”ç”¨åœºæ™¯å’Œå®é™…èƒŒæ™¯ï¼ˆåœ¨ä»€ä¹ˆæƒ…å¢ƒä¸‹ä½¿ç”¨ï¼Ÿï¼‰
   - è¯†åˆ«éšå«çš„çº¦æŸæ¡ä»¶å’Œå‰ç½®ä¾èµ–ï¼ˆæœ‰ä»€ä¹ˆé™åˆ¶ï¼Ÿéœ€è¦ä»€ä¹ˆåŸºç¡€ï¼Ÿï¼‰
   - è¯„ä¼°é—®é¢˜çš„æ—¶æ•ˆæ€§å’Œç´§æ€¥ç¨‹åº¦ï¼ˆæ˜¯å¦éœ€è¦æœ€æ–°ä¿¡æ¯ï¼Ÿæ˜¯å¦ç´§æ€¥ï¼Ÿï¼‰

2. **æ€è·¯è§„åˆ’**ï¼š
   - æå‡º2-3ä¸ªå¯èƒ½çš„è§£å†³æ–¹å‘æˆ–åˆ†æè§’åº¦
   - æŒ‡å‡ºéœ€è¦è€ƒè™‘çš„å…³é”®å› ç´ å’Œæ½œåœ¨æŒ‘æˆ˜
   - å¦‚æœæ˜¯å¤šæ­¥éª¤ä»»åŠ¡ï¼Œç®€è¦è¯´æ˜ä¸»è¦é˜¶æ®µ

3. **è¾“å‡ºè¦æ±‚**ï¼š
   - **å¼€å¤´å¿…é¡»**ï¼šç”¨ä¸€å¥è¯ç®€æ´å¤è¿°ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ï¼ˆä¾‹å¦‚ï¼š"ç”¨æˆ·è¯¢é—®çš„æ˜¯..."æˆ–"ç”¨æˆ·å¸Œæœ›äº†è§£..."ï¼‰
   - ä½¿ç”¨è‡ªç„¶ã€ä¸“ä¸šçš„è¯­è¨€ï¼ˆä¸è¦ä½¿ç”¨"è¿™æ˜¯ä¸€ä¸ª...é¢†åŸŸçš„ä»»åŠ¡"ç­‰æ¨¡æ¿åŒ–è¡¨è¾¾ï¼‰
   - é•¿åº¦æ§åˆ¶åœ¨150-300å­—
   - é‡ç‚¹çªå‡ºï¼Œé€»è¾‘æ¸…æ™°
   - æä¾›æ·±åº¦åˆ†æä»·å€¼ï¼Œè€Œä¸æ˜¯ç®€å•é‡å¤é—®é¢˜
   - é¿å…è¿‡äºå®½æ³›çš„å»ºè®®ï¼Œè¦æœ‰é’ˆå¯¹æ€§

ã€è¾“å‡ºæ ¼å¼ã€‘
ç›´æ¥è¾“å‡ºæ€ç»´ç§å­å†…å®¹ï¼Œä¸éœ€è¦JSONæ ¼å¼ï¼Œä¸éœ€è¦æ ‡é¢˜å‰ç¼€ã€‚ç”¨2-4ä¸ªè‡ªç„¶æ®µè½ç»„ç»‡ä½ çš„åˆ†æã€‚

"""
            
            # è°ƒç”¨LLMï¼ˆæ”¯æŒæµå¼ï¼‰
            if enable_streaming:
                # æµå¼æ¨¡å¼ï¼šå®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹
                logger.debug("ğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆæ€ç»´ç§å­...")
                
                llm_response = self._call_llm_streaming(
                    prompt=llm_prompt,
                    temperature=0.7,
                    max_tokens=800
                )
            else:
                # éæµå¼æ¨¡å¼ï¼šä¸€æ¬¡æ€§è¿”å›
                llm_response = self._call_llm(
                    prompt=llm_prompt,
                    temperature=0.7,
                    max_tokens=800,
                    enable_streaming=False
                )
            
            if llm_response:
                # æ¸…ç†å’ŒéªŒè¯å“åº”
                cleaned_seed = llm_response.strip()
                
                # åŸºæœ¬è´¨é‡æ£€æŸ¥
                if len(cleaned_seed) < 50:
                    logger.warning(f"âš ï¸ LLMç”Ÿæˆçš„ç§å­è¿‡çŸ­({len(cleaned_seed)}å­—ç¬¦)ï¼Œå¯èƒ½è´¨é‡ä¸ä½³")
                    return None
                
                if len(cleaned_seed) > 1000:
                    logger.warning(f"âš ï¸ LLMç”Ÿæˆçš„ç§å­è¿‡é•¿({len(cleaned_seed)}å­—ç¬¦)ï¼Œè¿›è¡Œæˆªæ–­")
                    cleaned_seed = cleaned_seed[:800] + "..."
                
                # éªŒè¯ä¸æ˜¯ç®€å•çš„é—®é¢˜å¤è¿°
                if user_query in cleaned_seed[:100] and len(user_query) > 20:
                    logger.debug("âš ï¸ LLMå¯èƒ½è¿‡å¤šå¤è¿°é—®é¢˜ï¼Œä½†ä»å¯æ¥å—")
                
                logger.debug(f"ğŸ¤– LLMç§å­é¢„è§ˆ: {cleaned_seed[:100]}...")
                return cleaned_seed
            
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLMå¢å¼ºç§å­ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _generate_heuristic_seed(self, user_query: str, analysis: Dict,
                                 execution_context: Optional[Dict] = None) -> str:
        """
        å¯å‘å¼ç§å­ç”Ÿæˆï¼ˆæ”¹è¿›ç‰ˆ - æ›´è‡ªç„¶çš„è¡¨è¾¾ï¼‰
        
        ç›¸æ¯”æ—§ç‰ˆçš„æ¨¡æ¿åŒ–è¾“å‡ºï¼Œæ–°ç‰ˆä½¿ç”¨æ›´è‡ªç„¶ã€æ›´æœ‰é’ˆå¯¹æ€§çš„è¯­è¨€ã€‚
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            analysis: å¿«é€Ÿåˆ†æç»“æœ
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            åŸºäºè§„åˆ™ä½†æ›´è‡ªç„¶çš„æ€ç»´ç§å­
        """
        seed_parts = []
        
        # é¢†åŸŸè¯†åˆ«ï¼ˆæ›´è‡ªç„¶çš„è¡¨è¾¾ï¼‰
        domain = analysis.get('domain', 'general')
        domain_descriptions = {
            'web_development': 'Webå¼€å‘ç›¸å…³',
            'data_science': 'æ•°æ®ç§‘å­¦å’Œåˆ†æç›¸å…³',
            'api_development': 'APIæ¥å£å¼€å‘ç›¸å…³',
            'database': 'æ•°æ®åº“è®¾è®¡å’Œæ“ä½œç›¸å…³',
            'system_admin': 'ç³»ç»Ÿè¿ç»´å’Œç®¡ç†ç›¸å…³',
            'machine_learning': 'æœºå™¨å­¦ä¹ å’ŒAIç›¸å…³',
            'security': 'å®‰å…¨å’Œé˜²æŠ¤ç›¸å…³',
            'mobile_development': 'ç§»åŠ¨åº”ç”¨å¼€å‘ç›¸å…³',
            'performance': 'æ€§èƒ½ä¼˜åŒ–ç›¸å…³',
            'automation': 'è‡ªåŠ¨åŒ–å’Œè„šæœ¬ç›¸å…³',
            'general': 'é€šç”¨æŠ€æœ¯'
        }
        domain_desc = domain_descriptions.get(domain, 'æŠ€æœ¯')
        seed_parts.append(f"è¿™ä¸ªé—®é¢˜æ¶‰åŠ{domain_desc}çš„å†…å®¹ã€‚")
        
        # å¤æ‚åº¦åˆ†æï¼ˆæ›´æœ‰é’ˆå¯¹æ€§ï¼‰
        complexity = analysis.get('complexity_score', 0.5)
        if complexity > 0.8:
            seed_parts.append("é—®é¢˜å…·æœ‰è¾ƒé«˜çš„æŠ€æœ¯å¤æ‚åº¦ï¼Œéœ€è¦æ·±å…¥åˆ†æå¤šä¸ªæŠ€æœ¯å±‚é¢ï¼Œå¹¶ç»¼åˆè€ƒè™‘ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆã€‚")
        elif complexity > 0.5:
            seed_parts.append("è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰å¤æ‚åº¦çš„é—®é¢˜ï¼Œéœ€è¦ç»“åˆç†è®ºçŸ¥è¯†å’Œå®è·µç»éªŒæ¥åˆ¶å®šåˆç†çš„è§£å†³æ–¹æ¡ˆã€‚")
        else:
            seed_parts.append("ä»æŠ€æœ¯è§’åº¦çœ‹ï¼Œè¿™ä¸ªé—®é¢˜æœ‰æ¯”è¾ƒæ¸…æ™°çš„è§£å†³è·¯å¾„ï¼Œå¯ä»¥é‡‡ç”¨æ ‡å‡†çš„æ–¹æ³•æ¥å¤„ç†ã€‚")
        
        # å…³é”®å› ç´ ï¼ˆå¦‚æœæœ‰ï¼‰
        key_factors = analysis.get('key_factors', [])
        if key_factors:
            factors_text = "ã€".join(key_factors[:3])
            seed_parts.append(f"å…³é”®éœ€è¦å…³æ³¨çš„æŠ€æœ¯ç‚¹åŒ…æ‹¬ï¼š{factors_text}ã€‚")
        
        # ç½®ä¿¡åº¦å’Œå»ºè®®
        confidence = analysis.get('confidence_score', 0.5)
        if confidence > 0.8:
            seed_parts.append("åŸºäºé—®é¢˜çš„æ¸…æ™°åº¦å’Œå¸¸è§æ€§ï¼Œæˆ‘ä»¬æœ‰è¾ƒé«˜æŠŠæ¡æ‰¾åˆ°æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚")
        elif confidence > 0.5:
            seed_parts.append("éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®å…·ä½“éœ€æ±‚å’ŒæŠ€æœ¯çº¦æŸï¼Œä»¥ä¾¿æä¾›æ›´ç²¾å‡†çš„è§£å†³æ–¹æ¡ˆã€‚")
        else:
            seed_parts.append("å»ºè®®å…ˆæ¾„æ¸…é—®é¢˜çš„å…·ä½“åœºæ™¯å’ŒæŠ€æœ¯èƒŒæ™¯ï¼Œè¿™æœ‰åŠ©äºåˆ¶å®šæ›´æœ‰é’ˆå¯¹æ€§çš„è§£å†³ç­–ç•¥ã€‚")
        
        # å¤šæ­¥éª¤å»ºè®®
        if analysis.get('requires_multi_step'):
            seed_parts.append("è¿™ä¸ªä»»åŠ¡éœ€è¦åˆ†æ­¥éª¤æ¥å®æ–½ï¼Œå»ºè®®é‡‡ç”¨æ¸è¿›å¼çš„æ–¹æ³•é€ä¸€éªŒè¯æ¯ä¸ªç¯èŠ‚ã€‚")
        
        # æ‰§è¡Œä¸Šä¸‹æ–‡è€ƒè™‘
        if execution_context:
            if execution_context.get('real_time_requirements'):
                seed_parts.append("ç‰¹åˆ«éœ€è¦æ³¨æ„å®æ—¶æ€§èƒ½çš„è¦æ±‚ã€‚")
            if execution_context.get('performance_critical'):
                seed_parts.append("æ€§èƒ½ä¼˜åŒ–åº”è¯¥ä½œä¸ºè®¾è®¡çš„æ ¸å¿ƒè€ƒè™‘å› ç´ ã€‚")
        
        return " ".join(seed_parts)
    
    def _generate_emergency_fallback_seed(self, user_query: str) -> str:
        """
        ç´§æ€¥å›é€€ç§å­ç”Ÿæˆï¼ˆæœ€ç®€å•ä½†å¯ç”¨ï¼‰
        
        å½“æ‰€æœ‰å…¶ä»–æ–¹æ³•éƒ½å¤±è´¥æ—¶ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œç¡®ä¿ç³»ç»Ÿå§‹ç»ˆèƒ½è¿”å›å¯ç”¨çš„ç§å­ã€‚
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            åŸºç¡€ä½†å¯ç”¨çš„æ€ç»´ç§å­
        """
        return (
            f"é’ˆå¯¹é—®é¢˜'{user_query}'ï¼Œéœ€è¦è¿›è¡Œç³»ç»Ÿæ€§çš„åˆ†æã€‚"
            f"å»ºè®®é¦–å…ˆæ˜ç¡®é—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚å’ŒæŠ€æœ¯èƒŒæ™¯ï¼Œ"
            f"ç„¶åæ¢ç´¢å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆå¹¶è¯„ä¼°å…¶ä¼˜åŠ£ï¼Œ"
            f"æœ€ååˆ¶å®šå…·ä½“çš„å®æ–½æ­¥éª¤å¹¶è¿›è¡ŒéªŒè¯ã€‚"
        )
    
    def generate_thinking_seed(self, user_query: str, execution_context: Optional[Dict] = None,
                              enable_streaming: bool = False) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ€ç»´ç§å­ - NeogenesisPlannerå…¼å®¹æ¥å£ï¼ˆæ”¯æŒæµå¼ï¼‰
        
        è¿™ä¸ªæ–¹æ³•æ˜¯ä¸ºäº†å…¼å®¹NeogenesisPlannerä¸­çš„è°ƒç”¨ï¼Œå°†get_thinking_seedçš„ç»“æœ
        åŒ…è£…æˆæœŸæœ›çš„å­—å…¸æ ¼å¼ã€‚
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            Dict: åŒ…å«thinking_seedã€reasoningã€confidenceçš„å­—å…¸
        """
        try:
            logger.info(f"ğŸ§  ç”Ÿæˆæ€ç»´ç§å­ (å…¼å®¹æ¥å£): {user_query[:30]}...")
            
            # è°ƒç”¨æ ¸å¿ƒçš„get_thinking_seedæ–¹æ³•ï¼ˆä¼ é€’æµå¼å‚æ•°ï¼‰
            thinking_seed = self.get_thinking_seed(user_query, execution_context, enable_streaming)
            
            # è·å–åˆ†æä¿¡æ¯
            analysis = self.get_quick_analysis_summary(user_query, execution_context)
            confidence = self.assess_task_confidence(user_query, execution_context)
            
            # æ„å»ºæ¨ç†è¿‡ç¨‹æè¿°
            reasoning_parts = [
                f"ä»»åŠ¡é¢†åŸŸ: {analysis.get('domain', 'é€šç”¨')}",
                f"å¤æ‚åº¦è¯„åˆ†: {analysis.get('complexity_score', 0.5):.2f}",
                f"ç½®ä¿¡åº¦è¯„åˆ†: {confidence:.2f}",
                f"æ¨èç­–ç•¥: {analysis.get('recommendation', 'ç³»ç»Ÿæ€§åˆ†æ')}"
            ]
            
            if analysis.get('key_factors'):
                factors_text = "ã€".join(analysis['key_factors'][:3])
                reasoning_parts.append(f"å…³é”®å› ç´ : {factors_text}")
            
            reasoning_process = "; ".join(reasoning_parts)
            
            # åŒ…è£…æˆæœŸæœ›çš„æ ¼å¼
            result = {
                "thinking_seed": thinking_seed,
                "reasoning": reasoning_process,
                "confidence": confidence,
                "generation_method": "prior_reasoner_enhanced",
                "domain": analysis.get('domain', 'é€šç”¨'),
                "complexity_score": analysis.get('complexity_score', 0.5),
                "user_query": user_query
            }
            
            logger.info(f"âœ… æ€ç»´ç§å­ç”Ÿæˆå®Œæˆ: {len(thinking_seed)}å­—ç¬¦, ç½®ä¿¡åº¦: {confidence:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ€ç»´ç§å­ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€ç»“æœä½†ä¿æŒæ ¼å¼ä¸€è‡´
            fallback_seed = f"é’ˆå¯¹æŸ¥è¯¢'{user_query}'çš„åŸºç¡€æ€ç»´ç§å­ï¼šéœ€è¦ç³»ç»Ÿæ€§åˆ†æå’Œè§£å†³æ–¹æ¡ˆåˆ¶å®šã€‚"
            return {
                "thinking_seed": fallback_seed,
                "reasoning": "ä½¿ç”¨å›é€€ç”Ÿæˆæ–¹æ³•",
                "confidence": 0.3,
                "generation_method": "fallback",
                "error": str(e)
            }
    
    def analyze_task_complexity(self, user_query: str) -> Dict[str, Any]:
        """
        åˆ†æä»»åŠ¡å¤æ‚åº¦
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            å¤æ‚åº¦åˆ†æç»“æœ
        """
        complexity_score = 0.5
        complexity_factors = {}
        
        # å…³é”®è¯å¤æ‚åº¦æŒ‡æ ‡
        complexity_keywords = {
            'å¤šæ­¥éª¤': 0.15,
            'é›†æˆ': 0.12,
            'ä¼˜åŒ–': 0.10,
            'åˆ†æ': 0.08,
            'è®¾è®¡': 0.08,
            'æ¶æ„': 0.12,
            'åˆ†å¸ƒå¼': 0.15,
            'å¹¶å‘': 0.13,
            'å®æ—¶': 0.10,
            'é«˜æ€§èƒ½': 0.11,
            'æœºå™¨å­¦ä¹ ': 0.14,
            'æ·±åº¦å­¦ä¹ ': 0.16,
            'ç®—æ³•': 0.09,
            'æ•°æ®åº“': 0.07,
            'ç½‘ç»œ': 0.06,
            'å®‰å…¨': 0.08
        }
        
        for keyword, weight in complexity_keywords.items():
            if keyword in user_query:
                complexity_score += weight
                complexity_factors[keyword] = weight
                logger.debug(f"ğŸ” æ£€æµ‹åˆ°å¤æ‚åº¦å…³é”®è¯: {keyword} (+{weight})")
        
        # å¥æ³•å¤æ‚åº¦
        sentences = user_query.split('ã€‚')
        if len(sentences) > 3:
            syntax_complexity = min(0.1, (len(sentences) - 3) * 0.02)
            complexity_score += syntax_complexity
            complexity_factors['å¤šå¥è¡¨è¾¾'] = syntax_complexity
        
        # å­—ç¬¦é•¿åº¦å¤æ‚åº¦
        if len(user_query) > 150:
            length_complexity = min(0.08, (len(user_query) - 150) / 1000)
            complexity_score += length_complexity
            complexity_factors['è¡¨è¾¾é•¿åº¦'] = length_complexity
        
        # æŠ€æœ¯è¯æ±‡å¯†åº¦
        tech_words = ['API', 'HTTP', 'JSON', 'SQL', 'Python', 'JavaScript', 'REST', 'GraphQL']
        tech_density = sum(1 for word in tech_words if word in user_query) / max(len(user_query.split()), 1)
        if tech_density > 0.1:
            tech_complexity = min(0.12, tech_density * 2)
            complexity_score += tech_complexity
            complexity_factors['æŠ€æœ¯è¯æ±‡å¯†åº¦'] = tech_complexity
        
        # é¢†åŸŸæ¨æ–­
        domain = self._infer_domain(user_query)
        
        # å¤šæ­¥éª¤æ£€æµ‹
        requires_multi_step = any(word in user_query for word in [
            'æ­¥éª¤', 'é˜¶æ®µ', 'åˆ†æ­¥', 'ç„¶å', 'æ¥ä¸‹æ¥', 'é¦–å…ˆ', 'æœ€å',
            'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'ä¾æ¬¡', 'é¡ºåº'
        ])
        
        # é™åˆ¶å¤æ‚åº¦åˆ†æ•°
        final_complexity = min(1.0, complexity_score)
        
        result = {
            'complexity_score': final_complexity,
            'complexity_factors': complexity_factors,
            'estimated_domain': domain,
            'requires_multi_step': requires_multi_step,
            'sentence_count': len(sentences),
            'word_count': len(user_query.split()),
            'tech_density': tech_density
        }
        
        logger.info(f"ğŸ§® å¤æ‚åº¦åˆ†æå®Œæˆ: {final_complexity:.3f} (å› å­æ•°:{len(complexity_factors)})")
        return result
    
    def _infer_domain(self, user_query: str) -> str:
        """
        æ¨æ–­ä»»åŠ¡é¢†åŸŸ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æ¨æ–­çš„é¢†åŸŸ
        """
        query_lower = user_query.lower()
        
        domain_indicators = {
            'web_development': ['ç½‘ç«™', 'web', 'html', 'css', 'javascript', 'å‰ç«¯', 'åç«¯'],
            'data_science': ['æ•°æ®åˆ†æ', 'æ•°æ®ç§‘å­¦', 'pandas', 'numpy', 'æœºå™¨å­¦ä¹ ', 'æ¨¡å‹', 'é¢„æµ‹'],
            'api_development': ['api', 'æ¥å£', 'rest', 'restful', 'graphql', 'endpoints'],
            'web_scraping': ['çˆ¬è™«', 'spider', 'scrapy', 'æŠ“å–', 'çˆ¬å–', 'crawl'],
            'database': ['æ•°æ®åº“', 'sql', 'mysql', 'postgresql', 'mongodb', 'æŸ¥è¯¢'],
            'system_admin': ['ç³»ç»Ÿ', 'æœåŠ¡å™¨', 'éƒ¨ç½²', 'è¿ç»´', 'docker', 'kubernetes'],
            'mobile_development': ['ç§»åŠ¨', 'app', 'å®‰å“', 'android', 'ios', 'react native'],
            'security': ['å®‰å…¨', 'åŠ å¯†', 'è®¤è¯', 'æˆæƒ', 'é˜²æŠ¤', 'security'],
            'performance': ['æ€§èƒ½', 'ä¼˜åŒ–', 'é€Ÿåº¦', 'æ•ˆç‡', 'benchmark', 'è´Ÿè½½'],
            'automation': ['è‡ªåŠ¨åŒ–', 'è„šæœ¬', 'å®šæ—¶', 'æ‰¹å¤„ç†', 'cron', 'schedule']
        }
        
        domain_scores = {}
        for domain, keywords in domain_indicators.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > 0:
                domain_scores[domain] = score
        
        if domain_scores:
            inferred_domain = max(domain_scores, key=domain_scores.get)
            logger.debug(f"ğŸ·ï¸ æ¨æ–­é¢†åŸŸ: {inferred_domain} (åŒ¹é…åº¦:{domain_scores[inferred_domain]})")
            return inferred_domain
        
        return 'general'
    
    def get_confidence_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç½®ä¿¡åº¦ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç½®ä¿¡åº¦ç»Ÿè®¡æ•°æ®
        """
        if not self.confidence_history:
            return {
                'total_assessments': 0,
                'avg_confidence': 0.0,
                'confidence_trend': 'insufficient_data',
                'cache_size': len(self.assessment_cache)
            }
        
        confidences = [item['predicted_confidence'] for item in self.confidence_history]
        avg_confidence = sum(confidences) / len(confidences)
        
        # è®¡ç®—è¶‹åŠ¿
        if len(confidences) >= 5:
            recent_avg = sum(confidences[-5:]) / 5
            earlier_avg = sum(confidences[-10:-5]) / 5 if len(confidences) >= 10 else avg_confidence
            
            if recent_avg > earlier_avg + 0.05:
                trend = 'improving'
            elif recent_avg < earlier_avg - 0.05:
                trend = 'declining'
            else:
                trend = 'stable'
        else:
            trend = 'insufficient_data'
        
        return {
            'total_assessments': len(self.confidence_history),
            'avg_confidence': avg_confidence,
            'min_confidence': min(confidences),
            'max_confidence': max(confidences),
            'confidence_trend': trend,
            'cache_size': len(self.assessment_cache),
            'recent_confidences': confidences[-5:] if len(confidences) >= 5 else confidences
        }
    
    def update_confidence_feedback(self, predicted_confidence: float, 
                                 actual_success: bool, execution_time: float):
        """
        æ›´æ–°ç½®ä¿¡åº¦åé¦ˆï¼Œç”¨äºæ”¹è¿›é¢„æµ‹å‡†ç¡®æ€§
        
        Args:
            predicted_confidence: é¢„æµ‹çš„ç½®ä¿¡åº¦
            actual_success: å®é™…æ‰§è¡Œæ˜¯å¦æˆåŠŸ
            execution_time: æ‰§è¡Œæ—¶é—´
        """
        feedback_record = {
            'timestamp': time.time(),
            'predicted_confidence': predicted_confidence,
            'actual_success': actual_success,
            'execution_time': execution_time,
            'confidence_accuracy': abs(predicted_confidence - (1.0 if actual_success else 0.0))
        }
        
        self.confidence_history.append(feedback_record)
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.confidence_history) > 200:
            self.confidence_history = self.confidence_history[-100:]
        
        logger.debug(f"ğŸ“ˆ æ›´æ–°ç½®ä¿¡åº¦åé¦ˆ: é¢„æµ‹={predicted_confidence:.3f}, å®é™…={'æˆåŠŸ' if actual_success else 'å¤±è´¥'}")
    
    def get_quick_analysis_summary(self, user_query: str, execution_context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        è·å–å¿«é€Ÿåˆ†ææ€»ç»“ - PriorReasonerçš„æ–°æ ¸å¿ƒåŠŸèƒ½
        
        æä¾›ä»»åŠ¡çš„å¿«é€Ÿæ¦‚è§ˆï¼ŒåŒ…æ‹¬å¤æ‚åº¦ã€ç½®ä¿¡åº¦ã€é¢†åŸŸç­‰å…³é”®ä¿¡æ¯
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            å¿«é€Ÿåˆ†ææ€»ç»“
        """
        start_time = time.time()
        
        # å¿«é€Ÿåˆ†æ
        complexity_analysis = self.analyze_task_complexity(user_query)
        confidence_score = self.assess_task_confidence(user_query, execution_context)
        
        analysis_time = time.time() - start_time
        
        summary = {
            'domain': complexity_analysis.get('estimated_domain', 'general'),
            'complexity_score': complexity_analysis.get('complexity_score', 0.5),
            'confidence_score': confidence_score,
            'requires_multi_step': complexity_analysis.get('requires_multi_step', False),
            'key_factors': list(complexity_analysis.get('complexity_factors', {}).keys())[:3],
            'analysis_time': analysis_time,
            'recommendation': self._get_analysis_recommendation(
                complexity_analysis.get('complexity_score', 0.5), 
                confidence_score
            )
        }
        
        logger.info(f"âš¡ å¿«é€Ÿåˆ†æå®Œæˆ: {summary['domain']}é¢†åŸŸ, å¤æ‚åº¦{summary['complexity_score']:.2f}, ç½®ä¿¡åº¦{summary['confidence_score']:.2f}")
        return summary
    
    def _get_analysis_recommendation(self, complexity_score: float, confidence_score: float) -> str:
        """
        åŸºäºåˆ†æç»“æœæä¾›å»ºè®®
        
        Args:
            complexity_score: å¤æ‚åº¦åˆ†æ•°
            confidence_score: ç½®ä¿¡åº¦åˆ†æ•°
            
        Returns:
            åˆ†æå»ºè®®
        """
        if complexity_score > 0.8 and confidence_score < 0.4:
            return "é«˜å¤æ‚åº¦ä½ç½®ä¿¡åº¦ä»»åŠ¡ï¼Œå»ºè®®é‡‡ç”¨å¤šé˜¶æ®µéªŒè¯å’Œä¿å®ˆç­–ç•¥"
        elif complexity_score > 0.7:
            return "å¤æ‚ä»»åŠ¡ï¼Œå»ºè®®é‡‡ç”¨ç³»ç»Ÿåˆ†æå’Œåˆ†æ­¥æ‰§è¡Œ"
        elif confidence_score > 0.8:
            return "é«˜ç½®ä¿¡åº¦ä»»åŠ¡ï¼Œå¯ä»¥é‡‡ç”¨ç›´æ¥æ‰§è¡Œç­–ç•¥"
        elif confidence_score < 0.3:
            return "ä½ç½®ä¿¡åº¦ä»»åŠ¡ï¼Œå»ºè®®å¯»æ±‚é¢å¤–ä¿¡æ¯æˆ–æ¾„æ¸…"
        else:
            return "ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡ï¼Œå»ºè®®é‡‡ç”¨å¹³è¡¡çš„åˆ†æå’Œæ‰§è¡Œç­–ç•¥"
    
    def reset_cache(self):
        """é‡ç½®è¯„ä¼°ç¼“å­˜"""
        self.assessment_cache.clear()
        logger.info("ğŸ”„ è½»é‡çº§åˆ†æåŠ©æ‰‹ç¼“å­˜å·²é‡ç½®")
