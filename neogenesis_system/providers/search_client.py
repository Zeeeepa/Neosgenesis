#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœç´¢å·¥å…·å®¢æˆ·ç«¯ - ç”¨äºè¿æ¥å¤–éƒ¨æœç´¢å¼•æ“
Search Tool Client - for connecting to external search engines
"""

import os
import json
import logging
import time
import ssl
import certifi
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.ssl_ import create_urllib3_context
from urllib3.poolmanager import PoolManager

# ğŸ”§ æ–°å¢ï¼šæ”¯æŒçœŸå®Firecrawlæœç´¢
try:
    from firecrawl import FirecrawlApp
    FIRECRAWL_AVAILABLE = True
except ImportError:
    FIRECRAWL_AVAILABLE = False
    logging.warning("âš ï¸ firecrawl-pyåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢ç»“æœ")

# ğŸ”¥ æ–°å¢ï¼šæ”¯æŒTavilyæœç´¢
try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except ImportError:
    TAVILY_AVAILABLE = False
    logging.warning("âš ï¸ tavily-pythonåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢ç»“æœ")

# ğŸ”‘ Tavily APIå¯†é’¥
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "Your_API_Key")


# ğŸ”¥ æ ¹æœ¬æ€§SSLä¿®å¤ï¼šè‡ªå®šä¹‰SSLé€‚é…å™¨
class SSLAdapter(HTTPAdapter):
    """
    è‡ªå®šä¹‰SSLé€‚é…å™¨ï¼Œè§£å†³SSL/TLSæ¡æ‰‹é—®é¢˜
    ä½¿ç”¨ç°ä»£åŒ–çš„TLSé…ç½®ï¼Œæ”¯æŒTLS 1.2å’Œ1.3
    """
    def init_poolmanager(self, *args, **kwargs):
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æœ€æ–°çš„TLSåè®®
        context = create_urllib3_context()
        
        # è®¾ç½®æ”¯æŒçš„TLSç‰ˆæœ¬ï¼ˆTLS 1.2 å’Œ 1.3ï¼‰
        context.minimum_version = ssl.TLSVersion.TLSv1_2
        context.maximum_version = ssl.TLSVersion.TLSv1_3
        
        # åŠ è½½ç³»ç»ŸCAè¯ä¹¦
        try:
            context.load_verify_locations(certifi.where())
        except Exception:
            pass
        
        # è®¾ç½®å¯†ç å¥—ä»¶ï¼ˆä½¿ç”¨ç°ä»£å®‰å…¨çš„å¯†ç å¥—ä»¶ï¼‰
        context.set_ciphers('DEFAULT@SECLEVEL=1')
        
        # é…ç½®SSLé€‰é¡¹
        context.check_hostname = True
        context.verify_mode = ssl.CERT_REQUIRED
        
        # è®¾ç½®ALPNåè®®ï¼ˆæ”¯æŒHTTP/2ï¼‰
        try:
            context.set_alpn_protocols(['h2', 'http/1.1'])
        except AttributeError:
            pass
        
        kwargs['ssl_context'] = context
        return super().init_poolmanager(*args, **kwargs)

# å¯¼å…¥é…ç½®
try:
    from neogenesis_system.config import RAG_CONFIG
except ImportError:
    try:
        from ..config import RAG_CONFIG
    except ImportError:
        # é»˜è®¤é…ç½®ï¼ˆå¦‚æœæ— æ³•å¯¼å…¥ï¼‰
        RAG_CONFIG = {"enable_real_web_search": False}

logger = logging.getLogger(__name__)

# å…¨å±€æœç´¢è¯·æ±‚ç®¡ç†å™¨ï¼Œç”¨äºæ§åˆ¶è¯·æ±‚é¢‘ç‡
class SearchRateLimiter:
    """æœç´¢é€Ÿç‡é™åˆ¶ç®¡ç†å™¨"""
    def __init__(self):
        self.last_request_time = 0
        # ä»é…ç½®è·å–è¯·æ±‚é—´éš”ï¼Œå¦‚æœé…ç½®ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.min_interval = RAG_CONFIG.get("search_rate_limit_interval", 1.5)
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªå…è®¸çš„è¯·æ±‚æ—¶é—´"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_interval:
            wait_time = self.min_interval - time_since_last
            logger.debug(f"â³ é€Ÿç‡é™åˆ¶ç­‰å¾…: {wait_time:.1f}ç§’")
            time.sleep(wait_time)
        
        self.last_request_time = time.time()

# å…¨å±€é€Ÿç‡é™åˆ¶å™¨å®ä¾‹
_rate_limiter = SearchRateLimiter()

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    snippet: str
    url: str
    relevance_score: float = 0.0

@dataclass
class SearchResponse:
    """æœç´¢å“åº”æ•°æ®ç»“æ„"""
    query: str
    results: List[SearchResult]
    total_results: int
    search_time: float
    success: bool = True
    error_message: str = ""
    metadata: Optional[Dict[str, Any]] = None  # ğŸ”¥ æ–°å¢ï¼šæ”¯æŒé¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¦‚Tavilyçš„AIç­”æ¡ˆï¼‰

@dataclass
class IdeaVerificationResult:
    """æƒ³æ³•éªŒè¯ç»“æœæ•°æ®ç»“æ„"""
    idea_text: str
    feasibility_score: float
    analysis_summary: str
    search_results: List[SearchResult]
    success: bool = True
    error_message: str = ""

class WebSearchClient:
    """ç½‘ç»œæœç´¢å®¢æˆ·ç«¯"""
    
    def __init__(self, search_engine: str = "tavily", max_results: int = 5):
        """
        åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯
        
        Args:
            search_engine: æœç´¢å¼•æ“ç±»å‹ ("tavily", "firecrawl", "bing", "google")
            max_results: æœ€å¤§ç»“æœæ•°é‡
        """
        self.search_engine = search_engine
        self.max_results = max_results
        
        # ğŸ”¥ æ ¹æœ¬æ€§SSLä¿®å¤ï¼šä½¿ç”¨è‡ªå®šä¹‰SSLé€‚é…å™¨
        self.session = requests.Session()
        
        # é…ç½®ç°ä»£åŒ–çš„è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
        # æŒ‚è½½è‡ªå®šä¹‰SSLé€‚é…å™¨åˆ°æ‰€æœ‰HTTPSè¿æ¥
        ssl_adapter = SSLAdapter(
            max_retries=3,  # è¿æ¥é‡è¯•æ¬¡æ•°
            pool_connections=10,  # è¿æ¥æ± å¤§å°
            pool_maxsize=10,  # æœ€å¤§è¿æ¥æ•°
        )
        self.session.mount('https://', ssl_adapter)
        self.session.mount('http://', HTTPAdapter(max_retries=3))
        
        # è®¾ç½®åˆç†çš„è¶…æ—¶é…ç½®
        self.timeout = (10, 30)  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
        
        # æœç´¢ç»Ÿè®¡
        self.search_stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'total_search_time': 0.0,
            'avg_search_time': 0.0
        }
        
        logger.info(f"ğŸ” WebSearchClientåˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨{search_engine}æœç´¢å¼•æ“")
        logger.info(f"ğŸ”’ SSLé…ç½®ï¼šTLS 1.2/1.3 + ç°ä»£å¯†ç å¥—ä»¶ + certifiè¯ä¹¦")
    
    def search(self, query: str, max_results: Optional[int] = None) -> SearchResponse:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°é‡ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            
        Returns:
            SearchResponse: æœç´¢å“åº”
        """
        start_time = time.time()
        max_results = max_results or self.max_results
        
        logger.info(f"ğŸ” [WebSearchClient] å¼€å§‹æœç´¢: {query[:50]}...")
        logger.info(f"ğŸ” [WebSearchClient] æœç´¢å¼•æ“: {self.search_engine}")
        logger.info(f"ğŸ” [WebSearchClient] æœ€å¤§ç»“æœæ•°: {max_results}")
        
        try:
            if self.search_engine == "tavily":
                logger.info(f"ğŸ” [WebSearchClient] è°ƒç”¨_search_tavilyæ–¹æ³•...")
                response = self._search_tavily(query, max_results)
            elif self.search_engine == "firecrawl":
                logger.info(f"ğŸ” [WebSearchClient] è°ƒç”¨_search_firecrawlæ–¹æ³•...")
                response = self._search_firecrawl(query, max_results)
            elif self.search_engine == "bing":
                response = self._search_bing(query, max_results)
            else:
                response = self._search_fallback(query, max_results)
            
            search_time = time.time() - start_time
            response.search_time = search_time
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_search_stats(search_time, response.success)
            
            logger.info(f"ğŸ” æœç´¢å®Œæˆ: æ‰¾åˆ°{len(response.results)}ä¸ªç»“æœï¼Œè€—æ—¶{search_time:.2f}ç§’")
            return response
            
        except Exception as e:
            search_time = time.time() - start_time
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            
            return SearchResponse(
                query=query,
                results=[],
                total_results=0,
                search_time=search_time,
                success=False,
                error_message=str(e)
            )
    
    def _search_tavily(self, query: str, max_results: int) -> SearchResponse:
        """ğŸ”¥ ä½¿ç”¨Tavilyæœç´¢ - AIæœç´¢å¼•æ“"""
        logger.info(f"ğŸŒ å¼€å§‹Tavilyæœç´¢: {query[:50]}...")
        
        if not TAVILY_AVAILABLE:
            logger.warning("âš ï¸ Tavilyåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæœç´¢æ¨¡å¼")
            return self._search_fallback(query, max_results)
        
        try:
            # åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
            tavily_client = TavilyClient(api_key=TAVILY_API_KEY)
            logger.info(f"âœ… Tavilyå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
            # æ‰§è¡Œæœç´¢
            logger.info(f"ğŸ” æ­£åœ¨æœç´¢: {query}")
            search_response = tavily_client.search(
                query=query,
                max_results=max_results,
                search_depth="advanced",  # ä½¿ç”¨é«˜çº§æœç´¢æ¨¡å¼
                include_answer=True,       # åŒ…å«AIæ€»ç»“ç­”æ¡ˆ
                include_raw_content=False  # ä¸åŒ…å«åŸå§‹å†…å®¹ï¼ˆèŠ‚çœtokenï¼‰
            )
            
            logger.debug(f"ğŸ” Tavilyå“åº”ç±»å‹: {type(search_response)}")
            logger.debug(f"ğŸ” Tavilyå“åº”å†…å®¹: {search_response}")
            
            # è§£ææœç´¢ç»“æœ
            results = []
            if isinstance(search_response, dict):
                tavily_results = search_response.get('results', [])
                
                for idx, item in enumerate(tavily_results[:max_results]):
                    try:
                        result = SearchResult(
                            title=item.get('title', f'ç»“æœ {idx + 1}'),
                            url=item.get('url', ''),
                            snippet=item.get('content', '')[:500],  # é™åˆ¶é•¿åº¦
                            relevance_score=item.get('score', 0.8)  # Tavilyæä¾›ç›¸å…³æ€§åˆ†æ•°
                        )
                        results.append(result)
                        logger.debug(f"  âœ… ç»“æœ {idx + 1}: {result.title[:50]}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ è§£æTavilyç»“æœé¡¹å¤±è´¥: {e}")
                        continue
                
                # æ„å»ºæˆåŠŸå“åº”
                return SearchResponse(
                    query=query,
                    results=results,
                    total_results=len(results),
                    search_time=0.0,  # å°†åœ¨å¤–å±‚è®¡ç®—
                    success=True,
                    metadata={
                        'search_engine': 'tavily',
                        'answer': search_response.get('answer', ''),  # Tavilyçš„AIæ€»ç»“ç­”æ¡ˆ
                        'search_depth': 'advanced'
                    }
                )
            else:
                logger.warning(f"âš ï¸ Tavilyè¿”å›äº†éé¢„æœŸçš„å“åº”æ ¼å¼: {type(search_response)}")
                return self._search_fallback(query, max_results)
                
        except Exception as e:
            logger.error(f"âŒ Tavilyæœç´¢å¤±è´¥: {e}")
            logger.exception(e)
            
            # æ„å»ºå¤±è´¥å“åº”
            return SearchResponse(
                query=query,
                results=[],
                total_results=0,
                search_time=0.0,
                success=False,
                error_message=f"Tavilyæœç´¢å¤±è´¥: {str(e)}"
            )
    
    def _search_firecrawl(self, query: str, max_results: int) -> SearchResponse:
        """ä½¿ç”¨Firecrawlæœç´¢ - å¸¦æ™ºèƒ½å¤‡ç”¨æœºåˆ¶"""
        
        # ğŸ”§ è¯¦ç»†çš„æœç´¢æ¨¡å¼è¯Šæ–­
        enable_real_search = RAG_CONFIG.get("enable_real_web_search", False)
        logger.info(f"ğŸ” æœç´¢æ¨¡å¼è¯Šæ–­:")
        logger.info(f"   - enable_real_web_search: {enable_real_search}")
        logger.info(f"   - REAL_SEARCH_AVAILABLE: {REAL_SEARCH_AVAILABLE}")
        logger.info(f"   - æŸ¥è¯¢: {query[:50]}...")
        
        # ğŸ”§ æ£€æŸ¥æ˜¯å¦å¯ç”¨çœŸå®æœç´¢
        if enable_real_search and REAL_SEARCH_AVAILABLE:
            logger.info("ğŸŒ å°è¯•çœŸå®Firecrawlæœç´¢...")
            # å°è¯•çœŸå®æœç´¢
            response = self._search_firecrawl_real(query, max_results)
            
            # ğŸ”¥ å¢å¼ºç‰ˆï¼šå¦‚æœçœŸå®æœç´¢å¤±è´¥ï¼Œæ™ºèƒ½å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢
            if not response.success:
                logger.warning(f"ğŸš¨ FirecrawlçœŸå®æœç´¢å¤±è´¥ï¼ŒåŸå› : {response.error_message}")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢
                should_fallback = self._should_fallback_to_mock(response.error_message)
                
                if should_fallback:
                    logger.warning("ğŸš¨ å»ºè®®æš‚æ—¶åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæœç´¢æ¨¡å¼")
                    logger.info("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šåœ¨config.pyä¸­è®¾ç½® 'enable_real_web_search': False")
                    logger.info("ğŸ”„ è‡ªåŠ¨å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢...")
                    
                    # è‡ªåŠ¨å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢
                    mock_result = self._search_firecrawl_mock(query, max_results)
                    # åœ¨æ¨¡æ‹Ÿç»“æœä¸­æ·»åŠ å›é€€ä¿¡æ¯
                    if mock_result.success:
                        mock_result.error_message = f"å·²ä»çœŸå®æœç´¢å›é€€: {response.error_message}"
                        logger.info(f"âœ… æ¨¡æ‹Ÿæœç´¢å›é€€æˆåŠŸï¼Œè¿”å›{len(mock_result.results)}ä¸ªç»“æœ")
                    return mock_result
                else:
                    # ä¸å›é€€ï¼Œè¿”å›åŸå§‹é”™è¯¯
                    return response
            else:
                logger.info(f"âœ… çœŸå®æœç´¢æˆåŠŸï¼Œè¿”å›{len(response.results)}ä¸ªç»“æœ")
                return response
        else:
            if not enable_real_search:
                logger.info("ğŸ­ é…ç½®ç¦ç”¨çœŸå®æœç´¢ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæœç´¢æ¨¡å¼")
                return self._search_firecrawl_mock(query, max_results)
            elif not REAL_SEARCH_AVAILABLE:
                logger.warning("âš ï¸ firecrawlåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæœç´¢æ¨¡å¼")
                return self._search_firecrawl_mock(query, max_results)
    
    def _search_firecrawl_real(self, query: str, max_results: int) -> SearchResponse:
        """ğŸ”¥ çœŸå®çš„Firecrawlæœç´¢ - ä½¿ç”¨æ­£ç¡®çš„SSLé…ç½®å’Œæ™ºèƒ½é‡è¯•æœºåˆ¶"""
        max_retries = RAG_CONFIG.get("search_max_retries", 2)
        base_delay = RAG_CONFIG.get("search_retry_base_delay", 2.0)
        
        # è·å–Firecrawl APIå¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡æˆ–é…ç½®ï¼‰
        FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY") or RAG_CONFIG.get("firecrawl_api_key", "")
        
        last_error = None
        
        for attempt in range(max_retries):
            try:
                # æ™ºèƒ½é€Ÿç‡é™åˆ¶å¤„ç†
                _rate_limiter.wait_if_needed()
                
                # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´å»¶è¿Ÿç­–ç•¥
                if attempt > 0:
                    if last_error and "rate" in str(last_error).lower():
                        delay = min(base_delay * (3 ** attempt), 60.0)
                        logger.info(f"â³ é€Ÿç‡é™åˆ¶é‡è¯•ï¼Œç­‰å¾…{delay:.1f}ç§’...")
                    else:
                        delay = min(base_delay * (2 ** attempt), 30.0)
                        logger.info(f"â³ ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œç­‰å¾…{delay:.1f}ç§’...")
                    time.sleep(delay)
                
                logger.info(f"ğŸŒ å¼€å§‹çœŸå®Firecrawlæœç´¢: {query} (å°è¯• {attempt + 1}/{max_retries})")
                search_start_time = time.time()
                
                # ğŸ”¥ æ ¹æœ¬æ€§ä¿®å¤ï¼šé…ç½®Firecrawlä½¿ç”¨æˆ‘ä»¬çš„SSLé€‚é…å™¨
                import requests as firecrawl_requests
                
                # åˆ›å»ºä¸€ä¸ªé…ç½®å¥½SSLçš„session
                configured_session = requests.Session()
                configured_session.headers.update(self.session.headers)
                
                # æŒ‚è½½æˆ‘ä»¬çš„SSLé€‚é…å™¨
                ssl_adapter = SSLAdapter(
                    max_retries=3,
                    pool_connections=10,
                    pool_maxsize=10,
                )
                configured_session.mount('https://', ssl_adapter)
                configured_session.mount('http://', HTTPAdapter(max_retries=3))
                
                # ä¿å­˜åŸå§‹Sessionç±»
                original_session_class = firecrawl_requests.Session
                
                # åˆ›å»ºä¸€ä¸ªè¿”å›æˆ‘ä»¬é…ç½®å¥½çš„sessionçš„åŒ…è£…ç±»
                class ConfiguredSession:
                    def __new__(cls):
                        return configured_session
                
                # æ›¿æ¢requests.Session
                firecrawl_requests.Session = ConfiguredSession
                
                try:
                    # åˆå§‹åŒ–Firecrawlå®¢æˆ·ç«¯ï¼ˆä¼šä½¿ç”¨æˆ‘ä»¬é…ç½®å¥½çš„sessionï¼‰
                    firecrawl = FirecrawlApp(api_key=FIRECRAWL_API_KEY)
                    
                    # æ‰§è¡Œæœç´¢ï¼Œå¸¦è¶…æ—¶é…ç½®
                    search_response = firecrawl.search(query=query, limit=max_results)
                finally:
                    # æ¢å¤åŸå§‹Sessionç±»
                    firecrawl_requests.Session = original_session_class
                
                search_time = time.time() - search_start_time
                
                # è½¬æ¢ç»“æœæ ¼å¼
                results = []
                logger.debug(f"ğŸ” Firecrawlå“åº”ç±»å‹: {type(search_response)}")
                
                # å¤„ç† Firecrawl SearchResponse å¯¹è±¡
                if hasattr(search_response, 'success') and search_response.success and hasattr(search_response, 'data'):
                    search_data = search_response.data
                    logger.debug(f"ğŸ” æœç´¢æ•°æ®ç±»å‹: {type(search_data)}, é•¿åº¦: {len(search_data) if search_data else 0}")
                    
                    if search_data:  # search_data æ˜¯ä¸€ä¸ªåˆ—è¡¨
                        for result in search_data:
                            results.append(SearchResult(
                                title=result.get('title', ''),
                                snippet=result.get('snippet', result.get('description', '')),
                                url=result.get('url', ''),
                                relevance_score=0.8  # Firecrawlä¸æä¾›ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            ))
                
                # å…¼å®¹å­—å…¸æ ¼å¼å“åº”ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
                elif isinstance(search_response, dict) and 'data' in search_response:
                    search_data = search_response['data']
                    if isinstance(search_data, list):
                        for result in search_data:
                            results.append(SearchResult(
                                title=result.get('title', ''),
                                snippet=result.get('snippet', result.get('description', '')),
                                url=result.get('url', ''),
                                relevance_score=0.8
                            ))
                    elif isinstance(search_data, dict) and 'web' in search_data:
                        for result in search_data['web']:
                            results.append(SearchResult(
                                title=result.get('title', ''),
                                snippet=result.get('snippet', result.get('description', '')),
                                url=result.get('url', ''),
                                relevance_score=0.8
                            ))
                
                logger.info(f"âœ… çœŸå®æœç´¢æˆåŠŸ: æ‰¾åˆ°{len(results)}ä¸ªç»“æœï¼Œè€—æ—¶{search_time:.2f}ç§’")
                
                return SearchResponse(
                    query=query,
                    results=results,
                    total_results=len(results),
                    search_time=search_time,
                    success=True
                )
                
            except Exception as e:
                error_msg = str(e).lower()
                last_error = e
                
                # ğŸ”¥ å¢å¼ºç‰ˆï¼šæ›´ç²¾ç¡®çš„é”™è¯¯åˆ†ç±»å’Œå¤„ç†
                if any(keyword in error_msg for keyword in ['rate', 'limit', '429', 'too many']):
                    logger.warning(f"âš ï¸ é‡åˆ°é€Ÿç‡é™åˆ¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    
                    # ğŸ”¥ ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–ç­‰å¾…æ—¶é—´
                    wait_time = self._extract_retry_after_time(str(e))
                    if wait_time and wait_time > 0:
                        logger.info(f"ğŸ“‹ APIå»ºè®®ç­‰å¾…æ—¶é—´: {wait_time}ç§’")
                        # å¦‚æœAPIå»ºè®®çš„ç­‰å¾…æ—¶é—´åˆç†ï¼Œä½¿ç”¨å®ƒ
                        if wait_time <= 120:  # æœ€å¤šç­‰å¾…2åˆ†é’Ÿ
                            time.sleep(wait_time)
                    
                    if attempt < max_retries - 1:
                        continue  # ç»§ç»­é‡è¯•
                    else:
                        logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€Ÿç‡é™åˆ¶ä»ç„¶å­˜åœ¨")
                        
                elif any(keyword in error_msg for keyword in ['timeout', 'connection', 'network']):
                    logger.warning(f"âš ï¸ ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        continue  # ç½‘ç»œé”™è¯¯ä¹Ÿé‡è¯•
                    else:
                        logger.error(f"âŒ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                        
                elif any(keyword in error_msg for keyword in ['auth', '401', '403', 'unauthorized']):
                    logger.error(f"âŒ è®¤è¯é”™è¯¯ï¼Œåœæ­¢é‡è¯•: {e}")
                    break  # è®¤è¯é”™è¯¯ä¸é‡è¯•
                    
                else:
                    # å…¶ä»–æœªçŸ¥é”™è¯¯
                    logger.error(f"âŒ çœŸå®Firecrawlæœç´¢å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        continue  # æœªçŸ¥é”™è¯¯ä¹Ÿé‡è¯•ä¸€æ¬¡
                    else:
                        break
        
        # ğŸ”¥ å¢å¼ºç‰ˆï¼šæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®
        logger.error("âŒ æ‰€æœ‰Firecrawlæœç´¢å°è¯•éƒ½å¤±è´¥äº†")
        
        # ç”Ÿæˆè¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
        error_report = "Firecrawlæœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥"
        if last_error:
            error_msg = str(last_error).lower()
            if "rate" in error_msg or "limit" in error_msg:
                error_report = "Firecrawl APIé€Ÿç‡é™åˆ¶ï¼Œå»ºè®®ç¨åé‡è¯•æˆ–å‡çº§è®¡åˆ’"
            elif "auth" in error_msg:
                error_report = "Firecrawl APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥"
            elif "network" in error_msg or "connection" in error_msg:
                error_report = "ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œçŠ¶æ€"
        
        return SearchResponse(
            query=query,
            results=[],
            total_results=0,
            search_time=0.0,
            success=False,
            error_message=error_report
        )
    
    def _extract_retry_after_time(self, error_message: str) -> Optional[int]:
        """ğŸ”¥ æ–°å¢ï¼šä»é”™è¯¯æ¶ˆæ¯ä¸­æå–å»ºè®®çš„é‡è¯•ç­‰å¾…æ—¶é—´"""
        import re
        
        # å°è¯•åŒ¹é…å„ç§æ ¼å¼çš„é‡è¯•æ—¶é—´
        patterns = [
            r'retry after (\d+)s',
            r'please retry after (\d+)s',
            r'wait (\d+) seconds',
            r'resets at.*?(\d+)s'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, error_message.lower())
            if match:
                try:
                    return int(match.group(1))
                except (ValueError, IndexError):
                    continue
        
        return None
    
    def _should_fallback_to_mock(self, error_message: Optional[str]) -> bool:
        """ğŸ”¥ æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢"""
        if not error_message:
            return True  # æ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼Œé»˜è®¤å›é€€
        
        error_msg = error_message.lower()
        
        # ä»¥ä¸‹æƒ…å†µåº”è¯¥å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢
        fallback_conditions = [
            "rate" in error_msg and "limit" in error_msg,  # é€Ÿç‡é™åˆ¶
            "quota" in error_msg and "exceeded" in error_msg,  # é…é¢è¶…é™
            "429" in error_msg,  # HTTP 429é”™è¯¯
            "service unavailable" in error_msg,  # æœåŠ¡ä¸å¯ç”¨
            "timeout" in error_msg,  # è¶…æ—¶
            "network" in error_msg,  # ç½‘ç»œé—®é¢˜
        ]
        
        # ä»¥ä¸‹æƒ…å†µä¸åº”è¯¥å›é€€ï¼ˆéœ€è¦ç”¨æˆ·ä¿®å¤ï¼‰
        no_fallback_conditions = [
            "auth" in error_msg,  # è®¤è¯é—®é¢˜
            "401" in error_msg,   # æœªæˆæƒ
            "403" in error_msg,   # ç¦æ­¢è®¿é—®
            "api key" in error_msg,  # APIå¯†é’¥é—®é¢˜
        ]
        
        # æ£€æŸ¥ä¸å›é€€æ¡ä»¶
        for condition in no_fallback_conditions:
            if condition:
                return False
        
        # æ£€æŸ¥å›é€€æ¡ä»¶
        for condition in fallback_conditions:
            if condition:
                return True
        
        # é»˜è®¤æƒ…å†µä¸‹å›é€€ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        return True
    
    def _search_firecrawl_mock(self, query: str, max_results: int) -> SearchResponse:
        """ğŸ”¥ æ–°å¢ï¼šæ¨¡æ‹ŸFirecrawlæœç´¢ç»“æœ"""
        logger.info(f"ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢æ¨¡å¼: {query}")
        
        # æ¨¡æ‹Ÿæœç´¢å»¶è¿Ÿ
        time.sleep(0.5)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿç»“æœ
        mock_results = []
        
        # æ ¹æ®æŸ¥è¯¢ç”Ÿæˆç›¸å…³çš„æ¨¡æ‹Ÿç»“æœ
        if "å®ç”¨åŠ¡å®" in query or "practical" in query.lower():
            mock_results = [
                SearchResult(
                    title="å®ç”¨ä¸»ä¹‰æ–¹æ³•è®º - æ³¨é‡å®é™…æ•ˆæœçš„è§£å†³æ–¹æ¡ˆ",
                    snippet="å®ç”¨åŠ¡å®å‹æ€ç»´å¼ºè°ƒä»¥ç»“æœä¸ºå¯¼å‘ï¼Œæ³¨é‡å¯è¡Œæ€§å’Œç«‹å³æ‰§è¡Œã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºéœ€è¦å¿«é€Ÿè§£å†³çš„å®é™…é—®é¢˜ï¼Œé€šè¿‡åŠ¡å®çš„ç­–ç•¥æ¥è¾¾æˆç›®æ ‡ã€‚",
                    url="https://example.com/practical-methodology",
                    relevance_score=0.9
                ),
                SearchResult(
                    title="å¿«é€Ÿæ‰§è¡Œç­–ç•¥ï¼šä»æƒ³æ³•åˆ°å®ç°",
                    snippet="æ¢è®¨å¦‚ä½•å°†æŠ½è±¡æ¦‚å¿µè½¬åŒ–ä¸ºå…·ä½“çš„æ‰§è¡Œè®¡åˆ’ï¼ŒåŒ…æ‹¬ä¼˜å…ˆçº§æ’åºã€èµ„æºåˆ†é…å’Œé£é™©è¯„ä¼°ç­‰å…³é”®è¦ç´ ã€‚",
                    url="https://example.com/execution-strategy",
                    relevance_score=0.8
                ),
                SearchResult(
                    title="å®é™…é—®é¢˜è§£å†³æ¡†æ¶",
                    snippet="æä¾›ä¸€å¥—ç³»ç»ŸåŒ–çš„é—®é¢˜è§£å†³æ–¹æ³•ï¼Œå¼ºè°ƒå®ç”¨æ€§å’Œå¯æ“ä½œæ€§ï¼Œå¸®åŠ©å¿«é€Ÿè¯†åˆ«å’Œè§£å†³å®é™…æŒ‘æˆ˜ã€‚",
                    url="https://example.com/problem-solving-framework",
                    relevance_score=0.7
                )
            ]
        else:
            # é€šç”¨æ¨¡æ‹Ÿç»“æœ
            mock_results = [
                SearchResult(
                    title=f"å…³äº'{query[:30]}'çš„ç»¼åˆåˆ†æ",
                    snippet=f"è¿™æ˜¯å…³äº{query[:30]}çš„è¯¦ç»†åˆ†æå’Œè§£å†³æ–¹æ¡ˆï¼ŒåŒ…å«å¤šä¸ªè§’åº¦çš„æ·±å…¥æ¢è®¨ã€‚",
                    url="https://example.com/analysis",
                    relevance_score=0.8
                ),
                SearchResult(
                    title=f"{query[:30]} - å®æ–½æŒ‡å—",
                    snippet=f"æä¾›{query[:30]}çš„å…·ä½“å®æ–½æ­¥éª¤å’Œæœ€ä½³å®è·µï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ã€‚",
                    url="https://example.com/guide",
                    relevance_score=0.7
                ),
                SearchResult(
                    title=f"{query[:30]} - æ¡ˆä¾‹ç ”ç©¶",
                    snippet=f"é€šè¿‡çœŸå®æ¡ˆä¾‹å±•ç¤º{query[:30]}çš„åº”ç”¨åœºæ™¯å’Œæ•ˆæœï¼Œä¸ºæ‚¨æä¾›å‚è€ƒã€‚",
                    url="https://example.com/case-study",
                    relevance_score=0.6
                )
            ]
        
        # é™åˆ¶ç»“æœæ•°é‡
        mock_results = mock_results[:max_results]
        
        logger.info(f"ğŸ­ æ¨¡æ‹Ÿæœç´¢å®Œæˆ: ç”Ÿæˆ{len(mock_results)}ä¸ªç»“æœ")
        
        return SearchResponse(
            query=query,
            results=mock_results,
            total_results=len(mock_results),
            search_time=0.5,
            success=True,
            error_message=""
        )
    
    
    def _search_bing(self, query: str, max_results: int) -> SearchResponse:
        """ä½¿ç”¨Bingæœç´¢ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
        logger.error("âŒ Bingæœç´¢æœªå®ç°")
        return SearchResponse(
            query=query,
            results=[],
            total_results=0,
            search_time=0.0,
            success=False,
            error_message="Bingæœç´¢åŠŸèƒ½æœªå®ç°ï¼Œè¯·ä½¿ç”¨Firecrawlæœç´¢"
        )
    
    def _update_search_stats(self, search_time: float, success: bool):
        """æ›´æ–°æœç´¢ç»Ÿè®¡"""
        self.search_stats['total_searches'] += 1
        if success:
            self.search_stats['successful_searches'] += 1
        
        self.search_stats['total_search_time'] += search_time
        self.search_stats['avg_search_time'] = (
            self.search_stats['total_search_time'] / self.search_stats['total_searches']
        )
    
    def get_search_stats(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return self.search_stats.copy()

class IdeaVerificationSearchClient:
    """æƒ³æ³•éªŒè¯ä¸“ç”¨æœç´¢å®¢æˆ·ç«¯"""
    
    def __init__(self, web_search_client: WebSearchClient, semantic_analyzer=None):
        """
        åˆå§‹åŒ–æƒ³æ³•éªŒè¯æœç´¢å®¢æˆ·ç«¯
        
        Args:
            web_search_client: ç½‘ç»œæœç´¢å®¢æˆ·ç«¯
            semantic_analyzer: è¯­ä¹‰åˆ†æå™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.web_search_client = web_search_client
        self.semantic_analyzer = semantic_analyzer
        self.verification_cache = {}  # éªŒè¯ç»“æœç¼“å­˜
        
        logger.info("ğŸ” IdeaVerificationSearchClientåˆå§‹åŒ–å®Œæˆ")
    
    def search_for_idea_verification(self, idea_text: str, context: Optional[Dict] = None) -> SearchResponse:
        """
        ä¸ºæƒ³æ³•éªŒè¯è¿›è¡Œä¸“é—¨çš„æœç´¢
        
        Args:
            idea_text: éœ€è¦éªŒè¯çš„æƒ³æ³•æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            SearchResponse: æœç´¢å“åº”
        """
        logger.info(f"ğŸ” [éªŒè¯æœç´¢] å¼€å§‹ä¸ºæƒ³æ³•éªŒè¯è¿›è¡Œæœç´¢")
        logger.info(f"ğŸ” [éªŒè¯æœç´¢] æƒ³æ³•æ–‡æœ¬é•¿åº¦: {len(idea_text)} å­—ç¬¦")
        logger.info(f"ğŸ” [éªŒè¯æœç´¢] ä¸Šä¸‹æ–‡: {context}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{idea_text}_{hash(str(context))}"
        if cache_key in self.verification_cache:
            logger.info(f"âœ… [éªŒè¯æœç´¢] ä½¿ç”¨ç¼“å­˜çš„æœç´¢ç»“æœ")
            return self.verification_cache[cache_key]
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        logger.info(f"ğŸ” [éªŒè¯æœç´¢] æ„å»ºæœç´¢æŸ¥è¯¢...")
        search_query = self._build_verification_query(idea_text, context)
        logger.info(f"ğŸ” [éªŒè¯æœç´¢] æœç´¢æŸ¥è¯¢: {search_query}")
        
        # æ‰§è¡Œæœç´¢
        logger.info(f"ğŸ” [éªŒè¯æœç´¢] è°ƒç”¨WebSearchClientè¿›è¡Œæœç´¢...")
        try:
            search_response = self.web_search_client.search(search_query, max_results=5)
            
            if search_response.success:
                logger.info(f"âœ… [éªŒè¯æœç´¢] æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(search_response.results)} ä¸ªç»“æœ")
                # ç¼“å­˜ç»“æœ
                self.verification_cache[cache_key] = search_response
            else:
                logger.warning(f"âš ï¸ [éªŒè¯æœç´¢] æœç´¢å¤±è´¥: {search_response.error_message}")
            
            return search_response
            
        except Exception as e:
            logger.error(f"âŒ [éªŒè¯æœç´¢] æœç´¢è¿‡ç¨‹å‡ºé”™: {e}")
            # è¿”å›ç©ºçš„æœç´¢å“åº”
            return SearchResponse(
                query=search_query,
                results=[],
                total_results=0,
                search_time=0.0,
                success=False,
                error_message=str(e)
            )
    
    def _build_verification_query(self, idea_text: str, context: Optional[Dict] = None) -> str:
        """
        æ„å»ºç”¨äºæƒ³æ³•éªŒè¯çš„æœç´¢æŸ¥è¯¢
        
        Args:
            idea_text: æƒ³æ³•æ–‡æœ¬ï¼ˆæ€ç»´ç§å­ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            str: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # ğŸ”¥ è°ƒè¯•ï¼šæ‰“å°ä¸Šä¸‹æ–‡ä¿¡æ¯
        logger.info(f"ğŸ” _build_verification_query æ¥æ”¶åˆ°çš„context: {context}")
        
        # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨LLMæ•´åˆæ€ç»´ç§å­å’Œç”¨æˆ·æŸ¥è¯¢
        user_query = None
        if context:
            user_query = context.get('user_query') or context.get('original_query')
            logger.info(f"ğŸ” ä»contextä¸­æå–çš„user_query: {user_query}")
        
        if user_query and self.semantic_analyzer:
            # ä½¿ç”¨LLMæ•´åˆæ€ç»´ç§å­å’Œç”¨æˆ·åŸå§‹æŸ¥è¯¢
            logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ç”¨æˆ·æŸ¥è¯¢ï¼Œå°è¯•LLMæ•´åˆ: {user_query[:30]}...")
            integrated_query = self._llm_integrate_seed_and_query(idea_text, user_query, context)
            logger.info(f"ğŸ§  LLMæ•´åˆæŸ¥è¯¢ç»“æœ: {integrated_query}")
            return integrated_query
        else:
            if not user_query:
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°ç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            if not self.semantic_analyzer:
                logger.warning("âš ï¸ è¯­ä¹‰åˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
        
        # ğŸ”¥ ä¿®å¤ï¼šå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢
        # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦æœ‰ç”¨æˆ·æŸ¥è¯¢
        if context and ('user_query' in context or 'original_query' in context):
            user_query_fallback = context.get('user_query') or context.get('original_query')
            if user_query_fallback:
                logger.info(f"ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿå›é€€æ–¹æ³•ï¼ŒåŸºäºç”¨æˆ·æŸ¥è¯¢: {user_query_fallback[:40]}")
                return self._fallback_integrate_query(idea_text, user_query_fallback, context)
        
        # å¦‚æœæ²¡æœ‰ç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬æˆªå–
        # ä½¿ç”¨åŸå§‹æ–‡æœ¬çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæŸ¥è¯¢
        query = idea_text[:50].strip()
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰å…·ä½“é¢†åŸŸï¼‰
        if context and 'domain' in context:
            domain = context['domain']
            # åªæœ‰åœ¨domainä¸æ˜¯é€šç”¨è¯æ—¶æ‰æ·»åŠ 
            if domain and domain not in ['general', 'unknown', 'é€šç”¨']:
                query = f"{query} {domain}"
        
        logger.debug(f"ğŸ” æ„å»ºéªŒè¯æŸ¥è¯¢ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰: {query}")
        return query
    
    
    def _llm_integrate_seed_and_query(self, thinking_seed: str, user_query: str, context: Optional[Dict] = None) -> str:
        """
        ä½¿ç”¨LLMæ•´åˆæ€ç»´ç§å­å’Œç”¨æˆ·åŸå§‹æŸ¥è¯¢ï¼Œç”Ÿæˆæ›´ç›¸å…³çš„æœç´¢æŸ¥è¯¢
        
        ğŸ¯ æ ¸å¿ƒåŸåˆ™ï¼šæœç´¢æŸ¥è¯¢å¿…é¡»èšç„¦äºç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒå†…å®¹ï¼Œè€Œä¸æ˜¯æ€ç»´è·¯å¾„çš„æ–¹æ³•è®º
        
        Args:
            thinking_seed: ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„æ€ç»´ç§å­ï¼ˆä»…ä½œä¸ºè¾…åŠ©å‚è€ƒï¼‰
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢ï¼ˆä¸»è¦ä¿¡æ¯æ¥æºï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            str: LLMæ•´åˆåçš„æœç´¢æŸ¥è¯¢
        """
        try:
            # å…³é”®ä¿®å¤ï¼šæ³¨å…¥å½“å‰æ—¶é—´ä¿¡æ¯
            from datetime import datetime
            now = datetime.now()
            current_year = now.year
            current_time_info = f"""
ğŸ“… **é‡è¦æ—¶é—´ä¿¡æ¯** (ç”Ÿæˆæœç´¢æŸ¥è¯¢æ—¶å¿…é¡»å‚è€ƒ):
- å½“å‰å¹´ä»½: {current_year}å¹´
- å½“å‰æ—¥æœŸ: {now.strftime('%Yå¹´%mæœˆ%dæ—¥')}
"""
            
            # ğŸ”¥ ä¿®å¤åçš„LLMæç¤º - å¼ºè°ƒæœç´¢ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒå†…å®¹
            integration_prompt = f"""ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªç²¾å‡†çš„æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚

{current_time_info}

**ç”¨æˆ·çš„é—®é¢˜ï¼š**
{user_query}

**AIå»ºè®®çš„æ€è€ƒè§’åº¦ï¼ˆä»…ä¾›å‚è€ƒï¼‰ï¼š**
{thinking_seed[:200]}

**ğŸ¯ æ ¸å¿ƒä»»åŠ¡ï¼š**
ç”Ÿæˆä¸€ä¸ªç½‘ç»œæœç´¢æŸ¥è¯¢ï¼Œç”¨äºæŸ¥æ‰¾èƒ½å¤Ÿ**ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜**çš„ä¿¡æ¯ã€‚

**âš ï¸ å…³é”®åŸåˆ™ï¼š**
1. âœ… **å¿…é¡»åš**ï¼šæå–ç”¨æˆ·æŸ¥è¯¢ä¸­çš„æ ¸å¿ƒä¸»é¢˜ã€å…³é”®å®ä½“ã€å…·ä½“é—®é¢˜
   - ä¾‹å¦‚ï¼šç”¨æˆ·é—®"ChatGPTæœ€æ–°æ¨¡å‹"ï¼Œåº”æœç´¢"ChatGPT æ¨¡å‹ {current_year}"
   - ä¾‹å¦‚ï¼šç”¨æˆ·é—®"Pythonçˆ¬è™«æŠ€æœ¯"ï¼Œåº”æœç´¢"Python çˆ¬è™«æŠ€æœ¯ æ•™ç¨‹"

2. âŒ **ç¦æ­¢åš**ï¼šæœç´¢æ€ç»´ç§å­ä¸­çš„æŠ½è±¡æ–¹æ³•è®ºè¯æ±‡
   - ç¦æ­¢æœç´¢ï¼š"ç³»ç»Ÿåˆ†ææ–¹æ³•"ã€"å®ç”¨åŠ¡å®å‹è§£å†³æ–¹æ¡ˆ"ã€"æ‰¹åˆ¤æ€§æ€ç»´"ç­‰
   - ç¦æ­¢æœç´¢ï¼š"å¯è¡Œæ€§"ã€"éªŒè¯"ã€"è¯„ä¼°"ç­‰éªŒè¯ç±»è¯æ±‡

3. ğŸ¯ **æœç´¢ç›®æ ‡**ï¼šæ‰¾åˆ°èƒ½ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„äº‹å®ã€æ•°æ®ã€æ¡ˆä¾‹ã€æ•™ç¨‹
   - æœç´¢ç”¨æˆ·æƒ³çŸ¥é“çš„**å…·ä½“å†…å®¹**ï¼Œè€Œä¸æ˜¯å¦‚ä½•æ€è€ƒè¿™ä¸ªé—®é¢˜

4. â° **æ—¶é—´å¤„ç†**ï¼š
   - å¦‚æœç”¨æˆ·é—®"æœ€æ–°"ã€"å½“å‰"ã€"{current_year}å¹´"ï¼Œæœç´¢æŸ¥è¯¢å¿…é¡»åŒ…å«å½“å‰å¹´ä»½
   - é¿å…ä½¿ç”¨è¿‡æ—¶çš„å¹´ä»½ä¿¡æ¯

**è¾“å‡ºè¦æ±‚ï¼š**
- ç›´æ¥è¾“å‡ºä¸€ä¸ªç®€æ´çš„æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆ30å­—ä»¥å†…ï¼‰
- ä¸è¦è§£é‡Šï¼Œä¸è¦æ·»åŠ å¼•å·
- ä¸“æ³¨äºç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒå…³é”®è¯

æœç´¢æŸ¥è¯¢ï¼š"""

            # è°ƒç”¨è¯­ä¹‰åˆ†æå™¨çš„LLMåŠŸèƒ½
            if hasattr(self.semantic_analyzer, 'llm_manager') and self.semantic_analyzer.llm_manager:
                logger.info("ğŸ”§ ä½¿ç”¨è¯­ä¹‰åˆ†æå™¨çš„LLMç®¡ç†å™¨è¿›è¡Œæ•´åˆ")
                try:
                    # ä½¿ç”¨LLMç®¡ç†å™¨çš„chat_completionæ–¹æ³•
                    response = self.semantic_analyzer.llm_manager.chat_completion(
                        messages=[{"role": "user", "content": integration_prompt}],
                        temperature=0.1,
                        max_tokens=200
                    )
                    
                    if response and response.content and response.content.strip():
                        integrated_query = response.content.strip()
                        logger.info(f"ğŸ§  LLMæˆåŠŸæ•´åˆæŸ¥è¯¢: {user_query[:30]}... -> {integrated_query}")
                        return integrated_query
                    else:
                        logger.warning("âš ï¸ LLMè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨å›é€€æ–¹æ³•")
                except Exception as e:
                    logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            else:
                logger.warning("âš ï¸ è¯­ä¹‰åˆ†æå™¨ç¼ºå°‘LLMç®¡ç†å™¨ï¼Œä½¿ç”¨å›é€€æ–¹æ³•")
                
        except Exception as e:
            logger.error(f"âŒ LLMæ•´åˆæŸ¥è¯¢å¤±è´¥: {e}")
        
        # å›é€€æ–¹æ³•ï¼šç®€å•ç»„åˆç”¨æˆ·æŸ¥è¯¢å’Œæ€ç»´ç§å­å…³é”®è¯
        return self._fallback_integrate_query(thinking_seed, user_query, context)
    
    def _fallback_integrate_query(self, thinking_seed: str, user_query: str, context: Optional[Dict] = None) -> str:
        """
        å›é€€æ–¹æ³•ï¼šç®€å•æ•´åˆæ€ç»´ç§å­å’Œç”¨æˆ·æŸ¥è¯¢
        
        ğŸ¯ æ ¸å¿ƒåŸåˆ™ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢çš„æ ¸å¿ƒå†…å®¹ï¼Œé¿å…æ·»åŠ æŠ½è±¡çš„æ–¹æ³•è®ºè¯æ±‡
        
        Args:
            thinking_seed: æ€ç»´ç§å­ï¼ˆè¾…åŠ©å‚è€ƒï¼‰
            user_query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆä¸»è¦æ¥æºï¼‰
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            str: æ•´åˆåçš„æœç´¢æŸ¥è¯¢
        """
        import re
        from datetime import datetime
        
        # ğŸ”¥ ä¿®å¤ï¼šç›´æ¥æå–ç”¨æˆ·æŸ¥è¯¢çš„æ ¸å¿ƒå†…å®¹
        # ç§»é™¤å¸¸è§çš„æ–¹æ³•è®ºè¯æ±‡ï¼Œä¿ç•™å®é™…é—®é¢˜
        method_keywords_to_remove = [
            'å®ç”¨åŠ¡å®å‹', 'è§£å†³æ–¹æ¡ˆ', 'ç³»ç»Ÿåˆ†æ', 'æ‰¹åˆ¤æ€§æ€ç»´', 'æ¢ç´¢æ€§ç ”ç©¶',
            'åˆ›æ–°æ€ç»´', 'æ–¹æ³•', 'ç­–ç•¥', 'æ€è·¯', 'æ–¹æ¡ˆ', 'è§’åº¦', 'é€”å¾„'
        ]
        
        # æ¸…ç†ç”¨æˆ·æŸ¥è¯¢
        cleaned_query = user_query
        for keyword in method_keywords_to_remove:
            cleaned_query = cleaned_query.replace(keyword, ' ')
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        cleaned_query = ' '.join(cleaned_query.split())
        
        # æå–å…³é”®å®ä½“å’Œä¸»é¢˜è¯ï¼ˆä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ï¼‰
        # ä¼˜å…ˆä¿ç•™ï¼šä¸“æœ‰åè¯ã€æŠ€æœ¯æœ¯è¯­ã€å…·ä½“æ¦‚å¿µ
        core_keywords = re.findall(r'[A-Z][a-zA-Z]+|[a-z]+|[\u4e00-\u9fa5]{2,}|\d+', cleaned_query)
        
        # å¦‚æœæå–çš„å…³é”®è¯å¤ªå°‘ï¼Œç›´æ¥ä½¿ç”¨æ¸…ç†åçš„æŸ¥è¯¢
        if len(core_keywords) < 2:
            query = cleaned_query[:60].strip()
        else:
            # é‡å»ºæŸ¥è¯¢ï¼Œä¿ç•™æ ¸å¿ƒå…³é”®è¯
            query = ' '.join(core_keywords[:8])  # æœ€å¤š8ä¸ªå…³é”®è¯
        
        # æ£€æŸ¥æ—¶é—´ç›¸å…³è¯æ±‡ï¼Œæ·»åŠ å½“å‰å¹´ä»½
        time_keywords = ['æœ€æ–°', 'å½“å‰', 'ä»Šå¹´', 'ç°åœ¨', 'æ–°ç‰ˆ', 'æœ€è¿‘']
        has_time_keyword = any(kw in user_query for kw in time_keywords)
        
        if has_time_keyword:
            current_year = datetime.now().year
            # é¿å…é‡å¤æ·»åŠ å¹´ä»½
            if str(current_year) not in query:
                query = f"{query} {current_year}"
        
        # ğŸ¯ æœ€ç»ˆæŸ¥è¯¢ï¼šä¼˜å…ˆä½¿ç”¨æ ¸å¿ƒå†…å®¹ï¼Œé¿å…æ·»åŠ "æ–¹æ³•"ã€"å®è·µ"ç­‰é€šç”¨è¯
        query = query.strip()
        
        logger.info(f"ğŸ”„ å›é€€æ•´åˆæŸ¥è¯¢: {user_query[:40]} -> {query}")
        return query
    
    def verify_idea_feasibility(self, idea_text: str, context: Optional[Dict] = None) -> IdeaVerificationResult:
        """
        éªŒè¯æƒ³æ³•çš„å¯è¡Œæ€§
        
        Args:
            idea_text: éœ€è¦éªŒè¯çš„æƒ³æ³•æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            IdeaVerificationResult: éªŒè¯ç»“æœ
        """
        try:
            logger.info(f"ğŸ” [å¯è¡Œæ€§éªŒè¯] å¼€å§‹éªŒè¯æƒ³æ³•å¯è¡Œæ€§: {idea_text[:50]}...")
            logger.info(f"ğŸ” [å¯è¡Œæ€§éªŒè¯] ä¸Šä¸‹æ–‡: {context}")
            
            # æ‰§è¡Œä¸“é—¨çš„éªŒè¯æœç´¢
            logger.info(f"ğŸ” [å¯è¡Œæ€§éªŒè¯] è°ƒç”¨search_for_idea_verification...")
            search_response = self.search_for_idea_verification(idea_text, context)
            logger.info(f"ğŸ” [å¯è¡Œæ€§éªŒè¯] æœç´¢å®Œæˆï¼ŒæˆåŠŸ: {search_response.success}")
            logger.info(f"ğŸ” [å¯è¡Œæ€§éªŒè¯] æœç´¢ç»“æœæ•°: {len(search_response.results)}")
            
            if not search_response.success:
                # ğŸ”¥ å¢å¼ºç‰ˆï¼šæœç´¢å¤±è´¥æ—¶æä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
                logger.warning(f"âš ï¸ [å¯è¡Œæ€§éªŒè¯] æœç´¢å¤±è´¥ï¼Œä½¿ç”¨å›é€€åˆ†æ: {search_response.error_message}")
                
                # å³ä½¿æœç´¢å¤±è´¥ï¼Œä¹Ÿå°è¯•æä¾›åŸºç¡€çš„å¯è¡Œæ€§åˆ†æ
                fallback_analysis = self._generate_fallback_analysis(idea_text, search_response.error_message)
                
                return IdeaVerificationResult(
                    idea_text=idea_text,
                    feasibility_score=fallback_analysis['score'],
                    analysis_summary=fallback_analysis['summary'],
                    search_results=[],
                    success=True,  # ğŸ”¥ æ ‡è®°ä¸ºæˆåŠŸï¼Œå› ä¸ºæˆ‘ä»¬æä¾›äº†å›é€€åˆ†æ
                    error_message=f"æœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œæä¾›åŸºç¡€åˆ†æ: {search_response.error_message}"
                )
            
            # æå–ç”¨æˆ·æŸ¥è¯¢ç”¨äºLLMè¯„åˆ†
            user_query = None
            if context:
                user_query = context.get('user_query') or context.get('original_query') or context.get('query')
            
            # åˆ†ææœç´¢ç»“æœè®¡ç®—å¯è¡Œæ€§åˆ†æ•°ï¼ˆä¼ å…¥user_queryç”¨äºLLMè¯„åˆ†ï¼‰
            feasibility_score = self._calculate_feasibility_score(
                search_response.results, 
                idea_text, 
                user_query=user_query,
                context=context
            )
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            analysis_summary = self._generate_analysis_summary(search_response.results, idea_text, feasibility_score)
            
            logger.info(f"âœ… æƒ³æ³•éªŒè¯å®Œæˆ - å¯è¡Œæ€§åˆ†æ•°: {feasibility_score:.2f}")
            
            return IdeaVerificationResult(
                idea_text=idea_text,
                feasibility_score=feasibility_score,
                analysis_summary=analysis_summary,
                search_results=search_response.results,
                success=True,
                error_message=""
            )
            
        except Exception as e:
            logger.error(f"âŒ æƒ³æ³•éªŒè¯å¤±è´¥: {e}")
            return IdeaVerificationResult(
                idea_text=idea_text,
                feasibility_score=0.0,
                analysis_summary=f"éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}",
                search_results=[],
                success=False,
                error_message=str(e)
            )
    
    def _calculate_feasibility_score(self, search_results: List[SearchResult], idea_text: str, 
                                     user_query: str = None, context: Optional[Dict] = None) -> float:
        """
        ğŸ”¥ å®Œå…¨åŸºäºLLMè¯­ä¹‰åˆ†æè®¡ç®—å¯è¡Œæ€§åˆ†æ•°
        
        æ”¹è¿›ï¼š
        1. å®Œå…¨ä¾èµ–LLMè¯­ä¹‰ç›¸å…³åº¦è¯„åˆ†ï¼ˆæ‘’å¼ƒå…³é”®è¯åŒ¹é…ï¼‰
        2. æ ¹æ®é—®é¢˜ç±»å‹è¿›è¡Œç²¾å‡†è¯„ä¼°
        3. LLMä¸å¯ç”¨æ—¶ä½¿ç”¨ä¿å®ˆçš„é»˜è®¤åˆ†æ•°
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            idea_text: æƒ³æ³•æ–‡æœ¬
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºLLMè¯„åˆ†ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            float: å¯è¡Œæ€§åˆ†æ•° (0.0-1.0)
        """
        if not search_results:
            return 0.1  # å¦‚æœæ²¡æœ‰æœç´¢ç»“æœï¼Œç»™ä¸€ä¸ªå¾ˆä½çš„åˆ†æ•°
        
        # ğŸ¯ æ£€æµ‹é—®é¢˜ç±»å‹ï¼ˆçŸ¥è¯†ç§‘æ™® vs æŠ€æœ¯å®ç°ï¼‰
        query_type = self._detect_query_type(user_query or idea_text)
        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°é—®é¢˜ç±»å‹: {query_type}")
        
        # ğŸ”¥ ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç›¸å…³åº¦è¯„åˆ†
        llm_semantic_score = self._calculate_llm_semantic_relevance(
            search_results, idea_text, user_query, context
        )
        
        if llm_semantic_score is not None:
            # LLMå¯ç”¨ï¼šç›´æ¥ä½¿ç”¨è¯­ä¹‰è¯„åˆ†
            final_score = llm_semantic_score
            logger.info(f"âœ… LLMè¯­ä¹‰è¯„åˆ†: {final_score:.3f} (é—®é¢˜ç±»å‹: {query_type})")
        else:
            # LLMä¸å¯ç”¨ï¼šä½¿ç”¨ä¿å®ˆçš„é»˜è®¤è¯„åˆ†
            # åŸºäºæœç´¢ç»“æœæ•°é‡ç»™å‡ºä¿å®ˆä¼°è®¡
            result_count = len(search_results)
            if result_count >= 5:
                final_score = 0.6  # æœ‰è¶³å¤Ÿå¤šç»“æœï¼Œä¸­ç­‰åä¸Š
            elif result_count >= 3:
                final_score = 0.5  # æœ‰ä¸€äº›ç»“æœï¼Œä¸­ç­‰
            else:
                final_score = 0.4  # ç»“æœè¾ƒå°‘ï¼Œä¸­ç­‰åä¸‹
            logger.warning(f"âš ï¸ LLMä¸å¯ç”¨ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤è¯„åˆ†: {final_score:.3f} (åŸºäº{result_count}ä¸ªæœç´¢ç»“æœ)")
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        final_score = max(0.0, min(1.0, final_score))
        
        return final_score
    
    def _detect_query_type(self, text: str) -> str:
        """
        æ£€æµ‹æŸ¥è¯¢ç±»å‹
        
        Returns:
            "knowledge": çŸ¥è¯†ç§‘æ™®ç±»ï¼ˆäº†è§£ã€å­¦ä¹ ã€ä»‹ç»ï¼‰
            "implementation": æŠ€æœ¯å®ç°ç±»ï¼ˆå®ç°ã€å¼€å‘ã€æ„å»ºï¼‰
            "general": é€šç”¨ç±»å‹
        """
        text_lower = text.lower()
        
        # çŸ¥è¯†ç§‘æ™®ç±»å…³é”®è¯
        knowledge_keywords = [
            'äº†è§£', 'å­¦ä¹ ', 'çŸ¥è¯†', 'ä»€ä¹ˆæ˜¯', 'ä»‹ç»', 'è§£é‡Š', 'ç†è§£', 'è®¤è¯†',
            'learn', 'know', 'understand', 'what is', 'introduce', 'explain'
        ]
        
        # æŠ€æœ¯å®ç°ç±»å…³é”®è¯
        implementation_keywords = [
            'å®ç°', 'å¼€å‘', 'æ„å»º', 'è®¾è®¡', 'åˆ›å»º', 'æ­å»º', 'ç¼–å†™', 'åˆ¶ä½œ',
            'implement', 'develop', 'build', 'create', 'design', 'code', 'make'
        ]
        
        knowledge_matches = sum(1 for kw in knowledge_keywords if kw in text_lower)
        implementation_matches = sum(1 for kw in implementation_keywords if kw in text_lower)
        
        if knowledge_matches > implementation_matches and knowledge_matches > 0:
            return "knowledge"
        elif implementation_matches > knowledge_matches and implementation_matches > 0:
            return "implementation"
        else:
            return "general"
    
    def _calculate_llm_semantic_relevance(self, search_results: List[SearchResult], 
                                          idea_text: str, user_query: str = None,
                                          context: Optional[Dict] = None) -> Optional[float]:
        """
        ğŸ”¥ ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç›¸å…³åº¦è¯„åˆ†ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            idea_text: æƒ³æ³•æ–‡æœ¬
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            Optional[float]: è¯­ä¹‰ç›¸å…³åº¦åˆ†æ•° (0.0-1.0)ï¼Œå¦‚æœLLMä¸å¯ç”¨åˆ™è¿”å›None
        """
        # æ£€æŸ¥è¯­ä¹‰åˆ†æå™¨æ˜¯å¦å¯ç”¨
        if not self.semantic_analyzer or not hasattr(self.semantic_analyzer, 'llm_manager'):
            logger.warning("âš ï¸ è¯­ä¹‰åˆ†æå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡LLMè¯„åˆ†")
            return None
        
        if not self.semantic_analyzer.llm_manager:
            logger.warning("âš ï¸ LLMç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡LLMè¯„åˆ†")
            return None
        
        try:
            # å‡†å¤‡æœç´¢ç»“æœæ‘˜è¦ï¼ˆå–å‰3ä¸ªç»“æœï¼‰
            search_summary = self._format_search_results_for_llm(search_results[:3])
            
            # æ„å»ºLLMè¯„åˆ†æç¤ºè¯
            evaluation_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯ç›¸å…³æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°æœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚

**ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š**
{user_query or idea_text}

**AIæ€è€ƒè¦ç‚¹ï¼š**
{idea_text[:300]}

**æœç´¢ç»“æœæ‘˜è¦ï¼š**
{search_summary}

**è¯„ä¼°ç»´åº¦ï¼š**
è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦è¯„ä¼°æœç´¢ç»“æœçš„è´¨é‡ï¼ˆæ¯ä¸ªç»´åº¦0.0-1.0åˆ†ï¼‰ï¼š

1. **å†…å®¹ç›¸å…³æ€§ (relevance)**ï¼šæœç´¢ç»“æœæ˜¯å¦ç›´æ¥å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ
   - 1.0: å®Œå…¨ç›¸å…³ï¼Œç›´æ¥å›ç­”é—®é¢˜
   - 0.7: é«˜åº¦ç›¸å…³ï¼Œæä¾›äº†æœ‰ç”¨ä¿¡æ¯
   - 0.4: éƒ¨åˆ†ç›¸å…³ï¼Œæœ‰ä¸€äº›å…³è”
   - 0.0: å®Œå…¨ä¸ç›¸å…³

2. **ä¿¡æ¯è´¨é‡ (quality)**ï¼šæœç´¢ç»“æœçš„å†…å®¹è´¨é‡å¦‚ä½•ï¼Ÿ
   - 1.0: ä¿¡æ¯è¯¦ç»†ã€æƒå¨ã€å‡†ç¡®
   - 0.7: ä¿¡æ¯è¾ƒä¸ºå®Œæ•´å’Œå¯é 
   - 0.4: ä¿¡æ¯ç®€å•ä½†åŸºæœ¬å‡†ç¡®
   - 0.0: ä¿¡æ¯ä¸è¶³æˆ–ä¸å¯é 

3. **å®ç”¨ä»·å€¼ (actionability)**ï¼šæœç´¢ç»“æœå¯¹ç”¨æˆ·æ˜¯å¦æœ‰å®ç”¨ä»·å€¼ï¼Ÿ
   - 1.0: æä¾›äº†æ˜ç¡®çš„ç­”æ¡ˆæˆ–å¯è¡Œçš„å»ºè®®
   - 0.7: æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯æˆ–æ€è·¯
   - 0.4: æä¾›äº†ä¸€äº›å‚è€ƒä»·å€¼
   - 0.0: ç¼ºä¹å®ç”¨ä»·å€¼

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»…è¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š
{{
    "relevance": 0.0-1.0çš„ç›¸å…³æ€§åˆ†æ•°,
    "quality": 0.0-1.0çš„è´¨é‡åˆ†æ•°,
    "actionability": 0.0-1.0çš„å®ç”¨ä»·å€¼åˆ†æ•°,
    "explanation": "ç®€çŸ­è¯´æ˜è¯„åˆ†ç†ç”±ï¼ˆ1-2å¥è¯ï¼‰"
}}"""

            logger.info("ğŸ” [LLMè¯„åˆ†] è°ƒç”¨è¯­ä¹‰åˆ†æå™¨è¿›è¡Œè¯„åˆ†...")
            
            # è°ƒç”¨LLM
            response = self.semantic_analyzer.llm_manager.chat_completion(
                messages=[{"role": "user", "content": evaluation_prompt}],
                temperature=0.1,  # ä½æ¸©åº¦ä¿è¯è¯„åˆ†ç¨³å®š
                max_tokens=300
            )
            
            if response and response.success and response.content:
                # è§£æJSONå“åº”
                from ..providers.rag_seed_generator import parse_json_response
                result_data = parse_json_response(response.content)
                
                if result_data and all(k in result_data for k in ['relevance', 'quality', 'actionability']):
                    # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆä¸‰ä¸ªç»´åº¦çš„åŠ æƒå¹³å‡ï¼‰
                    relevance = float(result_data['relevance'])
                    quality = float(result_data['quality'])
                    actionability = float(result_data['actionability'])
                    
                    # åŠ æƒï¼šç›¸å…³æ€§40%ï¼Œè´¨é‡30%ï¼Œå®ç”¨ä»·å€¼30%
                    semantic_score = relevance * 0.4 + quality * 0.3 + actionability * 0.3
                    
                    explanation = result_data.get('explanation', '')
                    logger.info(f"âœ… [LLMè¯„åˆ†] ç›¸å…³æ€§:{relevance:.2f} è´¨é‡:{quality:.2f} å®ç”¨:{actionability:.2f} â†’ ç»¼åˆ:{semantic_score:.3f}")
                    logger.info(f"ğŸ’¡ [LLMè¯„åˆ†] è¯„åˆ†ç†ç”±: {explanation}")
                    
                    return semantic_score
                else:
                    logger.warning(f"âš ï¸ [LLMè¯„åˆ†] JSONè§£æä¸å®Œæ•´: {result_data}")
                    return None
            else:
                logger.warning(f"âš ï¸ [LLMè¯„åˆ†] LLMè°ƒç”¨å¤±è´¥")
                return None
                
        except Exception as e:
            logger.error(f"âŒ [LLMè¯„åˆ†] è¯­ä¹‰ç›¸å…³åº¦è¯„åˆ†å¤±è´¥: {e}")
            return None
    
    def _format_search_results_for_llm(self, search_results: List[SearchResult]) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœä¾›LLMè¯„ä¼°"""
        if not search_results:
            return "æœªæ‰¾åˆ°æœç´¢ç»“æœ"
        
        formatted = []
        for i, result in enumerate(search_results, 1):
            formatted.append(f"{i}. æ ‡é¢˜: {result.title}")
            formatted.append(f"   æ‘˜è¦: {result.snippet[:150]}...")
            formatted.append("")
        
        return "\n".join(formatted)
    
    def _generate_analysis_summary(self, search_results: List[SearchResult], 
                                 idea_text: str, feasibility_score: float) -> str:
        """
        ç”Ÿæˆåˆ†ææ‘˜è¦
        
        Args:
            search_results: æœç´¢ç»“æœ
            idea_text: æƒ³æ³•æ–‡æœ¬
            feasibility_score: å¯è¡Œæ€§åˆ†æ•°
            
        Returns:
            str: åˆ†ææ‘˜è¦
        """
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå¯è¡Œæ€§åˆ†æå—é™ã€‚"
        
        # æ ¹æ®å¯è¡Œæ€§åˆ†æ•°ç”ŸæˆåŸºç¡€è¯„ä¼°
        if feasibility_score >= 0.8:
            base_assessment = "è¯¥æƒ³æ³•å…·æœ‰å¾ˆé«˜çš„å¯è¡Œæ€§"
        elif feasibility_score >= 0.6:
            base_assessment = "è¯¥æƒ³æ³•å…·æœ‰è¾ƒå¥½çš„å¯è¡Œæ€§" 
        elif feasibility_score >= 0.4:
            base_assessment = "è¯¥æƒ³æ³•å…·æœ‰ä¸€å®šçš„å¯è¡Œæ€§ï¼Œä½†éœ€è¦è°¨æ…è¯„ä¼°"
        elif feasibility_score >= 0.2:
            base_assessment = "è¯¥æƒ³æ³•å¯è¡Œæ€§è¾ƒä½ï¼Œå­˜åœ¨è¾ƒå¤§æŒ‘æˆ˜"
        else:
            base_assessment = "è¯¥æƒ³æ³•å¯è¡Œæ€§å¾ˆä½ï¼Œå®ç°å›°éš¾"
        
        # æå–å…³é”®ä¿¡æ¯
        key_findings = []
        
        # ç»Ÿè®¡ç›¸å…³ç»“æœæ•°é‡
        key_findings.append(f"æœç´¢åˆ°{len(search_results)}ä¸ªç›¸å…³ç»“æœ")
        
        # åˆ†ææœç´¢ç»“æœè´¨é‡ï¼ˆåŸºäºå†…å®¹é•¿åº¦ï¼‰
        if search_results:
            has_detailed_content = sum(1 for r in search_results if len(r.snippet) > 50)
            if has_detailed_content >= len(search_results) * 0.6:
                key_findings.append("æ‰¾åˆ°äº†è¯¦ç»†çš„ç›¸å…³èµ„æ–™")
            else:
                key_findings.append("ç›¸å…³èµ„æ–™æœ‰é™")
        
        # æ„å»ºå®Œæ•´æ‘˜è¦
        summary_parts = [base_assessment]
        
        if key_findings:
            summary_parts.append("ã€‚åˆ†æå‘ç°ï¼š" + "ï¼Œ".join(key_findings))
        
        summary_parts.append(f"ã€‚ç»¼åˆè¯„ä¼°å¯è¡Œæ€§å¾—åˆ†ä¸º{feasibility_score:.1f}/1.0ã€‚")
        
        return "".join(summary_parts)
    
    def _generate_fallback_analysis(self, idea_text: str, error_message: str) -> Dict[str, Any]:
        """
        ğŸ”¥ æ–°å¢ï¼šå½“æœç´¢å¤±è´¥æ—¶ç”Ÿæˆå›é€€åˆ†æ
        
        Args:
            idea_text: æƒ³æ³•æ–‡æœ¬
            error_message: é”™è¯¯æ¶ˆæ¯
            
        Returns:
            Dict: åŒ…å«scoreå’Œsummaryçš„åˆ†æç»“æœ
        """
        logger.info(f"ğŸ­ ç”Ÿæˆå›é€€åˆ†æ: {idea_text[:50]}...")
        
        # åŸºäºæ–‡æœ¬å†…å®¹çš„ç®€å•å¯è¡Œæ€§è¯„ä¼°
        score = 0.5  # é»˜è®¤ä¸­ç­‰å¯è¡Œæ€§
        
        # å…³é”®è¯åˆ†æ
        tech_indicators = [
            'API', 'api', 'ç®—æ³•', 'æ•°æ®åº“', 'ç³»ç»Ÿ', 'æ¶æ„', 'ä¼˜åŒ–',
            'æœºå™¨å­¦ä¹ ', 'ML', 'AI', 'äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ',
            'ç½‘ç»œ', 'çˆ¬è™«', 'æ•°æ®åˆ†æ', 'å®æ—¶', 'æ€§èƒ½', 'å®‰å…¨'
        ]
        
        positive_indicators = [
            'ç®€å•', 'åŸºç¡€', 'æ ‡å‡†', 'å¸¸è§', 'æˆç†Ÿ', 'å¼€æº',
            'simple', 'basic', 'standard', 'common', 'mature'
        ]
        
        challenging_indicators = [
            'å¤æ‚', 'é«˜çº§', 'åˆ›æ–°', 'å‰æ²¿', 'å®éªŒ', 'ç ”ç©¶',
            'complex', 'advanced', 'innovative', 'cutting-edge'
        ]
        
        text_lower = idea_text.lower()
        
        # æŠ€æœ¯å¤æ‚åº¦è¯„ä¼°
        tech_count = sum(1 for indicator in tech_indicators if indicator.lower() in text_lower)
        positive_count = sum(1 for indicator in positive_indicators if indicator in text_lower)
        challenging_count = sum(1 for indicator in challenging_indicators if indicator in text_lower)
        
        # è°ƒæ•´åˆ†æ•°
        if tech_count > 0:
            score += 0.1  # æœ‰æŠ€æœ¯å…³é”®è¯ï¼Œç¨å¾®æé«˜å¯è¡Œæ€§
        
        if positive_count > challenging_count:
            score += 0.2  # æ›´å¤šæ­£é¢æŒ‡æ ‡
        elif challenging_count > positive_count:
            score -= 0.1  # æ›´å¤šæŒ‘æˆ˜æ€§æŒ‡æ ‡
        
        # æ–‡æœ¬é•¿åº¦å½±å“ï¼ˆæ›´è¯¦ç»†çš„æè¿°é€šå¸¸æ›´å¯è¡Œï¼‰
        if len(idea_text) > 100:
            score += 0.1
        elif len(idea_text) < 50:
            score -= 0.1
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        score = max(0.1, min(0.9, score))
        
        # ç”Ÿæˆåˆ†ææ‘˜è¦
        summary_parts = [
            f"åŸºäºæ–‡æœ¬åˆ†æï¼Œã€Œ{idea_text[:50]}{'...' if len(idea_text) > 50 else ''}ã€"
        ]
        
        if score >= 0.7:
            summary_parts.append("æ˜¾ç¤ºå‡ºè¾ƒé«˜çš„å¯è¡Œæ€§")
        elif score >= 0.5:
            summary_parts.append("å…·æœ‰ä¸­ç­‰å¯è¡Œæ€§")
        else:
            summary_parts.append("å¯è¡Œæ€§ç›¸å¯¹è¾ƒä½")
        
        # æ·»åŠ æŠ€æœ¯è¯„ä¼°
        if tech_count > 2:
            summary_parts.append("ï¼Œæ¶‰åŠå¤šé¡¹æŠ€æœ¯è¦ç´ ")
        elif tech_count > 0:
            summary_parts.append("ï¼ŒåŒ…å«æŠ€æœ¯å®ç°è¦ç´ ")
        
        # æ·»åŠ å¤æ‚åº¦è¯„ä¼°
        if challenging_count > positive_count:
            summary_parts.append("ï¼Œä½†å®ç°éš¾åº¦è¾ƒé«˜")
        elif positive_count > 0:
            summary_parts.append("ï¼Œå®ç°ç›¸å¯¹ç®€å•")
        
        # æ·»åŠ æœç´¢å¤±è´¥è¯´æ˜
        summary_parts.append("ã€‚ç”±äºæœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œæ­¤åˆ†æåŸºäºæ–‡æœ¬å†…å®¹è¿›è¡Œ")
        
        if "rate" in error_message.lower() or "limit" in error_message.lower():
            summary_parts.append("ï¼Œå»ºè®®ç¨åé‡è¯•è·å–æ›´è¯¦ç»†çš„éªŒè¯ç»“æœ")
        else:
            summary_parts.append("ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•")
        
        summary = "".join(summary_parts)
        
        logger.info(f"ğŸ­ å›é€€åˆ†æå®Œæˆ: å¯è¡Œæ€§{score:.2f}")
        
        return {
            'score': score,
            'summary': summary
        }
