#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLMç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å¤šä¸ªLLMæä¾›å•†
LLM Manager - Unified management for multiple LLM providers
"""

import os
import time
import logging
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from collections import defaultdict

from .llm_base import BaseLLMClient, LLMConfig, LLMProvider, LLMResponse, LLMMessage
from .impl.deepseek_client import create_llm_client
try:
    from neogenesis_system.config import (
        LLM_PROVIDERS_CONFIG, DEFAULT_LLM_CONFIG, LLM_MANAGER_CONFIG, 
        COST_CONTROL_CONFIG, FEATURE_FLAGS
    )
except ImportError:
    try:
        from ..config import (
            LLM_PROVIDERS_CONFIG, DEFAULT_LLM_CONFIG, LLM_MANAGER_CONFIG, 
            COST_CONTROL_CONFIG, FEATURE_FLAGS
        )
    except ImportError:
        # æä¾›é»˜è®¤é…ç½®
        LLM_PROVIDERS_CONFIG = {}
        DEFAULT_LLM_CONFIG = {}
        LLM_MANAGER_CONFIG = {}
        COST_CONTROL_CONFIG = {}
        FEATURE_FLAGS = {}

logger = logging.getLogger(__name__)


@dataclass
class ProviderStatus:
    """æä¾›å•†çŠ¶æ€"""
    name: str
    enabled: bool
    healthy: bool
    last_check: float
    error_count: int
    success_count: int
    avg_response_time: float
    last_error: Optional[str] = None


class LLMManager:
    """
    LLMç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å¤šä¸ªLLMæä¾›å•†
    
    åŠŸèƒ½ï¼š
    - è‡ªåŠ¨å‘ç°å’Œåˆå§‹åŒ–å¯ç”¨çš„LLMæä¾›å•†
    - æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
    - è‡ªåŠ¨å›é€€æœºåˆ¶
    - æˆæœ¬è·Ÿè¸ªå’Œæ§åˆ¶
    - å¥åº·æ£€æŸ¥å’Œç›‘æ§
    """
    
    def __init__(self):
        """åˆå§‹åŒ–LLMç®¡ç†å™¨"""
        self.providers: Dict[str, BaseLLMClient] = {}
        self.provider_status: Dict[str, ProviderStatus] = {}
        self.config = DEFAULT_LLM_CONFIG.copy()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'fallback_count': 0,
            'provider_usage': defaultdict(int),
            'cost_tracking': defaultdict(float),
            'request_history': []
        }
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.initialized = False
        self.last_health_check = 0
        
        logger.info("ğŸš€ LLMç®¡ç†å™¨åˆå§‹åŒ–å¼€å§‹")
        self._initialize_providers()
        
    def _initialize_providers(self):
        """ğŸ”¥ å¢å¼ºç‰ˆï¼šåˆå§‹åŒ–å¯ç”¨çš„æä¾›å•†ï¼ˆåŒ…å«å¤‡ç”¨æä¾›å•†ï¼‰"""
        if not FEATURE_FLAGS.get("enable_multi_llm_support", False):
            logger.info("ğŸ“‹ å¤šLLMæ”¯æŒå·²ç¦ç”¨ï¼Œä»…ä½¿ç”¨DeepSeek")
            self._initialize_single_provider()
            return
        
        initialized_count = 0
        
        for provider_name, provider_config in LLM_PROVIDERS_CONFIG.items():
            try:
                if not provider_config.get("enabled", False):
                    logger.debug(f"â­ï¸ è·³è¿‡ç¦ç”¨çš„æä¾›å•†: {provider_name}")
                    continue
                
                # æ£€æŸ¥APIå¯†é’¥
                api_key = ""
                
                # ä¼˜å…ˆä½¿ç”¨ç¡¬ç¼–ç çš„APIå¯†é’¥
                if provider_config.get("hardcoded_api_key"):
                    api_key = provider_config["hardcoded_api_key"]
                    logger.info(f"ğŸ”‘ ä½¿ç”¨ç¡¬ç¼–ç APIå¯†é’¥for {provider_name}")
                else:
                    # ä½¿ç”¨ç¯å¢ƒå˜é‡
                    api_key_env = provider_config.get("api_key_env")
                    if api_key_env:
                        api_key = os.getenv(api_key_env, "")
                        if not api_key:
                            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{provider_name}çš„APIå¯†é’¥: {api_key_env}")
                            continue
                
                # åˆ›å»ºLLMé…ç½®
                llm_config = self._create_llm_config(provider_name, provider_config, api_key)
                
                # åˆ›å»ºå®¢æˆ·ç«¯
                client = self._create_provider_client(provider_name, llm_config)
                
                # å¿«é€Ÿå¥åº·æ£€æŸ¥
                if self._quick_health_check(client, provider_name):
                    self.providers[provider_name] = client
                    self.provider_status[provider_name] = ProviderStatus(
                        name=provider_name,
                        enabled=True,
                        healthy=True,
                        last_check=time.time(),
                        error_count=0,
                        success_count=1,
                        avg_response_time=0.0
                    )
                    initialized_count += 1
                    logger.info(f"âœ… {provider_name}æä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.warning(f"âŒ {provider_name}æä¾›å•†å¥åº·æ£€æŸ¥å¤±è´¥")
                    
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ–{provider_name}æä¾›å•†å¤±è´¥: {e}")
                continue
        
        if initialized_count == 0:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œå›é€€åˆ°å•ä¸€æä¾›å•†æ¨¡å¼")
            self._initialize_single_provider()
        else:
            logger.info(f"ğŸ‰ LLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æä¾›å•†: {initialized_count}ä¸ª")
            self.initialized = True
            
            # ğŸ”¥ æ–°å¢ï¼šç¡®ä¿æœ‰å¤‡ç”¨æä¾›å•†é…ç½®
            self._ensure_fallback_configuration()
    
    def _initialize_single_provider(self):
        """åˆå§‹åŒ–å•ä¸€æä¾›å•†ï¼ˆDeepSeekï¼‰"""
        try:
            from .client_adapter import get_or_create_unified_client
            
            api_key = os.getenv("DEEPSEEK_API_KEY", "")
            if not api_key:
                logger.error("âŒ æœªæ‰¾åˆ°DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
                return
            
            client = get_or_create_unified_client(api_key)
            self.providers["deepseek"] = client
            self.provider_status["deepseek"] = ProviderStatus(
                name="deepseek",
                enabled=True,
                healthy=True,
                last_check=time.time(),
                error_count=0,
                success_count=1,
                avg_response_time=0.0
            )
            
            logger.info("âœ… å•ä¸€æä¾›å•†æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ (DeepSeek)")
            self.initialized = True
            
        except Exception as e:
            logger.error(f"âŒ å•ä¸€æä¾›å•†åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _create_provider_client(self, provider_name: str, llm_config: LLMConfig) -> BaseLLMClient:
        """åˆ›å»ºç‰¹å®šæä¾›å•†çš„å®¢æˆ·ç«¯"""
        try:
            # æ£€æŸ¥é…ç½®ä¸­çš„provider_typeæ¥å†³å®šä½¿ç”¨å“ªä¸ªå®¢æˆ·ç«¯
            provider_config = LLM_PROVIDERS_CONFIG.get(provider_name, {})
            provider_type = provider_config.get("provider_type", provider_name)
            
            if provider_type == "gemini_openai" or (provider_name == "gemini" and provider_type == "gemini_openai"):
                from .impl.gemini_openai_client import GeminiOpenAIClient
                return GeminiOpenAIClient(llm_config)
            elif provider_name == "gemini" or provider_type == "gemini":
                from .impl.gemini_client import GeminiClient
                return GeminiClient(llm_config)
            elif provider_name == "deepseek":
                from .impl.deepseek_client import create_llm_client
                return create_llm_client(llm_config.api_key)
            elif provider_name == "openai":
                from .impl.openai_client import OpenAIClient
                return OpenAIClient(llm_config)
            elif provider_name == "anthropic":
                from .impl.anthropic_client import AnthropicClient
                return AnthropicClient(llm_config)
            elif provider_name == "ollama":
                from .impl.ollama_client import OllamaClient
                return OllamaClient(llm_config)
            else:
                # é»˜è®¤ä½¿ç”¨DeepSeekå®¢æˆ·ç«¯ä½œä¸ºå›é€€
                from .impl.deepseek_client import create_llm_client
                return create_llm_client(llm_config.api_key)
        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥{provider_name}å®¢æˆ·ç«¯: {e}")
            # å›é€€åˆ°DeepSeek
            from .impl.deepseek_client import create_llm_client
            return create_llm_client(llm_config.api_key)
    
    def _create_llm_config(self, provider_name: str, provider_config: Dict, api_key: str) -> LLMConfig:
        """åˆ›å»ºLLMé…ç½®"""
        provider_type = provider_config["provider_type"]
        provider_enum = LLMProvider(provider_type)
        
        # ç‰¹æ®Šå¤„ç†Azure OpenAI
        extra_params = {}
        base_url = provider_config.get("base_url")
        
        # ç¡®ä¿base_urlä¸ä¸ºNone
        if base_url is None:
            if provider_type == "deepseek":
                base_url = "https://api.deepseek.com/v1"
            elif provider_type == "gemini_openai":
                base_url = "https://hiapi.online"
            elif provider_type == "openai":
                base_url = "https://api.openai.com/v1"
            else:
                base_url = "https://api.deepseek.com/v1"  # é»˜è®¤ä½¿ç”¨deepseek
            logger.warning(f"âš ï¸ {provider_name}çš„base_urlä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼: {base_url}")
        
        if provider_type == "azure_openai":
            azure_endpoint = os.getenv(provider_config.get("azure_endpoint_env", ""), "")
            if azure_endpoint:
                base_url = azure_endpoint
                extra_params["api_version"] = provider_config.get("api_version", "2024-02-15-preview")
        
        return LLMConfig(
            provider=provider_enum,
            api_key=api_key,
            model_name=provider_config["default_model"],
            temperature=provider_config.get("temperature", 0.7),
            max_tokens=provider_config.get("max_tokens", 2000),
            base_url=base_url,
            timeout=tuple(provider_config.get("timeout", (30, 120))),
            max_retries=provider_config.get("max_retries", 3),
            retry_delay_base=provider_config.get("retry_delay_base", 2.0),
            request_interval=provider_config.get("request_interval", 1.0),
            extra_params=extra_params
        )
    
    def _quick_health_check(self, client: BaseLLMClient, provider_name: str) -> bool:
        """å¿«é€Ÿå¥åº·æ£€æŸ¥"""
        try:
            # ç®€å•çš„é…ç½®éªŒè¯
            return client.validate_config()
        except Exception as e:
            logger.debug(f"ğŸ” {provider_name}å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def call_api(self, prompt: str, 
                 system_message: Optional[str] = None,
                 temperature: Optional[float] = None,
                 **kwargs) -> str:
        """
        ç®€åŒ–çš„APIè°ƒç”¨æ¥å£ - å…¼å®¹ç°æœ‰ä»£ç 
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: LLMå“åº”å†…å®¹
            
        Raises:
            Exception: è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = []
            if system_message:
                messages.append(LLMMessage(role="system", content=system_message))
            messages.append(LLMMessage(role="user", content=prompt))
            
            # è°ƒç”¨chat_completion
            response = self.chat_completion(
                messages=messages,
                temperature=temperature,
                **kwargs
            )
            
            if response.success:
                return response.content
            else:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {response.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ LLM APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def chat_completion(self, 
                       messages: Union[str, List[LLMMessage]], 
                       provider_name: Optional[str] = None,
                       temperature: Optional[float] = None,
                       **kwargs) -> LLMResponse:
        """
        èŠå¤©å®Œæˆ - æ™ºèƒ½è·¯ç”±åˆ°æœ€ä½³æä¾›å•†
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹
            provider_name: æŒ‡å®šæä¾›å•†ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€å“åº”
        """
        self.stats['total_requests'] += 1
        
        if not self.initialized or not self.providers:
            return self._create_error_response("æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†")
        
        # å¤„ç†ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²çš„æƒ…å†µ
        if isinstance(messages, str):
            messages = [LLMMessage(role="user", content=messages)]
        
        # é€‰æ‹©æä¾›å•†
        selected_provider = self._select_provider(provider_name)
        if not selected_provider:
            return self._create_error_response("æ— æ³•é€‰æ‹©åˆé€‚çš„æä¾›å•†")
        
        # æ·»åŠ temperatureåˆ°kwargsä¸­
        if temperature is not None:
            kwargs['temperature'] = temperature
        
        # æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰
        return self._execute_with_fallback(selected_provider, messages, **kwargs)
    
    def _select_provider(self, preferred_provider: Optional[str] = None) -> Optional[str]:
        """é€‰æ‹©æä¾›å•†"""
        # å¦‚æœæŒ‡å®šäº†æä¾›å•†ä¸”å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨
        if preferred_provider and preferred_provider in self.providers:
            if self.provider_status[preferred_provider].healthy:
                return preferred_provider
        
        # æ£€æŸ¥ä¸»è¦æä¾›å•†è®¾ç½®
        primary = self.config.get("primary_provider", "auto")
        
        if primary == "auto":
            # è‡ªåŠ¨é€‰æ‹©ï¼šæŒ‰é¦–é€‰é¡ºåºé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†
            preferred_order = self.config.get("preferred_providers", ["deepseek", "openai", "anthropic"])
            for provider_name in preferred_order:
                if provider_name in self.providers and self.provider_status[provider_name].healthy:
                    return provider_name
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ä¸»è¦æä¾›å•†
            if primary in self.providers and self.provider_status[primary].healthy:
                return primary
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¥åº·çš„æä¾›å•†
        for name, status in self.provider_status.items():
            if status.healthy and name in self.providers:
                return name
        
        return None
    
    def _execute_with_fallback(self, provider_name: str, messages: Union[str, List[LLMMessage]], **kwargs) -> LLMResponse:
        """ğŸ”¥ å¢å¼ºç‰ˆï¼šæ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦æ™ºèƒ½å›é€€æœºåˆ¶ï¼‰"""
        providers_to_try = [provider_name]
        
        # ğŸ”¥ å¢å¼ºç‰ˆï¼šæ™ºèƒ½å›é€€ç­–ç•¥
        if self.config.get("auto_fallback", True):
            # è·å–æ‰€æœ‰å¥åº·çš„æä¾›å•†ä½œä¸ºå›é€€é€‰é¡¹
            healthy_providers = [
                name for name, status in self.provider_status.items() 
                if status.healthy and name != provider_name and name in self.providers
            ]
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºå›é€€æä¾›å•†
            fallback_providers = self.config.get("fallback_providers", ["openai", "anthropic", "gemini"])
            ordered_fallbacks = []
            
            # é¦–å…ˆæ·»åŠ é…ç½®ä¸­æŒ‡å®šçš„å›é€€æä¾›å•†
            for fallback in fallback_providers:
                if fallback in healthy_providers:
                    ordered_fallbacks.append(fallback)
                    healthy_providers.remove(fallback)
            
            # ç„¶åæ·»åŠ å…¶ä»–å¥åº·çš„æä¾›å•†
            ordered_fallbacks.extend(healthy_providers)
            providers_to_try.extend(ordered_fallbacks)
        
        last_error = None
        network_errors = []
        auth_errors = []
        
        for i, current_provider in enumerate(providers_to_try):
            try:
                # ğŸ”¥ å¢å¼ºç‰ˆï¼šåŠ¨æ€å¥åº·æ£€æŸ¥
                if not self.provider_status[current_provider].healthy:
                    # å¦‚æœæ˜¯ä¸»è¦æä¾›å•†ï¼Œå°è¯•å¿«é€Ÿå¥åº·æ£€æŸ¥
                    if i == 0 and hasattr(self.providers[current_provider], 'health_check'):
                        logger.info(f"ğŸ” å¯¹{current_provider}æ‰§è¡Œå¿«é€Ÿå¥åº·æ£€æŸ¥...")
                        health_status = self.providers[current_provider].health_check(timeout=10.0)
                        if health_status.get('is_healthy', False):
                            self.provider_status[current_provider].healthy = True
                            logger.info(f"âœ… {current_provider}å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œæ¢å¤ä½¿ç”¨")
                        else:
                            logger.warning(f"âš ï¸ {current_provider}å¥åº·æ£€æŸ¥å¤±è´¥: {health_status.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
                            continue
                    else:
                        continue
                
                start_time = time.time()
                client = self.providers[current_provider]
                
                # ğŸ”¥ å¢å¼ºç‰ˆï¼šæ˜¾ç¤ºå›é€€ä¿¡æ¯
                if i == 0:
                    logger.info(f"ğŸ¤– ä½¿ç”¨ä¸»è¦æä¾›å•†: {current_provider}")
                else:
                    logger.warning(f"ğŸ”„ å›é€€åˆ°æä¾›å•†: {current_provider} (ç¬¬{i+1}é€‰æ‹©)")
                
                response = client.chat_completion(messages, **kwargs)
                response_time = time.time() - start_time
                
                if response.success:
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_provider_stats(current_provider, True, response_time)
                    self.stats['successful_requests'] += 1
                    self.stats['provider_usage'][current_provider] += 1
                    
                    # ğŸ”¥ å¢å¼ºç‰ˆï¼šè®°å½•å›é€€æˆåŠŸ
                    if i > 0:
                        self.stats['fallback_count'] += 1
                        logger.info(f"âœ… å›é€€æˆåŠŸ: {current_provider}å¤„ç†äº†è¯·æ±‚")
                    
                    # æˆæœ¬è·Ÿè¸ª
                    if response.usage and COST_CONTROL_CONFIG.get("token_usage_tracking", True):
                        self._track_cost(current_provider, response.usage)
                    
                    return response
                else:
                    # ğŸ”¥ å¢å¼ºç‰ˆï¼šåˆ†ç±»é”™è¯¯ç±»å‹
                    self._update_provider_stats(current_provider, False, response_time)
                    error_msg = response.error_message or "æœªçŸ¥é”™è¯¯"
                    last_error = error_msg
                    
                    # åˆ†ç±»é”™è¯¯ä»¥ä¾¿æ›´å¥½çš„è¯Šæ–­
                    if "network" in error_msg.lower() or "connection" in error_msg.lower():
                        network_errors.append(f"{current_provider}: {error_msg}")
                    elif "auth" in error_msg.lower() or "401" in error_msg:
                        auth_errors.append(f"{current_provider}: {error_msg}")
                    
                    logger.warning(f"âš ï¸ {current_provider}è¯·æ±‚å¤±è´¥: {error_msg}")
                    
                    # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæä¾›å•†
                    continue
                        
            except Exception as e:
                self._update_provider_stats(current_provider, False, 0)
                error_msg = str(e)
                last_error = error_msg
                
                # åˆ†ç±»å¼‚å¸¸
                if "timeout" in error_msg.lower() or "connection" in error_msg.lower():
                    network_errors.append(f"{current_provider}: {error_msg}")
                
                logger.error(f"âŒ {current_provider}æ‰§è¡Œå¼‚å¸¸: {e}")
                continue
        
        # ğŸ”¥ å¢å¼ºç‰ˆï¼šæ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥æ—¶çš„è¯¦ç»†é”™è¯¯æŠ¥å‘Š
        self.stats['failed_requests'] += 1
        
        # ç”Ÿæˆè¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
        error_report = f"æ‰€æœ‰{len(providers_to_try)}ä¸ªæä¾›å•†éƒ½ä¸å¯ç”¨"
        if network_errors:
            error_report += f"\nç½‘ç»œé”™è¯¯: {len(network_errors)}ä¸ª"
        if auth_errors:
            error_report += f"\nè®¤è¯é”™è¯¯: {len(auth_errors)}ä¸ª"
        if last_error:
            error_report += f"\næœ€åé”™è¯¯: {last_error}"
        
        logger.error(f"âŒ {error_report}")
        
        # ğŸ”¥ å¢å¼ºç‰ˆï¼šæä¾›æ¢å¤å»ºè®®
        logger.error("ğŸ’¡ æ¢å¤å»ºè®®:")
        logger.error("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€")
        logger.error("   2. éªŒè¯APIå¯†é’¥é…ç½®")
        logger.error("   3. æ£€æŸ¥æä¾›å•†æœåŠ¡çŠ¶æ€")
        logger.error("   4. è€ƒè™‘å¢åŠ æ›´å¤šå¤‡ç”¨æä¾›å•†")
        
        return self._create_error_response(error_report)
    
    def _update_provider_stats(self, provider_name: str, success: bool, response_time: float):
        """æ›´æ–°æä¾›å•†ç»Ÿè®¡"""
        if provider_name not in self.provider_status:
            return
        
        status = self.provider_status[provider_name]
        status.last_check = time.time()
        
        if success:
            status.success_count += 1
            status.error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
            status.healthy = True
            
            # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
            if status.avg_response_time == 0:
                status.avg_response_time = response_time
            else:
                status.avg_response_time = (status.avg_response_time + response_time) / 2
        else:
            status.error_count += 1
            
            # è¿ç»­é”™è¯¯è¿‡å¤šæ—¶æ ‡è®°ä¸ºä¸å¥åº·
            if status.error_count >= 3:
                status.healthy = False
                logger.warning(f"âš ï¸ {provider_name}è¢«æ ‡è®°ä¸ºä¸å¥åº·")
    
    def _track_cost(self, provider_name: str, usage):
        """è·Ÿè¸ªæˆæœ¬"""
        try:
            provider_config = LLM_PROVIDERS_CONFIG.get(provider_name, {})
            cost_per_1k = provider_config.get("cost_per_1k_tokens", {})
            
            input_cost = (usage.prompt_tokens / 1000) * cost_per_1k.get("input", 0)
            output_cost = (usage.completion_tokens / 1000) * cost_per_1k.get("output", 0)
            total_cost = input_cost + output_cost
            
            self.stats['cost_tracking'][provider_name] += total_cost
            
            logger.debug(f"ğŸ’° {provider_name}æˆæœ¬: ${total_cost:.6f}")
            
        except Exception as e:
            logger.debug(f"âŒ æˆæœ¬è·Ÿè¸ªå¤±è´¥: {e}")
    
    def _create_error_response(self, error_message: str) -> LLMResponse:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        from .llm_base import LLMErrorType, create_error_response
        return create_error_response(
            provider="llm_manager",
            error_type=LLMErrorType.UNKNOWN_ERROR,
            error_message=error_message
        )
    
    def get_provider_status(self) -> Dict[str, Any]:
        """è·å–æä¾›å•†çŠ¶æ€"""
        return {
            'initialized': self.initialized,
            'total_providers': len(self.providers),
            'healthy_providers': sum(1 for s in self.provider_status.values() if s.healthy),
            'providers': {name: {
                'enabled': status.enabled,
                'healthy': status.healthy,
                'success_count': status.success_count,
                'error_count': status.error_count,
                'avg_response_time': status.avg_response_time,
                'last_error': status.last_error
            } for name, status in self.provider_status.items()},
            'stats': self.stats.copy()
        }
    
    def get_available_models(self, provider_name: Optional[str] = None) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨æ¨¡å‹"""
        if provider_name:
            if provider_name in self.providers:
                return {provider_name: self.providers[provider_name].get_available_models()}
            else:
                return {}
        
        return {
            name: client.get_available_models() 
            for name, client in self.providers.items()
        }
    
    def switch_primary_provider(self, provider_name: str) -> bool:
        """åˆ‡æ¢ä¸»è¦æä¾›å•†"""
        if provider_name in self.providers and self.provider_status[provider_name].healthy:
            self.config["primary_provider"] = provider_name
            logger.info(f"ğŸ”„ ä¸»è¦æä¾›å•†å·²åˆ‡æ¢åˆ°: {provider_name}")
            return True
        return False
    
    def health_check(self, force: bool = False) -> Dict[str, bool]:
        """å¥åº·æ£€æŸ¥"""
        current_time = time.time()
        check_interval = LLM_MANAGER_CONFIG.get("health_check_interval", 300)
        
        if not force and (current_time - self.last_health_check) < check_interval:
            return {name: status.healthy for name, status in self.provider_status.items()}
        
        logger.info("ğŸ” æ‰§è¡ŒLLMæä¾›å•†å¥åº·æ£€æŸ¥")
        results = {}
        
        for name, client in self.providers.items():
            try:
                healthy = self._quick_health_check(client, name)
                self.provider_status[name].healthy = healthy
                results[name] = healthy
                logger.debug(f"ğŸ” {name}: {'âœ…' if healthy else 'âŒ'}")
            except Exception as e:
                self.provider_status[name].healthy = False
                results[name] = False
                logger.error(f"ğŸ” {name}å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        
        self.last_health_check = current_time
        return results
    
    # ==================== ğŸ”¥ æ–°å¢ï¼šå¤‡ç”¨æä¾›å•†ç®¡ç†æ–¹æ³• ====================
    
    def _ensure_fallback_configuration(self):
        """ğŸ”¥ æ–°å¢ï¼šç¡®ä¿æœ‰åˆç†çš„å¤‡ç”¨æä¾›å•†é…ç½®"""
        try:
            available_providers = list(self.providers.keys())
            healthy_providers = [
                name for name, status in self.provider_status.items() 
                if status.healthy
            ]
            
            logger.info(f"ğŸ” æ£€æŸ¥å¤‡ç”¨æä¾›å•†é…ç½®...")
            logger.info(f"   å¯ç”¨æä¾›å•†: {available_providers}")
            logger.info(f"   å¥åº·æä¾›å•†: {healthy_providers}")
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªæä¾›å•†ï¼Œç»™å‡ºå»ºè®®
            if len(healthy_providers) == 1:
                logger.warning("âš ï¸ åªæœ‰ä¸€ä¸ªå¥åº·çš„LLMæä¾›å•†ï¼Œå»ºè®®é…ç½®å¤‡ç”¨æä¾›å•†")
                logger.info("ğŸ’¡ å¤‡ç”¨æä¾›å•†å»ºè®®:")
                logger.info("   1. é…ç½®OpenAIä½œä¸ºå¤‡ç”¨")
                logger.info("   2. é…ç½®Anthropic Claudeä½œä¸ºå¤‡ç”¨")
                logger.info("   3. é…ç½®Google Geminiä½œä¸ºå¤‡ç”¨")
            
            # è‡ªåŠ¨é…ç½®å›é€€é¡ºåº
            if not self.config.get("fallback_providers"):
                # æ ¹æ®å¯ç”¨æä¾›å•†è‡ªåŠ¨é…ç½®å›é€€é¡ºåº
                fallback_order = []
                preferred_fallbacks = ["openai", "anthropic", "gemini", "groq"]
                
                for fallback in preferred_fallbacks:
                    if fallback in healthy_providers:
                        fallback_order.append(fallback)
                
                # æ·»åŠ å…¶ä»–å¥åº·çš„æä¾›å•†
                for provider in healthy_providers:
                    if provider not in fallback_order:
                        fallback_order.append(provider)
                
                self.config["fallback_providers"] = fallback_order
                logger.info(f"ğŸ”„ è‡ªåŠ¨é…ç½®å›é€€é¡ºåº: {fallback_order}")
            
            # ç¡®ä¿è‡ªåŠ¨å›é€€å·²å¯ç”¨
            if not self.config.get("auto_fallback", True):
                self.config["auto_fallback"] = True
                logger.info("âœ… è‡ªåŠ¨å›é€€åŠŸèƒ½å·²å¯ç”¨")
                
        except Exception as e:
            logger.error(f"âŒ é…ç½®å¤‡ç”¨æä¾›å•†å¤±è´¥: {e}")
    
    def add_emergency_fallback(self, provider_name: str, api_key: str) -> bool:
        """
        ğŸ”¥ æ–°å¢ï¼šæ·»åŠ ç´§æ€¥å¤‡ç”¨æä¾›å•†
        
        Args:
            provider_name: æä¾›å•†åç§° (å¦‚ "openai", "anthropic")
            api_key: APIå¯†é’¥
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        try:
            logger.info(f"ğŸš¨ æ·»åŠ ç´§æ€¥å¤‡ç”¨æä¾›å•†: {provider_name}")
            
            # åˆ›å»ºåŸºæœ¬é…ç½®
            from .llm_base import LLMConfig, LLMProvider
            
            provider_enum = None
            base_url = None
            
            if provider_name == "openai":
                provider_enum = LLMProvider.OPENAI
                base_url = "https://api.openai.com/v1"
            elif provider_name == "anthropic":
                provider_enum = LLMProvider.ANTHROPIC
                base_url = "https://api.anthropic.com"
            elif provider_name == "gemini":
                provider_enum = LLMProvider.GEMINI
                base_url = "https://generativelanguage.googleapis.com/v1"
            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„ç´§æ€¥å¤‡ç”¨æä¾›å•†: {provider_name}")
                return False
            
            # åˆ›å»ºLLMé…ç½®
            llm_config = LLMConfig(
                provider=provider_enum,
                api_key=api_key,
                model_name=self._get_default_model(provider_name),
                base_url=base_url,
                timeout=(60, 300),  # ä½¿ç”¨è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
                max_retries=3
            )
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = self._create_provider_client(provider_name, llm_config)
            
            # å¿«é€Ÿå¥åº·æ£€æŸ¥
            if self._quick_health_check(client, provider_name):
                self.providers[provider_name] = client
                self.provider_status[provider_name] = ProviderStatus(
                    name=provider_name,
                    enabled=True,
                    healthy=True,
                    last_check=time.time(),
                    error_count=0,
                    success_count=1,
                    avg_response_time=0.0
                )
                
                # æ›´æ–°å›é€€é…ç½®
                fallback_providers = self.config.get("fallback_providers", [])
                if provider_name not in fallback_providers:
                    fallback_providers.append(provider_name)
                    self.config["fallback_providers"] = fallback_providers
                
                logger.info(f"âœ… ç´§æ€¥å¤‡ç”¨æä¾›å•†{provider_name}æ·»åŠ æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ ç´§æ€¥å¤‡ç”¨æä¾›å•†{provider_name}å¥åº·æ£€æŸ¥å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç´§æ€¥å¤‡ç”¨æä¾›å•†å¤±è´¥: {e}")
            return False
    
    def _get_default_model(self, provider_name: str) -> str:
        """è·å–æä¾›å•†çš„é»˜è®¤æ¨¡å‹"""
        default_models = {
            "openai": "gpt-3.5-turbo",
            "anthropic": "claude-3-haiku-20240307",
            "gemini": "gemini-pro",
            "groq": "llama2-70b-4096",
            "deepseek": "deepseek-chat"
        }
        return default_models.get(provider_name, "gpt-3.5-turbo")

    def generate_response(self, query: str, provider: str = 'deepseek', **kwargs) -> str:
        """
        ç”Ÿæˆå“åº” - ç®€åŒ–çš„APIè°ƒç”¨æ¥å£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            provider: æŒ‡å®šæä¾›å•†
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: ç”Ÿæˆçš„å“åº”å†…å®¹
        """
        try:
            from .llm_base import LLMMessage
            messages = [LLMMessage(role='user', content=query)]
            response = self.chat_completion(
                messages=messages,
                provider_name=provider,
                **kwargs
            )
            return response.content
        except Exception as e:
            logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶é‡åˆ°é”™è¯¯: {str(e)}"
