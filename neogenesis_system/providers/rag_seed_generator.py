
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RAGç§å­ç”Ÿæˆå™¨ - åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ™ºèƒ½æ€ç»´ç§å­åˆ›å»ºå™¨
RAG Seed Generator - Intelligent thinking seed creator based on Retrieval-Augmented Generation

æ ¸å¿ƒèŒè´£ï¼š
1. ç†è§£é—®é¢˜å¹¶æ„æ€æœç´¢ç­–ç•¥ï¼šåˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œç”Ÿæˆç²¾å‡†æœç´¢å…³é”®è¯
2. æ‰§è¡Œç½‘ç»œæœç´¢ï¼šè°ƒç”¨WebSearchClientè·å–å®æ—¶ä¿¡æ¯
3. ç»¼åˆä¿¡æ¯ç”Ÿæˆç§å­ï¼šç»“åˆç”¨æˆ·é—®é¢˜å’Œæœç´¢ç»“æœï¼Œç”ŸæˆåŸºäºäº‹å®çš„æ€ç»´ç§å­

æŠ€æœ¯ç‰¹è‰²ï¼š
- åŸºäºAnthropicçš„contextual retrievalç†å¿µ
- å¤šæºä¿¡æ¯éªŒè¯å’Œäº¤å‰å¼•ç”¨
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä¿¡æ¯æ•´åˆ
- å®æ—¶ä¿¡æ¯è·å–èƒ½åŠ›
"""

import time
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from .search_client import WebSearchClient, SearchResult, SearchResponse
# from .utils.client_adapter import DeepSeekClientAdapter  # ä¸å†éœ€è¦ï¼Œä½¿ç”¨ä¾èµ–æ³¨å…¥
from ..shared.common_utils import parse_json_response

# å¯¼å…¥è¯­ä¹‰åˆ†æå™¨
try:
    from ..cognitive_engine.semantic_analyzer import create_semantic_analyzer
    SEMANTIC_ANALYZER_AVAILABLE = True
except ImportError:
    SEMANTIC_ANALYZER_AVAILABLE = False
try:
    from neogenesis_system.config import PROMPT_TEMPLATES, RAG_CONFIG
except ImportError:
    try:
        from ..config import PROMPT_TEMPLATES, RAG_CONFIG
    except ImportError:
        PROMPT_TEMPLATES = {}
        RAG_CONFIG = {}

logger = logging.getLogger(__name__)


@dataclass
class RAGSearchStrategy:
    """RAGæœç´¢ç­–ç•¥æ•°æ®ç»“æ„"""
    primary_keywords: List[str]      # ä¸»è¦å…³é”®è¯
    secondary_keywords: List[str]    # æ¬¡è¦å…³é”®è¯  
    search_intent: str              # æœç´¢æ„å›¾
    domain_focus: str               # é¢†åŸŸèšç„¦
    information_types: List[str]    # éœ€è¦çš„ä¿¡æ¯ç±»å‹
    search_depth: str               # æœç´¢æ·±åº¦ (shallow/medium/deep)


@dataclass
class RAGInformationSynthesis:
    """RAGä¿¡æ¯ç»¼åˆç»“æœ"""
    contextual_seed: str            # ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ€ç»´ç§å­
    information_sources: List[str]  # ä¿¡æ¯æ¥æº
    confidence_score: float         # ä¿¡æ¯å¯ä¿¡åº¦
    key_insights: List[str]         # å…³é”®æ´å¯Ÿ
    knowledge_gaps: List[str]       # çŸ¥è¯†ç¼ºå£
    verification_status: str        # éªŒè¯çŠ¶æ€


class RAGSeedGenerator:
    """
    RAGç§å­ç”Ÿæˆå™¨ - ä¸“é—¨è´Ÿè´£åŸºäºå®æ—¶ä¿¡æ¯æ£€ç´¢çš„æ€ç»´ç§å­ç”Ÿæˆ
    
    è®¾è®¡ç†å¿µï¼š
    - é‡‡ç”¨"ç†è§£-æœç´¢-ç»¼åˆ"ä¸‰é˜¶æ®µæµç¨‹
    - å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä¿¡æ¯æ£€ç´¢
    - å¤šæºä¿¡æ¯éªŒè¯å’Œäº¤å‰å¼•ç”¨
    - ç”Ÿæˆå…·æœ‰ä¸°å¯Œä¸Šä¸‹æ–‡çš„æ€ç»´ç§å­
    """
    
    def __init__(self, api_key: str = "", search_engine: str = "tavily", 
                 web_search_client=None, llm_client=None):
        """
        åˆå§‹åŒ–RAGç§å­ç”Ÿæˆå™¨
        
        Args:
            api_key: LLM APIå¯†é’¥ï¼ˆå‘åå…¼å®¹ï¼‰
            search_engine: æœç´¢å¼•æ“ç±»å‹ï¼ˆå‘åå…¼å®¹ï¼‰
            web_search_client: å…±äº«çš„Webæœç´¢å®¢æˆ·ç«¯ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
            llm_client: å…±äº«çš„LLMå®¢æˆ·ç«¯ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.api_key = api_key
        self.search_engine = search_engine
        
        # ğŸ”§ ä¾èµ–æ³¨å…¥ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„æœç´¢å®¢æˆ·ç«¯
        if web_search_client:
            self.web_search_client = web_search_client
            logger.info("ğŸ” RAGç§å­ç”Ÿæˆå™¨ä½¿ç”¨å…±äº«æœç´¢å®¢æˆ·ç«¯")
        else:
            # å‘åå…¼å®¹ï¼šåˆ›å»ºè‡ªå·±çš„æœç´¢å®¢æˆ·ç«¯
            self.web_search_client = WebSearchClient(
                search_engine=search_engine, 
                max_results=RAG_CONFIG.get("max_search_results", 8)
            )
            logger.info("ğŸ” RAGç§å­ç”Ÿæˆå™¨åˆ›å»ºç‹¬ç«‹æœç´¢å®¢æˆ·ç«¯")
        
        # ğŸ”§ ä¾èµ–æ³¨å…¥ï¼šä½¿ç”¨ä¼ å…¥çš„LLMå®¢æˆ·ç«¯ï¼ˆçº¯ä¾èµ–æ³¨å…¥æ¨¡å¼ï¼‰
        self.llm_client = llm_client
        if self.llm_client:
            logger.info("ğŸ§  RAGç§å­ç”Ÿæˆå™¨ä½¿ç”¨å…±äº«LLMå®¢æˆ·ç«¯")
        else:
            logger.warning("âš ï¸ æœªæä¾›LLMå®¢æˆ·ç«¯ï¼ŒRAGå°†è¿è¡Œåœ¨ä»…æœç´¢æ¨¡å¼")
            logger.info("ğŸ’¡ è¯·ç¡®ä¿ä»ä¸Šå±‚ï¼ˆMainControllerï¼‰ä¼ å…¥æœ‰æ•ˆçš„llm_client")
        
        # æ€§èƒ½ç»Ÿè®¡å’Œç¼“å­˜
        self.performance_stats = {
            'total_generations': 0,
            'successful_generations': 0,
            'avg_generation_time': 0.0,
            'search_success_rate': 0.0,
            'synthesis_success_rate': 0.0
        }
        
        # æœç´¢ç­–ç•¥ç¼“å­˜
        self.strategy_cache = {}  # æŸ¥è¯¢æ¨¡å¼ -> æœç´¢ç­–ç•¥
        self.information_cache = {}  # å…³é”®è¯ -> æœç´¢ç»“æœ
        
        # ğŸš€ åˆå§‹åŒ–è¯­ä¹‰åˆ†æå™¨
        self.semantic_analyzer = None
        if SEMANTIC_ANALYZER_AVAILABLE:
            try:
                self.semantic_analyzer = create_semantic_analyzer()
                logger.info("ğŸ” RAGSeedGenerator å·²é›†æˆè¯­ä¹‰åˆ†æå™¨")
            except Exception as e:
                logger.warning(f"âš ï¸ è¯­ä¹‰åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨é™çº§æ–¹æ³•: {e}")
                self.semantic_analyzer = None
        else:
            logger.info("ğŸ“ æœªå‘ç°è¯­ä¹‰åˆ†æå™¨ï¼Œä½¿ç”¨ä¼ ç»Ÿå…³é”®è¯æ–¹æ³•")
        self.synthesis_cache = {}  # æŸ¥è¯¢+ä¿¡æ¯å“ˆå¸Œ -> ç»¼åˆç»“æœ
        
        # RAGè´¨é‡è·Ÿè¸ª
        self.rag_quality_metrics = {
            'information_diversity': defaultdict(float),  # ä¿¡æ¯å¤šæ ·æ€§
            'source_reliability': defaultdict(float),    # æ¥æºå¯é æ€§
            'contextual_relevance': defaultdict(float),  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            'factual_accuracy': defaultdict(float)       # äº‹å®å‡†ç¡®æ€§
        }
        
        logger.info("ğŸš€ RAGç§å­ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ” æœç´¢å¼•æ“: {search_engine}")
        logger.info(f"   ğŸ§  AIåˆ†æ: {'å¯ç”¨' if self.llm_client else 'ç¦ç”¨'}")
    
    def clear_cache(self):
        """
        æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        
        åœ¨ä»¥ä¸‹æƒ…å†µä¸‹åº”è¯¥è°ƒç”¨æ­¤æ–¹æ³•ï¼š
        - ä¿®å¤äº†å¹´ä»½æ³¨å…¥é—®é¢˜å
        - æœç´¢ç­–ç•¥é€»è¾‘æ›´æ–°å
        - æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜æ—¶
        """
        self.strategy_cache.clear()
        self.synthesis_cache.clear()
        logger.info("å·²æ¸…é™¤RAGç§å­ç”Ÿæˆå™¨çš„æ‰€æœ‰ç¼“å­˜")
    
    def generate_rag_seed(self, user_query: str, execution_context: Optional[Dict] = None) -> str:
        """
        ç”ŸæˆåŸºäºRAGçš„æ€ç»´ç§å­ - ä¸‰é˜¶æ®µæ ¸å¿ƒæµç¨‹
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            åŸºäºå®æ—¶ä¿¡æ¯çš„æ€ç»´ç§å­
        """
        start_time = time.time()
        self.performance_stats['total_generations'] += 1
        
        logger.info(f"ğŸ¯ å¼€å§‹RAGç§å­ç”Ÿæˆ: {user_query[:50]}...")
        
        try:
            # é˜¶æ®µä¸€ï¼šç†è§£é—®é¢˜å¹¶æ„æ€æœç´¢ç­–ç•¥
            search_strategy = self._analyze_and_plan_search(user_query, execution_context)
            logger.info(f"ğŸ“‹ æœç´¢ç­–ç•¥: {search_strategy.search_intent}")
            
            # é˜¶æ®µäºŒï¼šæ‰§è¡Œç½‘ç»œæœç´¢
            search_results = self._execute_web_search(search_strategy)
            logger.info(f"ğŸ” æœç´¢å®Œæˆ: è·å– {len(search_results)} æ¡ç»“æœ")
            
            # é˜¶æ®µä¸‰ï¼šç»¼åˆä¿¡æ¯å¹¶ç”Ÿæˆç§å­
            synthesis_result = self._synthesize_information(
                user_query, search_strategy, search_results, execution_context
            )
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            generation_time = time.time() - start_time
            self.performance_stats['successful_generations'] += 1
            self._update_performance_stats(generation_time)
            
            logger.info(f"âœ… RAGç§å­ç”ŸæˆæˆåŠŸ (è€—æ—¶: {generation_time:.2f}s)")
            logger.info(f"   ğŸ“Š ä¿¡æ¯å¯ä¿¡åº¦: {synthesis_result.confidence_score:.2f}")
            logger.info(f"   ğŸ“š ä¿¡æ¯æºæ•°é‡: {len(synthesis_result.information_sources)}")
            
            return synthesis_result.contextual_seed
            
        except Exception as e:
            logger.error(f"âŒ RAGç§å­ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€åˆ†ææ¨¡å¼
            return self._generate_fallback_seed(user_query, execution_context)
    
    def _analyze_and_plan_search(self, user_query: str, execution_context: Optional[Dict]) -> RAGSearchStrategy:
        """
        é˜¶æ®µä¸€ï¼šåˆ†æç”¨æˆ·é—®é¢˜å¹¶è§„åˆ’æœç´¢ç­–ç•¥
        
        æ ¸å¿ƒä»»åŠ¡ï¼š
        1. ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
        2. è¯†åˆ«å…³é”®æ¦‚å¿µå’Œå®ä½“
        3. ç¡®å®šæœç´¢é¢†åŸŸå’Œä¿¡æ¯ç±»å‹
        4. ç”Ÿæˆå¤šå±‚æ¬¡æœç´¢å…³é”®è¯
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æœç´¢ç­–ç•¥å¯¹è±¡
        """
        # æ£€æŸ¥ç­–ç•¥ç¼“å­˜
        cache_key = f"{user_query}_{hash(str(execution_context))}"
        if cache_key in self.strategy_cache:
            logger.debug("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æœç´¢ç­–ç•¥")
            return self.strategy_cache[cache_key]
        
        if self.llm_client:
            try:
                return self._llm_based_search_planning(user_query, execution_context)
            except Exception as e:
                logger.warning(f"âš ï¸ LLMæœç´¢è§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•: {e}")
        
        # å¯å‘å¼æœç´¢ç­–ç•¥ç”Ÿæˆ
        return self._heuristic_search_planning(user_query, execution_context)
    
    def _llm_based_search_planning(self, user_query: str, execution_context: Optional[Dict]) -> RAGSearchStrategy:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æœç´¢ç­–ç•¥è§„åˆ’"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{user_query}_{hash(str(execution_context))}"
        if cache_key in self.strategy_cache:
            logger.debug("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„LLMæœç´¢ç­–ç•¥")
            return self.strategy_cache[cache_key]
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè‡ªåŠ¨æ³¨å…¥å½“å‰æ—¶é—´ä¿¡æ¯
        from datetime import datetime
        now = datetime.now()
        current_time_info = f"""
ğŸ“… **å½“å‰æ—¶é—´ä¿¡æ¯** (ç”Ÿæˆæœç´¢å…³é”®è¯æ—¶è¯·åŠ¡å¿…å‚è€ƒ):
- å½“å‰å¹´ä»½: {now.year}å¹´
- å½“å‰æ—¥æœŸ: {now.strftime('%Yå¹´%mæœˆ%dæ—¥')}
- æ˜ŸæœŸ: {now.strftime('%A')}
"""
        
        context_info = ""
        if execution_context:
            context_items = [f"- {k}: {v}" for k, v in execution_context.items()]
            context_info = f"\n\nğŸ“‹ **æ‰§è¡Œä¸Šä¸‹æ–‡**:\n" + "\n".join(context_items)
        
        planning_prompt = f"""
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æ£€ç´¢ç­–ç•¥å¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹ç”¨æˆ·é—®é¢˜åˆ¶å®šç²¾å‡†çš„æœç´¢ç­–ç•¥ã€‚
{current_time_info}

ğŸ¯ **ç”¨æˆ·é—®é¢˜**: {user_query}
{context_info}

ğŸ“ **ä»»åŠ¡è¦æ±‚**:
1. æ·±åº¦ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾å’Œä¿¡æ¯éœ€æ±‚
2. è¯†åˆ«å…³é”®æ¦‚å¿µã€å®ä½“å’ŒæŠ€æœ¯æœ¯è¯­
3. âš ï¸ **é‡è¦**ï¼šåœ¨ç”Ÿæˆæœç´¢å…³é”®è¯æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šæ–¹æä¾›çš„å½“å‰å¹´ä»½ ({now.year}å¹´)ï¼Œè€Œä¸æ˜¯å†å²å¹´ä»½
4. ç¡®å®šæœ€ä½³æœç´¢é¢†åŸŸå’Œä¿¡æ¯ç±»å‹
5. ç”Ÿæˆå¤šå±‚æ¬¡ã€å¤šè§’åº¦çš„æœç´¢å…³é”®è¯ç»„åˆï¼Œç¡®ä¿åŒ…å«æ­£ç¡®çš„æ—¶é—´ä¿¡æ¯
6. è¯„ä¼°æœç´¢æ·±åº¦éœ€æ±‚

ğŸ” **è¾“å‡ºæ ¼å¼** (ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼):
```json
{{
    "search_intent": "ç”¨æˆ·æœç´¢çš„æ ¸å¿ƒæ„å›¾æè¿°",
    "domain_focus": "ä¸»è¦é¢†åŸŸï¼ˆå¦‚ï¼šæŠ€æœ¯ã€å•†ä¸šã€å­¦æœ¯ã€æ–°é—»ç­‰ï¼‰",
    "primary_keywords": ["ä¸»è¦å…³é”®è¯1ï¼ˆåŒ…å«{now.year}å¹´ï¼‰", "ä¸»è¦å…³é”®è¯2", "ä¸»è¦å…³é”®è¯3"],
    "secondary_keywords": ["è¡¥å……å…³é”®è¯1", "è¡¥å……å…³é”®è¯2"],
    "information_types": ["éœ€è¦çš„ä¿¡æ¯ç±»å‹ï¼Œå¦‚ï¼šå®šä¹‰ã€æ•™ç¨‹ã€æ¡ˆä¾‹ã€ç»Ÿè®¡æ•°æ®"],
    "search_depth": "shallow/medium/deepï¼ˆæœç´¢æ·±åº¦ï¼‰"
}}
```

âš ï¸ **ç‰¹åˆ«æé†’**: å¦‚æœç”¨æˆ·é—®é¢˜æ¶‰åŠ"æœ€æ–°"ã€"å½“å‰"ã€"ä»Šå¹´"ç­‰æ—¶é—´ç›¸å…³çš„è¯æ±‡ï¼Œè¯·åœ¨æœç´¢å…³é”®è¯ä¸­æ˜ç¡®ä½¿ç”¨ {now.year}å¹´ï¼Œè€Œä¸æ˜¯å…¶ä»–å¹´ä»½ã€‚
è¯·åŸºäºé—®é¢˜çš„å¤æ‚æ€§å’Œæ—¶æ•ˆæ€§éœ€æ±‚ï¼Œåˆ¶å®šæœ€ä¼˜çš„æœç´¢ç­–ç•¥ã€‚
"""
        
        llm_response = self.llm_client.call_api(planning_prompt, temperature=0.3)
        strategy_data = parse_json_response(llm_response)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„æœç´¢ç­–ç•¥å­—æ®µ
        required_fields = ['search_intent', 'domain_focus', 'primary_keywords']
        if strategy_data and all(field in strategy_data for field in required_fields):
            strategy = RAGSearchStrategy(
                primary_keywords=strategy_data.get('primary_keywords', []),
                secondary_keywords=strategy_data.get('secondary_keywords', []),
                search_intent=strategy_data.get('search_intent', ''),
                domain_focus=strategy_data.get('domain_focus', 'general'),
                information_types=strategy_data.get('information_types', []),
                search_depth=strategy_data.get('search_depth', 'medium')
            )
            
            # ç¼“å­˜ç­–ç•¥
            self.strategy_cache[cache_key] = strategy
            
            return strategy
        else:
            raise ValueError("LLMæœç´¢ç­–ç•¥è§£æå¤±è´¥")
    
    def _heuristic_search_planning(self, user_query: str, execution_context: Optional[Dict]) -> RAGSearchStrategy:
        """åŸºäºå¯å‘å¼è§„åˆ™çš„æœç´¢ç­–ç•¥ç”Ÿæˆ - ğŸš€ æ™ºèƒ½è¯­ä¹‰åˆ†æç‰ˆ"""
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨å¯å‘å¼æ–¹æ³•ä¸­ä¹Ÿæ³¨å…¥å½“å‰å¹´ä»½
        from datetime import datetime
        current_year = datetime.now().year
        
        if self.semantic_analyzer:
            # ğŸš€ ä½¿ç”¨è¯­ä¹‰åˆ†æå™¨è¿›è¡Œæ™ºèƒ½ç­–ç•¥ç”Ÿæˆ
            try:
                # æ‰§è¡Œå¤šä»»åŠ¡è¯­ä¹‰åˆ†æ
                analysis_result = self.semantic_analyzer.analyze(
                    user_query, 
                    ['intent_detection', 'complexity_assessment', 'domain_classification']
                )
                
                primary_keywords = []
                secondary_keywords = []
                search_intent = "å¯»æ‰¾ç›¸å…³äº‹å®ä¿¡æ¯"
                information_types = ["äº‹å®", "æ•°æ®", "æ¡ˆä¾‹"]
                search_depth = "medium"
                domain_focus = "é€šç”¨"
                
                # åŸºäºæ„å›¾åˆ†æç¡®å®šæœç´¢ç­–ç•¥
                if 'intent_detection' in analysis_result.analysis_results:
                    intent_result = analysis_result.analysis_results['intent_detection'].result
                    primary_intent = intent_result.get('primary_intent', '').lower()
                    
                    # æ™ºèƒ½æœç´¢æ„å›¾æ˜ å°„
                    if any(word in primary_intent for word in ['question', 'information', 'explain', 'understand']):
                        search_intent = "å¯»æ‰¾è§£é‡Šæˆ–æŒ‡å¯¼ä¿¡æ¯"
                        information_types = ["æ•™ç¨‹", "å®šä¹‰", "æŒ‡å—", "è§£é‡Š"]
                    elif any(word in primary_intent for word in ['compare', 'evaluation', 'best', 'choose']):
                        search_intent = "å¯»æ‰¾æ¯”è¾ƒå’Œæ¨èä¿¡æ¯"
                        information_types = ["æ¯”è¾ƒ", "è¯„æµ‹", "æ¨è", "å¯¹æ¯”"]
                    elif any(word in primary_intent for word in ['solve', 'problem', 'help', 'fix']):
                        search_intent = "å¯»æ‰¾è§£å†³æ–¹æ¡ˆä¿¡æ¯"
                        information_types = ["è§£å†³æ–¹æ¡ˆ", "æ•™ç¨‹", "å®è·µ", "æ¡ˆä¾‹"]
                
                # åŸºäºå¤æ‚åº¦åˆ†æç¡®å®šæœç´¢æ·±åº¦
                if 'complexity_assessment' in analysis_result.analysis_results:
                    complexity_result = analysis_result.analysis_results['complexity_assessment'].result
                    complexity_level = complexity_result.get('complexity_level', 'medium')
                    
                    if complexity_level in ['high', 'expert']:
                        search_depth = "deep"
                    elif complexity_level == 'low':
                        search_depth = "shallow"
                    else:
                        search_depth = "medium"
                
                # åŸºäºé¢†åŸŸåˆ†æç¡®å®šé¢†åŸŸç„¦ç‚¹
                if 'domain_classification' in analysis_result.analysis_results:
                    domain_result = analysis_result.analysis_results['domain_classification'].result
                    primary_domain = domain_result.get('primary_domain', 'general')
                    
                    domain_mapping = {
                        'technology': 'æŠ€æœ¯',
                        'business': 'å•†ä¸š', 
                        'academic': 'å­¦æœ¯',
                        'health': 'å¥åº·',
                        'creative': 'åˆ›æ„',
                        'general': 'é€šç”¨'
                    }
                    domain_focus = domain_mapping.get(primary_domain, 'é€šç”¨')
                
                # æ™ºèƒ½å…³é”®è¯æå–ï¼ˆåŸºäºæŸ¥è¯¢åˆ†æè€Œéç¡¬ç¼–ç åˆ—è¡¨ï¼‰
                # å…³é”®ä¿®å¤ï¼šä¼ é€’å½“å‰å¹´ä»½
                primary_keywords = self._extract_semantic_keywords(user_query, analysis_result, current_year)
                secondary_keywords = self._generate_secondary_keywords(primary_keywords, domain_focus)
                
                logger.debug("ğŸ” RAGæœç´¢ç­–ç•¥è¯­ä¹‰åˆ†ææˆåŠŸ")
                
                return RAGSearchStrategy(
                    primary_keywords=primary_keywords,
                    secondary_keywords=secondary_keywords,
                    search_intent=search_intent,
                    domain_focus=domain_focus,
                    information_types=information_types,
                    search_depth=search_depth
                )
                
            except Exception as e:
                logger.warning(f"âš ï¸ RAGç­–ç•¥è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤æœç´¢ç­–ç•¥ï¼Œä½†ä»è¦æ³¨å…¥å½“å‰å¹´ä»½
                keywords = user_query.split()[:5]
                # ğŸ”¥ æ£€æµ‹æ—¶é—´ç›¸å…³è¯æ±‡å¹¶æ³¨å…¥å¹´ä»½
                time_related_words = ['æœ€æ–°', 'å½“å‰', 'ä»Šå¹´', 'ç°åœ¨', 'æœ€è¿‘', 'latest', 'current', 'recent']
                if any(word in user_query.lower() for word in time_related_words):
                    keywords.insert(0, f"{current_year}å¹´")
                    logger.info(f"ğŸ• å›é€€ç­–ç•¥ä¸­æ³¨å…¥å½“å‰å¹´ä»½: {current_year}å¹´")
                
                return RAGSearchStrategy(
                    primary_keywords=keywords,
                    secondary_keywords=[],
                    search_intent="å¯»æ‰¾ç›¸å…³ä¿¡æ¯",
                    domain_focus="é€šç”¨",
                    information_types=["äº‹å®", "æ•°æ®"],
                    search_depth="medium"
                )
        else:
            logger.debug("ğŸ“ è¯­ä¹‰åˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–æœç´¢ç­–ç•¥")
            # ç®€åŒ–çš„æœç´¢ç­–ç•¥ï¼Œä½†ä»è¦æ³¨å…¥å½“å‰å¹´ä»½
            keywords = user_query.split()[:5]
            # ğŸ”¥ æ£€æµ‹æ—¶é—´ç›¸å…³è¯æ±‡å¹¶æ³¨å…¥å¹´ä»½
            time_related_words = ['æœ€æ–°', 'å½“å‰', 'ä»Šå¹´', 'ç°åœ¨', 'æœ€è¿‘', 'latest', 'current', 'recent']
            if any(word in user_query.lower() for word in time_related_words):
                keywords.insert(0, f"{current_year}å¹´")
                logger.info(f"ğŸ• ç®€åŒ–ç­–ç•¥ä¸­æ³¨å…¥å½“å‰å¹´ä»½: {current_year}å¹´")
            
            return RAGSearchStrategy(
                primary_keywords=keywords,
                secondary_keywords=[],
                search_intent="å¯»æ‰¾ç›¸å…³ä¿¡æ¯",
                domain_focus="é€šç”¨", 
                information_types=["äº‹å®", "æ•°æ®"],
                search_depth="medium"
            )
    
    def _extract_semantic_keywords(self, user_query: str, analysis_result, current_year: int = None) -> List[str]:
        """
        åŸºäºè¯­ä¹‰åˆ†ææå–å…³é”®è¯
        
        ğŸ”¥ å¢å¼ºç‰ˆï¼šè‡ªåŠ¨æ£€æµ‹æ—¶é—´ç›¸å…³æŸ¥è¯¢å¹¶æ³¨å…¥å½“å‰å¹´ä»½
        """
        keywords = []
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æµ‹æ—¶é—´ç›¸å…³çš„è¯æ±‡
        if current_year is None:
            from datetime import datetime
            current_year = datetime.now().year
        
        time_related_words = ['æœ€æ–°', 'å½“å‰', 'ä»Šå¹´', 'ç°åœ¨', 'æœ€è¿‘', 'æ–°', 'å‘å±•', 'è¶‹åŠ¿', 'latest', 'current', 'recent', 'new', 'trend']
        query_lower = user_query.lower()
        has_time_context = any(word in query_lower for word in time_related_words)
        
        # å¦‚æœæŸ¥è¯¢åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡ï¼Œåœ¨å…³é”®è¯ä¸­åŠ å…¥å½“å‰å¹´ä»½
        if has_time_context:
            keywords.append(f"{current_year}å¹´")
            keywords.append(str(current_year))
            logger.info(f"ğŸ• æ£€æµ‹åˆ°æ—¶é—´ç›¸å…³æŸ¥è¯¢ï¼Œå·²æ³¨å…¥å½“å‰å¹´ä»½: {current_year}å¹´")
        
        # åŸºç¡€è¯æ±‡æå–ï¼ˆä¿ç•™ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼‰
        import re
        words = re.findall(r'\b\w+\b', user_query.lower())
        important_words = [w for w in words if len(w) > 3][:5]
        keywords.extend(important_words)
        
        # åŸºäºé¢†åŸŸåˆ†ææ·»åŠ ä¸“ä¸šè¯æ±‡
        if 'domain_classification' in analysis_result.analysis_results:
            domain_result = analysis_result.analysis_results['domain_classification'].result
            primary_domain = domain_result.get('primary_domain', 'general')
            
            # æ ¹æ®é¢†åŸŸæ·»åŠ ç›¸å…³æœç´¢è¯æ±‡
            domain_keywords = {
                'technology': ['æŠ€æœ¯', 'solution', 'implementation', 'å®ç°'],
                'business': ['strategy', 'ç­–ç•¥', 'business', 'å•†ä¸š'],
                'academic': ['research', 'ç ”ç©¶', 'study', 'å­¦æœ¯'],
                'health': ['health', 'å¥åº·', 'medical', 'åŒ»ç–—'],
                'creative': ['design', 'è®¾è®¡', 'creative', 'åˆ›æ„']
            }
            
            if primary_domain in domain_keywords:
                keywords.extend(domain_keywords[primary_domain])
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        keywords = list(set(keywords))[:8]
        return keywords
    
    def _generate_secondary_keywords(self, primary_keywords: List[str], domain_focus: str) -> List[str]:
        """ç”Ÿæˆæ¬¡è¦å…³é”®è¯"""
        secondary = []
        
        # åŸºäºé¢†åŸŸæ·»åŠ ç›¸å…³è¯æ±‡
        domain_secondary = {
            'æŠ€æœ¯': ['best practices', 'æœ€ä½³å®è·µ', 'tutorial', 'æ•™ç¨‹'],
            'å•†ä¸š': ['case study', 'æ¡ˆä¾‹', 'market', 'å¸‚åœº'],
            'å­¦æœ¯': ['literature', 'æ–‡çŒ®', 'methodology', 'æ–¹æ³•'],
            'å¥åº·': ['guidelines', 'æŒ‡å—', 'symptoms', 'ç—‡çŠ¶'],
            'åˆ›æ„': ['inspiration', 'çµæ„Ÿ', 'examples', 'ç¤ºä¾‹'],
            'é€šç”¨': ['guide', 'æŒ‡å—', 'tips', 'æŠ€å·§']
        }
        
        if domain_focus in domain_secondary:
            secondary.extend(domain_secondary[domain_focus])
        
        return secondary[:4]
    
    
    def _execute_web_search(self, strategy: RAGSearchStrategy) -> List[SearchResult]:
        """
        é˜¶æ®µäºŒï¼šæ‰§è¡Œç½‘ç»œæœç´¢ï¼ˆæ”¯æŒå¹¶è¡Œä¼˜åŒ–ï¼‰
        
        æ ¸å¿ƒä»»åŠ¡ï¼š
        1. åŸºäºæœç´¢ç­–ç•¥æ‰§è¡Œå¤šè½®æœç´¢
        2. ç»„åˆä¸åŒå…³é”®è¯è¿›è¡Œå…¨é¢æ£€ç´¢
        3. è¿‡æ»¤å’Œå»é‡æœç´¢ç»“æœ
        4. æŒ‰ç›¸å…³æ€§æ’åº
        
        ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼š
        - æ”¯æŒå¹¶è¡Œæœç´¢ï¼Œå¤§å¹…ç¼©çŸ­æ€»è€—æ—¶
        - æ™ºèƒ½é…ç½®å¹¶å‘æ•°é‡
        - å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œé™çº§
        
        Args:
            strategy: æœç´¢ç­–ç•¥
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []
        search_queries = []
        
        #ç»ˆä¿é™©æªæ–½ï¼šè·å–å½“å‰å¹´ä»½
        from datetime import datetime
        current_year = datetime.now().year
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        # ä¸»è¦å…³é”®è¯ç»„åˆ
        for keyword in strategy.primary_keywords[:3]:  # é™åˆ¶ä¸»è¦æœç´¢æ¬¡æ•°
            search_queries.append(keyword)
        
        # ç»„åˆæŸ¥è¯¢ï¼ˆä¸»è¦+æ¬¡è¦å…³é”®è¯ï¼‰
        if strategy.secondary_keywords:
            for primary in strategy.primary_keywords[:2]:
                for secondary in strategy.secondary_keywords[:2]:
                    search_queries.append(f"{primary} {secondary}")
        
        # é™åˆ¶æ€»æœç´¢æ¬¡æ•°
        search_queries = search_queries[:5]
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæœ€ç»ˆå¹´ä»½éªŒè¯å’Œæ›¿æ¢
        search_queries = self._validate_and_fix_year_in_queries(search_queries, current_year)
        
        logger.info(f"ğŸ” æ‰§è¡Œ {len(search_queries)} è½®æœç´¢æŸ¥è¯¢")
        
        # ğŸš€ å†³å®šä½¿ç”¨å¹¶è¡Œæœç´¢è¿˜æ˜¯ä¸²è¡Œæœç´¢
        enable_parallel = RAG_CONFIG.get("enable_parallel_search", True)
        max_workers = RAG_CONFIG.get("max_search_workers", 3)
        
        if enable_parallel and len(search_queries) > 1:
            logger.info(f"âš¡ å¯ç”¨å¹¶è¡Œæœç´¢æ¨¡å¼ - æœ€å¤§å¹¶å‘æ•°: {max_workers}")
            all_results = self._execute_parallel_search(search_queries, max_workers)
        else:
            logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»Ÿä¸²è¡Œæœç´¢æ¨¡å¼")
            all_results = self._execute_serial_search(search_queries)
        
        # å»é‡å’Œè¿‡æ»¤
        unique_results = self._filter_and_deduplicate_results(all_results, strategy)
        
        logger.info(f"ğŸ¯ æœç´¢å®Œæˆ: {len(all_results)} -> {len(unique_results)} (å»é‡å)")
        return unique_results
    
    def _validate_and_fix_year_in_queries(self, queries: List[str], current_year: int) -> List[str]:
        """
        æœ€ç»ˆå¹´ä»½éªŒè¯ï¼šæ£€æŸ¥å¹¶æ›¿æ¢æŸ¥è¯¢ä¸­çš„é”™è¯¯å¹´ä»½ï¼Œå¹¶ä¸ºæ—¶é—´ç›¸å…³æŸ¥è¯¢ä¸»åŠ¨æ·»åŠ å¹´ä»½
        
        è¿™æ˜¯æœ€åä¸€é“é˜²çº¿ï¼Œç¡®ä¿æ‰€æœ‰å‘é€åˆ°æœç´¢å¼•æ“çš„æŸ¥è¯¢éƒ½ä½¿ç”¨æ­£ç¡®çš„å¹´ä»½
        
        Args:
            queries: åŸå§‹æŸ¥è¯¢åˆ—è¡¨
            current_year: å½“å‰æ­£ç¡®çš„å¹´ä»½
            
        Returns:
            ä¿®æ­£åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        import re
        
        fixed_queries = []
        year_pattern = r'20\d{2}å¹´?'  # åŒ¹é…2000-2099å¹´çš„å¹´ä»½
        
        # æ—¶é—´ç›¸å…³å…³é”®è¯ - éœ€è¦æ³¨å…¥å¹´ä»½
        time_related_keywords = [
            'æœ€æ–°', 'å½“å‰', 'ä»Šå¹´', 'ç°åœ¨', 'æœ€è¿‘', 'æ–°', 'å‘å±•', 'è¶‹åŠ¿', 'åŠ¨æ€', 'è¿›å±•',
            'latest', 'current', 'recent', 'new', 'trend', 'update', 'progress', 'development'
        ]
        
        for query in queries:
            original_query = query
            modified = False
            
            # 1. æ›¿æ¢é”™è¯¯çš„å¹´ä»½
            years_found = re.findall(year_pattern, query)
            if years_found:
                for year_str in years_found:
                    year_num = int(re.sub(r'[^\d]', '', year_str))
                    if year_num != current_year:
                        logger.warning(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯å¹´ä»½: {year_str} (åº”ä¸º {current_year}å¹´)")
                        query = query.replace(year_str, f"{current_year}å¹´")
                        modified = True
            
            # 2. ğŸ”¥ å…³é”®å¢å¼ºï¼šä¸ºæ—¶é—´ç›¸å…³æŸ¥è¯¢ä¸»åŠ¨æ·»åŠ å¹´ä»½ï¼ˆå¦‚æœå°šæœªåŒ…å«ï¼‰
            query_lower = query.lower()
            has_time_context = any(keyword in query_lower for keyword in time_related_keywords)
            has_year = bool(re.search(year_pattern, query))
            
            if has_time_context and not has_year:
                # æŸ¥è¯¢åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡ä½†æ²¡æœ‰å¹´ä»½ï¼Œä¸»åŠ¨æ·»åŠ 
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ—¶é—´ç›¸å…³æŸ¥è¯¢ä½†ç¼ºå°‘å¹´ä»½: {query}")
                # æ™ºèƒ½æ’å…¥å¹´ä»½ï¼šåœ¨ç¬¬ä¸€ä¸ªæ—¶é—´ç›¸å…³è¯æ±‡åæ·»åŠ 
                for keyword in time_related_keywords:
                    if keyword in query_lower:
                        # æ‰¾åˆ°å…³é”®è¯ä½ç½®å¹¶æ’å…¥å¹´ä»½
                        idx = query_lower.index(keyword)
                        insert_pos = idx + len(keyword)
                        query = query[:insert_pos] + f" {current_year}å¹´" + query[insert_pos:]
                        modified = True
                        break
            
            if modified:
                logger.info(f"ğŸ”§ å·²ä¿®æ­£æŸ¥è¯¢: {original_query} -> {query}")
            
            fixed_queries.append(query)
            logger.debug(f"   âœ“ æœ€ç»ˆæŸ¥è¯¢: {query}")
        
        return fixed_queries
    
    def _execute_parallel_search(self, search_queries: List[str], max_workers: int) -> List[SearchResult]:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæœç´¢æŸ¥è¯¢
        
        Args:
            search_queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []
        start_time = time.time()
        
        # åˆ›å»ºæœç´¢ä»»åŠ¡
        def search_single_query(query: str) -> Tuple[str, List[SearchResult]]:
            """æ‰§è¡Œå•ä¸ªæœç´¢æŸ¥è¯¢"""
            try:
                logger.debug(f"   ğŸ” å¹¶è¡Œæœç´¢: {query}")
                response = self.web_search_client.search(query)
                
                if response and response.results:
                    logger.debug(f"   âœ… æœç´¢æˆåŠŸ: {query} -> {len(response.results)} æ¡ç»“æœ")
                    return query, response.results
                else:
                    logger.debug(f"   ğŸ” æœç´¢æ— ç»“æœ: {query}")
                    return query, []
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æœç´¢å¤±è´¥ '{query}': {e}")
                return query, []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œæœç´¢
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
            future_to_query = {
                executor.submit(search_single_query, query): query 
                for query in search_queries
            }
            
            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    query_name, results = future.result()
                    all_results.extend(results)
                    completed_count += 1
                    
                    logger.debug(f"âœ… æœç´¢å®Œæˆ ({completed_count}/{len(search_queries)}): {query_name}")
                    
                except Exception as e:
                    logger.error(f"âŒ æœç´¢ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {query} - {e}")
        
        duration = time.time() - start_time
        logger.info(f"ğŸ¯ å¹¶è¡Œæœç´¢å®Œæˆ - è€—æ—¶: {duration:.2f}s, è·å¾— {len(all_results)} æ¡ç»“æœ")
        
        return all_results
    
    def _execute_serial_search(self, search_queries: List[str]) -> List[SearchResult]:
        """
        ä¸²è¡Œæ‰§è¡Œæœç´¢æŸ¥è¯¢ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        
        Args:
            search_queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []
        
        for query in search_queries:
            try:
                logger.debug(f"   æœç´¢: {query}")
                response = self.web_search_client.search(query)
                
                if response and response.results:
                    all_results.extend(response.results)
                    logger.debug(f"   è·å¾— {len(response.results)} æ¡ç»“æœ")
                else:
                    logger.debug(f"   æœç´¢æ— ç»“æœ: {query}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æœç´¢å¤±è´¥ '{query}': {e}")
                continue
        
        return all_results
    
    def _filter_and_deduplicate_results(self, results: List[SearchResult], 
                                       strategy: RAGSearchStrategy) -> List[SearchResult]:
        """è¿‡æ»¤å’Œå»é‡æœç´¢ç»“æœ"""
        if not results:
            return []
        
        # æŒ‰URLå»é‡
        seen_urls = set()
        unique_results = []
        
        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§æ’åºï¼ˆåŸºäºæ ‡é¢˜å’Œæ‘˜è¦ä¸­å…³é”®è¯åŒ¹é…åº¦ï¼‰
        def relevance_score(result: SearchResult) -> float:
            score = 0.0
            text = f"{result.title} {result.snippet}".lower()
            
            # ä¸»è¦å…³é”®è¯æƒé‡æ›´é«˜
            for keyword in strategy.primary_keywords:
                if keyword.lower() in text:
                    score += 2.0
            
            # æ¬¡è¦å…³é”®è¯æƒé‡è¾ƒä½
            for keyword in strategy.secondary_keywords:
                if keyword.lower() in text:
                    score += 1.0
            
            return score
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ªç»“æœ
        unique_results.sort(key=relevance_score, reverse=True)
        return unique_results[:8]  # è¿”å›æœ€ç›¸å…³çš„8ä¸ªç»“æœ
    
    def _synthesize_information(self, user_query: str, strategy: RAGSearchStrategy, 
                               search_results: List[SearchResult], 
                               execution_context: Optional[Dict]) -> RAGInformationSynthesis:
        """
        é˜¶æ®µä¸‰ï¼šç»¼åˆä¿¡æ¯å¹¶ç”Ÿæˆæ€ç»´ç§å­
        
        æ ¸å¿ƒä»»åŠ¡ï¼š
        1. åˆ†ææ‰€æœ‰æœç´¢ç»“æœçš„å†…å®¹
        2. è¯†åˆ«å…³é”®ä¿¡æ¯å’Œæ´å¯Ÿ
        3. éªŒè¯ä¿¡æ¯ä¸€è‡´æ€§å’Œå¯é æ€§
        4. ç”Ÿæˆä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ€ç»´ç§å­
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            strategy: æœç´¢ç­–ç•¥
            search_results: æœç´¢ç»“æœ
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            ä¿¡æ¯ç»¼åˆç»“æœ
        """
        if not search_results:
            logger.warning("âš ï¸ æ— æœç´¢ç»“æœï¼Œç”ŸæˆåŸºç¡€ç§å­")
            return RAGInformationSynthesis(
                contextual_seed=f"åŸºäºç”¨æˆ·é—®é¢˜'{user_query}'çš„åŸºç¡€åˆ†æã€‚ç”±äºç¼ºä¹å®æ—¶ä¿¡æ¯ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒç ”ç›¸å…³èµ„æ–™ã€‚",
                information_sources=[],
                confidence_score=0.3,
                key_insights=["éœ€è¦æ›´å¤šä¿¡æ¯"],
                knowledge_gaps=["å®æ—¶æ•°æ®ç¼ºå¤±"],
                verification_status="insufficient_data"
            )
        
        if self.llm_client:
            try:
                return self._llm_based_synthesis(user_query, strategy, search_results, execution_context)
            except Exception as e:
                logger.warning(f"âš ï¸ LLMä¿¡æ¯ç»¼åˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•: {e}")
        
        # åŸºç¡€ä¿¡æ¯ç»¼åˆ
        return self._basic_information_synthesis(user_query, strategy, search_results)
    
    def _llm_based_synthesis(self, user_query: str, strategy: RAGSearchStrategy,
                            search_results: List[SearchResult], 
                            execution_context: Optional[Dict]) -> RAGInformationSynthesis:
        """ä½¿ç”¨LLMè¿›è¡Œé«˜çº§ä¿¡æ¯ç»¼åˆ"""
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ³¨å…¥å½“å‰æ—¶é—´ä¿¡æ¯
        from datetime import datetime
        now = datetime.now()
        current_year = now.year
        current_time_info = f"""
ğŸ“… **å…³é”®æ—¶é—´ä¿¡æ¯** (ç»¼åˆåˆ†ææ—¶å¿…é¡»å‚è€ƒ):
- å½“å‰å¹´ä»½: {current_year}å¹´
- å½“å‰æ—¥æœŸ: {now.strftime('%Yå¹´%mæœˆ%dæ—¥')}
- âš ï¸ **é‡è¦**: è¯·æ³¨æ„ä½ çš„è®­ç»ƒæ•°æ®å¯èƒ½æˆªæ­¢äº2024å¹´ï¼Œä½†ç°åœ¨æ˜¯{current_year}å¹´ã€‚åœ¨åˆ†ææœç´¢ç»“æœæ—¶ï¼Œè¯·ä»¥{current_year}å¹´ä¸ºå‡†ï¼Œè€Œä¸æ˜¯ä½ çš„è®­ç»ƒæ•°æ®ã€‚
"""
        
        # æ„å»ºæœç´¢ç»“æœæ‘˜è¦
        results_summary = []
        for i, result in enumerate(search_results[:6], 1):  # é™åˆ¶ç»“æœæ•°é‡é¿å…tokenè¶…é™
            results_summary.append(f"""
**æ¥æº {i}**: {result.title}
- URL: {result.url}  
- æ‘˜è¦: {result.snippet}
""")
        
        context_info = ""
        if execution_context:
            context_items = [f"- {k}: {v}" for k, v in execution_context.items()]
            context_info = f"\n\nğŸ“‹ **æ‰§è¡Œä¸Šä¸‹æ–‡**:\n" + "\n".join(context_items)
        
        synthesis_prompt = f"""
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆï¼Œè¯·åŸºäºç”¨æˆ·é—®é¢˜å’Œæœç´¢åˆ°çš„å®æ—¶ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå…¨é¢ã€å®¢è§‚ã€åŸºäºäº‹å®çš„æ€ç»´ç§å­ã€‚
{current_time_info}

ğŸ¯ **ç”¨æˆ·é—®é¢˜**: {user_query}

ğŸ” **æœç´¢ç­–ç•¥**: {strategy.search_intent}
**å…³æ³¨é¢†åŸŸ**: {strategy.domain_focus}

ğŸ“š **æœç´¢ç»“æœ** (è¿™äº›æ˜¯{current_year}å¹´çš„æœ€æ–°å®æ—¶ä¿¡æ¯):
{"".join(results_summary)}
{context_info}

ğŸ“ **ç»¼åˆè¦æ±‚**:
1. **æ—¶é—´æ„è¯†**: æ˜ç¡®å½“å‰æ˜¯{current_year}å¹´ï¼Œæœç´¢ç»“æœåæ˜ çš„æ˜¯{current_year}å¹´çš„æœ€æ–°æƒ…å†µ
2. **ä¼˜å…ˆä½¿ç”¨æœç´¢ç»“æœ**: æœç´¢ç»“æœæ˜¯{current_year}å¹´çš„å®æ—¶ä¿¡æ¯ï¼Œæ¯”ä½ çš„è®­ç»ƒæ•°æ®ï¼ˆå¯èƒ½æˆªæ­¢2024å¹´ï¼‰æ›´æ–°ï¼Œè¯·ä¼˜å…ˆåŸºäºæœç´¢ç»“æœå›ç­”
3. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: å……åˆ†ç†è§£ç”¨æˆ·é—®é¢˜çš„èƒŒæ™¯å’Œæ„å›¾
4. **äº‹å®åŸºç¡€**: ä¸¥æ ¼åŸºäºæœç´¢ç»“æœçš„çœŸå®ä¿¡æ¯ï¼Œé¿å…ä½¿ç”¨è¿‡æ—¶çš„è®­ç»ƒæ•°æ®
5. **ä¿¡æ¯æ•´åˆ**: å°†å¤šä¸ªæ¥æºçš„ä¿¡æ¯è¿›è¡Œæœ‰æœºæ•´åˆ
6. **å…³é”®æ´å¯Ÿ**: æå–æœ€é‡è¦çš„è§‚ç‚¹å’Œå‘ç°
7. **çŸ¥è¯†ç¼ºå£**: è¯†åˆ«ä¿¡æ¯ä¸è¶³æˆ–éœ€è¦è¿›ä¸€æ­¥éªŒè¯çš„é¢†åŸŸ
8. **å®ç”¨å¯¼å‘**: ç”Ÿæˆå¯¹åç»­å†³ç­–æœ‰å¸®åŠ©çš„æ€è€ƒæ–¹å‘

âš ï¸ **ç‰¹åˆ«æé†’**: å¦‚æœæœç´¢ç»“æœä¸­æåˆ°çš„æ—¶é—´æ˜¯{current_year}å¹´ï¼Œè¯·åœ¨æ€ç»´ç§å­ä¸­ä¹Ÿä½¿ç”¨{current_year}å¹´ï¼Œè€Œä¸æ˜¯é»˜è®¤ä½¿ç”¨2024å¹´æˆ–å…¶ä»–è¿‡å»çš„å¹´ä»½ã€‚

ğŸ¯ **è¾“å‡ºæ ¼å¼** (ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼):
```json
{{
    "contextual_seed": "åŸºäº{current_year}å¹´å®æ—¶ä¿¡æ¯çš„å…¨é¢æ€ç»´ç§å­ï¼ˆ200-400å­—ï¼‰",
    "key_insights": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2", "å…³é”®æ´å¯Ÿ3"],
    "knowledge_gaps": ["éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„æ–¹é¢1", "éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„æ–¹é¢2"],
    "confidence_score": 0.85,
    "information_sources": ["å¯é æ¥æº1", "å¯é æ¥æº2"],
    "verification_status": "verified/partially_verified/needs_verification"
}}
```

è¯·ç¡®ä¿ç”Ÿæˆçš„æ€ç»´ç§å­å…·æœ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œèƒ½å¤Ÿä¸ºåç»­çš„æ€ç»´è·¯å¾„é€‰æ‹©æä¾›åšå®çš„äº‹å®åŸºç¡€ã€‚
"""
        
        llm_response = self.llm_client.call_api(synthesis_prompt, temperature=0.4)
        synthesis_data = parse_json_response(llm_response)
        
        if synthesis_data:
            return RAGInformationSynthesis(
                contextual_seed=synthesis_data.get('contextual_seed', ''),
                information_sources=synthesis_data.get('information_sources', []),
                confidence_score=float(synthesis_data.get('confidence_score', 0.5)),
                key_insights=synthesis_data.get('key_insights', []),
                knowledge_gaps=synthesis_data.get('knowledge_gaps', []),
                verification_status=synthesis_data.get('verification_status', 'unknown')
            )
        else:
            raise ValueError("LLMä¿¡æ¯ç»¼åˆè§£æå¤±è´¥")
    
    def _basic_information_synthesis(self, user_query: str, strategy: RAGSearchStrategy,
                                   search_results: List[SearchResult]) -> RAGInformationSynthesis:
        """åŸºç¡€ä¿¡æ¯ç»¼åˆæ–¹æ³•"""
        
        # æå–å…³é”®ä¿¡æ¯
        all_snippets = [result.snippet for result in search_results if result.snippet]
        all_titles = [result.title for result in search_results if result.title]
        
        # ç”ŸæˆåŸºç¡€ç§å­
        seed_parts = [
            f"åŸºäºå¯¹'{user_query}'çš„æœç´¢è°ƒç ”ï¼Œ",
            f"ä»{len(search_results)}ä¸ªä¿¡æ¯æºè·å¾—ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š",
        ]
        
        # æ·»åŠ ä¸»è¦ä¿¡æ¯ç‚¹
        for i, snippet in enumerate(all_snippets[:3], 1):
            seed_parts.append(f"{i}. {snippet[:100]}...")
        
        # æ·»åŠ ç»“è®º
        seed_parts.append(f"è¿™äº›ä¿¡æ¯è¡¨æ˜{strategy.search_intent.lower()}çš„é‡è¦æ€§ï¼Œ")
        seed_parts.append("å»ºè®®åœ¨åˆ¶å®šè§£å†³æ–¹æ¡ˆæ—¶å……åˆ†è€ƒè™‘è¿™äº›å®æ—¶ä¿¡æ¯ã€‚")
        
        contextual_seed = " ".join(seed_parts)
        
        return RAGInformationSynthesis(
            contextual_seed=contextual_seed,
            information_sources=[result.url for result in search_results[:3]],
            confidence_score=0.6,
            key_insights=all_titles[:3],
            knowledge_gaps=["éœ€è¦æ›´è¯¦ç»†çš„åˆ†æ"],
            verification_status="basic_verified"
        )
    
    def _generate_fallback_seed(self, user_query: str, execution_context: Optional[Dict]) -> str:
        """ç”Ÿæˆå›é€€æ€ç»´ç§å­"""
        logger.info("ğŸ”„ ä½¿ç”¨å›é€€æ¨¡å¼ç”Ÿæˆæ€ç»´ç§å­")
        
        fallback_seed = f"""
åŸºäºå¯¹é—®é¢˜'{user_query}'çš„åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦ç»¼åˆè€ƒè™‘çš„é—®é¢˜ã€‚
è™½ç„¶å½“å‰æ— æ³•è·å–å®æ—¶ä¿¡æ¯ï¼Œä½†å¯ä»¥ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œæ€è€ƒï¼š
1. é—®é¢˜çš„æ ¸å¿ƒè¦æ±‚å’Œçº¦æŸæ¡ä»¶
2. å¯èƒ½çš„è§£å†³æ–¹æ¡ˆå’Œå®ç°è·¯å¾„  
3. éœ€è¦è€ƒè™‘çš„é£é™©å’ŒæŒ‘æˆ˜
4. ç›¸å…³çš„æœ€ä½³å®è·µå’Œç»éªŒ
å»ºè®®åœ¨å…·ä½“å®æ–½å‰è·å–æœ€æ–°çš„ç›¸å…³ä¿¡æ¯å’Œæ•°æ®ã€‚
"""
        return fallback_seed.strip()
    
    def _update_performance_stats(self, generation_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        total = self.performance_stats['total_generations']
        current_avg = self.performance_stats['avg_generation_time']
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡æ—¶é—´
        if total == 1:
            self.performance_stats['avg_generation_time'] = generation_time
        else:
            self.performance_stats['avg_generation_time'] = (
                current_avg * (total - 1) + generation_time
            ) / total
        
        # æ›´æ–°æˆåŠŸç‡
        success_count = self.performance_stats['successful_generations']
        self.performance_stats['search_success_rate'] = success_count / total
    
    def get_rag_performance_stats(self) -> Dict[str, Any]:
        """è·å–RAGæ€§èƒ½ç»Ÿè®¡"""
        return {
            'component': 'RAG_Seed_Generator',
            'performance_stats': self.performance_stats.copy(),
            'cache_stats': {
                'strategy_cache_size': len(self.strategy_cache),
                'information_cache_size': len(self.information_cache),
                'synthesis_cache_size': len(self.synthesis_cache)
            },
            'quality_metrics': {
                'information_diversity': dict(self.rag_quality_metrics['information_diversity']),
                'source_reliability': dict(self.rag_quality_metrics['source_reliability']),
                'contextual_relevance': dict(self.rag_quality_metrics['contextual_relevance'])
            }
        }
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.strategy_cache.clear()
        self.information_cache.clear() 
        self.synthesis_cache.clear()
        logger.info("ğŸ§¹ RAGç§å­ç”Ÿæˆå™¨ç¼“å­˜å·²æ¸…ç©º")
